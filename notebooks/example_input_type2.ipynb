{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import stelarImputation as tsi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:00:42.820661Z",
     "start_time": "2024-04-18T13:00:41.188311Z"
    }
   },
   "id": "7d06e22ee8dfb7bd",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gap Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deb58fa26858ef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_input = '../datasets/example_input_type2.csv'\n",
    "\n",
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'datetime_format' : '%Y-%m-%d',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    'train_params': {\n",
    "        \"gap_type\": \"no_overlap\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 100,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "datetime_format = parameters['datetime_format']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:00:42.825327Z",
     "start_time": "2024-04-18T13:00:42.822019Z"
    }
   },
   "id": "88fa2ad58b835c1e",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 480\n"
     ]
    },
    {
     "data": {
      "text/plain": "          time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0   2017-01-02                      NaN                 -1.347635   \n1   2017-01-03                      NaN                 -1.189826   \n2   2017-01-04                      NaN                 -1.325091   \n3   2017-01-05                      NaN                 -1.336363   \n4   2017-01-06                      NaN                 -1.313819   \n..         ...                      ...                       ...   \n516 2018-12-25                -1.509627                  0.405175   \n517 2018-12-26                -1.509627                  0.405175   \n518 2018-12-27                -1.310175                  0.433355   \n519 2018-12-28                -1.027002                  0.861694   \n520 2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                   1.753102  ...                          -0.465855   \n4                   1.890973  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>NaN</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>NaN</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>NaN</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>NaN</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>1.753102</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>NaN</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>1.890973</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missing = tsi.run_gap_generation(ground_truth=df_input, \n",
    "                                    train_params=train_params, \n",
    "                                    time_column=time_column, \n",
    "                                    datetime_format=datetime_format, \n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "\n",
    "missing = df_missing.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "df_missing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:00:45.208254Z",
     "start_time": "2024-04-18T13:00:45.166054Z"
    }
   },
   "id": "658dfb14d4145728",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06dd72064d4e510"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'datetime_format' : '%Y-%m-%d',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": { \n",
    "        \"SoftImpute\": { \"max_rank\": 5 },\n",
    "        \"IterativeSVD\": { \"rank\": 3 }, \n",
    "        \"SVT\": { \"tauScale\": 0.7}, \n",
    "        \"TimesNet\":{ \n",
    "            \"n_layers\": 2, \"top_k\": 3, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"n_kernels\":1, \"dropout\":0.05, \n",
    "            \"apply_nonstationary_norm\": False,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0                                                        \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "datetime_format = parameters['datetime_format']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:01:27.145568Z",
     "start_time": "2024-04-18T13:01:27.141274Z"
    }
   },
   "id": "eab173986d693e6b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "freeing copy memory @ 0x1dcfb6a0\n",
      "freeing copy memory @ 0x1dd5d1e0\n",
      "freeing copy memory @ 0x1ddbed20\n",
      "2024-04-18 16:01:56 [INFO]: No given device, using default device: cuda\n",
      "2024-04-18 16:01:56 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-04-18 16:01:56 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,105\n",
      "2024-04-18 16:01:56 [INFO]: Epoch 001 - training loss: 1.4196\n",
      "2024-04-18 16:01:56 [INFO]: Epoch 002 - training loss: 0.6113\n",
      "2024-04-18 16:01:56 [INFO]: Epoch 003 - training loss: 0.3391\n",
      "2024-04-18 16:01:56 [INFO]: Epoch 004 - training loss: 0.3217\n",
      "2024-04-18 16:01:56 [INFO]: Epoch 005 - training loss: 0.3105\n",
      "2024-04-18 16:01:56 [INFO]: Epoch 006 - training loss: 0.2794\n",
      "2024-04-18 16:01:56 [INFO]: Epoch 007 - training loss: 0.2406\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 008 - training loss: 0.2041\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 009 - training loss: 0.1992\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 010 - training loss: 0.2112\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 011 - training loss: 0.1943\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 012 - training loss: 0.1833\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 013 - training loss: 0.1690\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 014 - training loss: 0.1786\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 015 - training loss: 0.1669\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 016 - training loss: 0.1653\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 017 - training loss: 0.1583\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 018 - training loss: 0.1439\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 019 - training loss: 0.1491\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 020 - training loss: 0.1453\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 021 - training loss: 0.1356\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 022 - training loss: 0.1330\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 023 - training loss: 0.1281\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 024 - training loss: 0.1279\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 025 - training loss: 0.1248\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 026 - training loss: 0.1258\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 027 - training loss: 0.1228\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 028 - training loss: 0.1218\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 029 - training loss: 0.1185\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 030 - training loss: 0.1162\n",
      "2024-04-18 16:01:57 [INFO]: Epoch 031 - training loss: 0.1156\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 032 - training loss: 0.1117\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 033 - training loss: 0.1083\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 034 - training loss: 0.1094\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 035 - training loss: 0.1062\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 036 - training loss: 0.1004\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 037 - training loss: 0.1010\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 038 - training loss: 0.0911\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 039 - training loss: 0.0985\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 040 - training loss: 0.0948\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 041 - training loss: 0.0953\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 042 - training loss: 0.0934\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 043 - training loss: 0.0900\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 044 - training loss: 0.1035\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 045 - training loss: 0.0935\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 046 - training loss: 0.0941\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 047 - training loss: 0.0941\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 048 - training loss: 0.1035\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 049 - training loss: 0.0905\n",
      "2024-04-18 16:01:58 [INFO]: Epoch 050 - training loss: 0.0865\n",
      "2024-04-18 16:01:58 [INFO]: Finished training.\n"
     ]
    }
   ],
   "source": [
    "dict_of_imputed_dfs = tsi.run_imputation(missing = df_missing, \n",
    "                                         algorithms=algorithms, \n",
    "                                         params=params, \n",
    "                                         time_column=time_column,\n",
    "                                         datetime_format=datetime_format, \n",
    "                                         header=header, \n",
    "                                         sep=sep, \n",
    "                                         is_multivariate=is_multivariate, \n",
    "                                         areaVStime=areaVStime, \n",
    "                                         preprocessing=preprocessing, \n",
    "                                         index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:01:58.793369Z",
     "start_time": "2024-04-18T13:01:50.797380Z"
    }
   },
   "id": "bf4f82d7f54c335f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "          time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0   2017-01-02                -2.603107                 -1.347635   \n1   2017-01-03                -2.581500                 -1.189826   \n2   2017-01-04                -2.526286                 -1.325091   \n3   2017-01-05                -2.510187                 -1.336363   \n4   2017-01-06                -2.494425                 -1.313819   \n..         ...                      ...                       ...   \n516 2018-12-25                -1.509627                  0.405175   \n517 2018-12-26                -1.509627                  0.405175   \n518 2018-12-27                -1.310175                  0.433355   \n519 2018-12-28                -1.027002                  0.861694   \n520 2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                   1.753102  ...                          -0.465855   \n4                   1.890973  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-2.603107</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-2.581500</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-2.526286</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-2.510187</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>1.753102</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-2.494425</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>1.890973</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df =dict_of_imputed_dfs['SoftImpute']\n",
    "missing = imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:01:58.813307Z",
     "start_time": "2024-04-18T13:01:58.794584Z"
    }
   },
   "id": "43857f333cf8cfd9",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "851d34f0bc65c4b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'datetime_format' : '%Y-%m-%d',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": { \n",
    "        \"SoftImpute\": { \"max_rank\": 5 },\n",
    "        \"IterativeSVD\": { \"rank\": 3 }, \n",
    "        \"SVT\": { \"tauScale\": 0.7}, \n",
    "        \"TimesNet\":{ \n",
    "            \"n_layers\": 2, \"top_k\": 3, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"n_kernels\":1, \"dropout\":0.05, \n",
    "            \"apply_nonstationary_norm\": False,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0                                                        \n",
    "        }\n",
    "    },\n",
    "    'train_params': {\n",
    "        \"smooth\": False,\n",
    "        \"window\": 2,\n",
    "        \"order\": 1,\n",
    "        \"normalize\": False,\n",
    "        \"gap_type\": \"no_overlap\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 100,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "datetime_format = parameters['datetime_format']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:02:24.421149Z",
     "start_time": "2024-04-18T13:02:24.414455Z"
    }
   },
   "id": "c2c8104cca230c33",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "freeing copy memory @ 0x1e077180\n",
      "freeing copy memory @ 0x1e0d8c60\n",
      "freeing copy memory @ 0x1e0d8c60\n",
      "2024-04-18 16:02:37 [INFO]: No given device, using default device: cuda\n",
      "2024-04-18 16:02:37 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-04-18 16:02:37 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,105\n",
      "2024-04-18 16:02:37 [INFO]: Epoch 001 - training loss: 0.5664\n",
      "2024-04-18 16:02:37 [INFO]: Epoch 002 - training loss: 0.3004\n",
      "2024-04-18 16:02:37 [INFO]: Epoch 003 - training loss: 0.2838\n",
      "2024-04-18 16:02:37 [INFO]: Epoch 004 - training loss: 0.2533\n",
      "2024-04-18 16:02:37 [INFO]: Epoch 005 - training loss: 0.2143\n",
      "2024-04-18 16:02:37 [INFO]: Epoch 006 - training loss: 0.2007\n",
      "2024-04-18 16:02:37 [INFO]: Epoch 007 - training loss: 0.2015\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 008 - training loss: 0.1905\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 009 - training loss: 0.1725\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 010 - training loss: 0.1661\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 011 - training loss: 0.1543\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 012 - training loss: 0.1582\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 013 - training loss: 0.1420\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 014 - training loss: 0.1407\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 015 - training loss: 0.1373\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 016 - training loss: 0.1312\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 017 - training loss: 0.1268\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 018 - training loss: 0.1240\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 019 - training loss: 0.1234\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 020 - training loss: 0.1131\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 021 - training loss: 0.1111\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 022 - training loss: 0.1104\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 023 - training loss: 0.1097\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 024 - training loss: 0.1088\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 025 - training loss: 0.1094\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 026 - training loss: 0.1048\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 027 - training loss: 0.1072\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 028 - training loss: 0.0998\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 029 - training loss: 0.1042\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 030 - training loss: 0.1008\n",
      "2024-04-18 16:02:38 [INFO]: Epoch 031 - training loss: 0.1000\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 032 - training loss: 0.0933\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 033 - training loss: 0.1000\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 034 - training loss: 0.0866\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 035 - training loss: 0.0952\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 036 - training loss: 0.0870\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 037 - training loss: 0.0894\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 038 - training loss: 0.0907\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 039 - training loss: 0.0872\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 040 - training loss: 0.0912\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 041 - training loss: 0.0917\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 042 - training loss: 0.0880\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 043 - training loss: 0.0960\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 044 - training loss: 0.0851\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 045 - training loss: 0.0943\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 046 - training loss: 0.0836\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 047 - training loss: 0.0897\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 048 - training loss: 0.0777\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 049 - training loss: 0.0821\n",
      "2024-04-18 16:02:39 [INFO]: Epoch 050 - training loss: 0.0822\n",
      "2024-04-18 16:02:39 [INFO]: Finished training.\n",
      "freeing copy memory @ 0x1e19c280\n",
      "freeing copy memory @ 0x1e0a1bc0\n",
      "freeing copy memory @ 0x1e1651e0\n",
      "2024-04-18 16:02:48 [INFO]: No given device, using default device: cuda\n",
      "2024-04-18 16:02:48 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-04-18 16:02:48 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,105\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 001 - training loss: 0.6310\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 002 - training loss: 0.3041\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 003 - training loss: 0.2916\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 004 - training loss: 0.2707\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 005 - training loss: 0.2117\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 006 - training loss: 0.1977\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 007 - training loss: 0.1980\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 008 - training loss: 0.1836\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 009 - training loss: 0.1774\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 010 - training loss: 0.1643\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 011 - training loss: 0.1553\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 012 - training loss: 0.1409\n",
      "2024-04-18 16:02:48 [INFO]: Epoch 013 - training loss: 0.1468\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 014 - training loss: 0.1353\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 015 - training loss: 0.1276\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 016 - training loss: 0.1319\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 017 - training loss: 0.1260\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 018 - training loss: 0.1254\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 019 - training loss: 0.1188\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 020 - training loss: 0.1179\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 021 - training loss: 0.1082\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 022 - training loss: 0.1115\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 023 - training loss: 0.1008\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 024 - training loss: 0.1108\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 025 - training loss: 0.1008\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 026 - training loss: 0.0987\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 027 - training loss: 0.0967\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 028 - training loss: 0.1006\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 029 - training loss: 0.1011\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 030 - training loss: 0.1012\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 031 - training loss: 0.0942\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 032 - training loss: 0.1019\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 033 - training loss: 0.0926\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 034 - training loss: 0.0904\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 035 - training loss: 0.0947\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 036 - training loss: 0.0861\n",
      "2024-04-18 16:02:49 [INFO]: Epoch 037 - training loss: 0.0872\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 038 - training loss: 0.0940\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 039 - training loss: 0.0910\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 040 - training loss: 0.0912\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 041 - training loss: 0.0876\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 042 - training loss: 0.0842\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 043 - training loss: 0.0935\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 044 - training loss: 0.0888\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 045 - training loss: 0.0837\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 046 - training loss: 0.0826\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 047 - training loss: 0.0899\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 048 - training loss: 0.0848\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 049 - training loss: 0.0840\n",
      "2024-04-18 16:02:50 [INFO]: Epoch 050 - training loss: 0.0813\n",
      "2024-04-18 16:02:50 [INFO]: Finished training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'SoftImpute': {'mae': 0.3219266070325954,\n  'mse': 0.18658043936819818,\n  'rmse': 0.4319495796597077,\n  'r2': 0.7920643014497039,\n  'euclidean_distance': 207.33579823665974},\n 'IterativeSVD': {'mae': 0.4263449182479925,\n  'mse': 0.29746515592730716,\n  'rmse': 0.5454036632873923,\n  'r2': 0.6684881587718029,\n  'euclidean_distance': 261.7937583779483},\n 'SVT': {'mae': 0.1767880887466014,\n  'mse': 0.07526445645365355,\n  'rmse': 0.27434368309413204,\n  'r2': 0.9161210715244661,\n  'euclidean_distance': 131.68496788518337},\n 'TimesNet': {'mae': 0.5354786291961865,\n  'mse': 0.5444717148289551,\n  'rmse': 0.7378832663971687,\n  'r2': 0.39321020602584444,\n  'euclidean_distance': 354.183967870641},\n 'Ensemble_Model': {'mae': 0.13919305939084572,\n  'mse': 0.05264141138764481,\n  'rmse': 0.22943716217658552,\n  'r2': 0.9413334608567272,\n  'euclidean_distance': 110.12983784476106}}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, metrics = tsi.train_ensemble(ground_truth = df_input, \n",
    "                                    algorithms=algorithms,\n",
    "                                    params=params, \n",
    "                                    train_params=train_params,\n",
    "                                    time_column=time_column, \n",
    "                                    datetime_format=datetime_format,\n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    is_multivariate=is_multivariate, \n",
    "                                    areaVStime=areaVStime, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:02:50.814191Z",
     "start_time": "2024-04-18T13:02:30.931409Z"
    }
   },
   "id": "e8463e4866230439",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation with Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d95313e789bb4fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'datetime_format' : '%Y-%m-%d',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": { \n",
    "        \"SoftImpute\": { \"max_rank\": 5 },\n",
    "        \"IterativeSVD\": { \"rank\": 3 }, \n",
    "        \"SVT\": { \"tauScale\": 0.7}, \n",
    "        \"TimesNet\":{ \n",
    "            \"n_layers\": 2, \"top_k\": 3, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"n_kernels\":1, \"dropout\":0.05, \n",
    "            \"apply_nonstationary_norm\": False,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0                                                        \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "datetime_format = parameters['datetime_format']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:03:05.980317Z",
     "start_time": "2024-04-18T13:03:05.974111Z"
    }
   },
   "id": "7316c22364f48853",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "freeing copy memory @ 0x1ddbed20\n",
      "freeing copy memory @ 0x1de20860\n",
      "freeing copy memory @ 0x1de20860\n",
      "2024-04-18 16:04:14 [INFO]: No given device, using default device: cuda\n",
      "2024-04-18 16:04:14 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-04-18 16:04:14 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,105\n",
      "2024-04-18 16:04:14 [INFO]: Epoch 001 - training loss: 1.3716\n",
      "2024-04-18 16:04:14 [INFO]: Epoch 002 - training loss: 0.6072\n",
      "2024-04-18 16:04:14 [INFO]: Epoch 003 - training loss: 0.3212\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 004 - training loss: 0.2769\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 005 - training loss: 0.2696\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 006 - training loss: 0.2499\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 007 - training loss: 0.2221\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 008 - training loss: 0.1962\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 009 - training loss: 0.1941\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 010 - training loss: 0.1895\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 011 - training loss: 0.1755\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 012 - training loss: 0.1643\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 013 - training loss: 0.1603\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 014 - training loss: 0.1519\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 015 - training loss: 0.1454\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 016 - training loss: 0.1461\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 017 - training loss: 0.1358\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 018 - training loss: 0.1310\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 019 - training loss: 0.1350\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 020 - training loss: 0.1286\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 021 - training loss: 0.1272\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 022 - training loss: 0.1126\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 023 - training loss: 0.1175\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 024 - training loss: 0.1134\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 025 - training loss: 0.1151\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 026 - training loss: 0.1121\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 027 - training loss: 0.1043\n",
      "2024-04-18 16:04:15 [INFO]: Epoch 028 - training loss: 0.1115\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 029 - training loss: 0.1087\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 030 - training loss: 0.1031\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 031 - training loss: 0.0958\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 032 - training loss: 0.0999\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 033 - training loss: 0.0994\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 034 - training loss: 0.1013\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 035 - training loss: 0.0965\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 036 - training loss: 0.0967\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 037 - training loss: 0.1013\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 038 - training loss: 0.0923\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 039 - training loss: 0.0959\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 040 - training loss: 0.0876\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 041 - training loss: 0.0873\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 042 - training loss: 0.0839\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 043 - training loss: 0.0903\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 044 - training loss: 0.0931\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 045 - training loss: 0.0878\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 046 - training loss: 0.0883\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 047 - training loss: 0.0852\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 048 - training loss: 0.0989\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 049 - training loss: 0.0894\n",
      "2024-04-18 16:04:16 [INFO]: Epoch 050 - training loss: 0.0904\n",
      "2024-04-18 16:04:16 [INFO]: Finished training.\n"
     ]
    }
   ],
   "source": [
    "model_imputed_df = tsi.run_imputation_ensemble(missing = df_missing, \n",
    "                                               algorithms=algorithms,\n",
    "                                               params=params, \n",
    "                                               model=model,\n",
    "                                               time_column=time_column,\n",
    "                                               datetime_format=datetime_format,\n",
    "                                               header=header, \n",
    "                                               sep=sep, \n",
    "                                               is_multivariate=is_multivariate, \n",
    "                                               areaVStime=areaVStime, \n",
    "                                               preprocessing=preprocessing,\n",
    "                                               index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:04:17.106822Z",
     "start_time": "2024-04-18T13:04:09.510811Z"
    }
   },
   "id": "b2cf91b790d58cd6",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "          time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0   2017-01-02                -0.272915                 -1.347635   \n1   2017-01-03                -0.170053                 -1.189826   \n2   2017-01-04                -0.105915                 -1.325091   \n3   2017-01-05                -0.168553                 -1.336363   \n4   2017-01-06                -1.251226                 -1.313819   \n..         ...                      ...                       ...   \n516 2018-12-25                -1.509627                  0.405175   \n517 2018-12-26                -1.509627                  0.405175   \n518 2018-12-27                -1.310175                  0.433355   \n519 2018-12-28                -1.027002                  0.861694   \n520 2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                   1.753102  ...                          -0.465855   \n4                   1.890973  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-0.272915</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-0.170053</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-0.105915</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-0.168553</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>1.753102</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-1.251226</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>1.890973</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = model_imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "model_imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T13:04:17.122495Z",
     "start_time": "2024-04-18T13:04:17.108132Z"
    }
   },
   "id": "83aeb8e669ea512d",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d74ee07920c98cfb",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
