{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import stelarImputation as tsi\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:47.360100Z",
     "start_time": "2024-08-07T13:31:46.004237Z"
    }
   },
   "id": "7d06e22ee8dfb7bd",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scale Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8315eb0bd1dadb6c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_input = '../datasets/example_input_type1.csv'\n",
    "\n",
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:47.364934Z",
     "start_time": "2024-08-07T13:31:47.361607Z"
    }
   },
   "id": "4666cc372e6ee305",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...             0.129009             0.151553   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1              0.326270             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>0.129009</td>\n      <td>0.151553</td>\n      <td>0.326270</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled, scaler = tsi.dataframe_scaler(df_input=df_input, \n",
    "                                 dimension_column=dimension_column, \n",
    "                                 spatial_x_column=spatial_x_column, \n",
    "                                 spatial_y_column=spatial_y_column, \n",
    "                                 header=header, \n",
    "                                 sep=sep, \n",
    "                                 preprocessing=preprocessing, \n",
    "                                 index=index)\n",
    "\n",
    "df_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:47.412008Z",
     "start_time": "2024-08-07T13:31:47.366658Z"
    }
   },
   "id": "e8dc9c0849a30ec",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gap Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deb58fa26858ef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    'train_params': {\n",
    "        \"gap_type\": \"random\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 10,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5,\n",
    "        \"random_seed\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:47.418569Z",
     "start_time": "2024-08-07T13:31:47.413971Z"
    }
   },
   "id": "88fa2ad58b835c1e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 1636\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...                  NaN                  NaN   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1                   NaN             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missing = tsi.run_gap_generation(ground_truth=df_scaled, \n",
    "                                    train_params=train_params, \n",
    "                                    dimension_column=dimension_column, \n",
    "                                    spatial_x_column=spatial_x_column, \n",
    "                                    spatial_y_column=spatial_y_column, \n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "\n",
    "missing = df_missing.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "df_missing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:47.446156Z",
     "start_time": "2024-08-07T13:31:47.420442Z"
    }
   },
   "id": "658dfb14d4145728",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06dd72064d4e510"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\", \"NonStationary_Transformer\", \"Autoformer\"],\n",
    "    \"params\": { \n",
    "        \"SoftImpute\": { \"max_rank\": 5 },\n",
    "        \"IterativeSVD\": { \"rank\": 3 }, \n",
    "        \"SVT\": { \"tauScale\": 0.7}, \n",
    "        \"TimesNet\":{ \n",
    "            \"n_layers\": 2, \"top_k\": 3, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"n_kernels\":1, \"dropout\":0.05, \n",
    "            \"apply_nonstationary_norm\": False,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        },\n",
    "        \"NonStationary_Transformer\":{ \n",
    "            \"n_layers\": 2, \"n_heads\": 2, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"d_projector_hidden\":[64, 64], \n",
    "            \"dropout\":0.05, \n",
    "            \"n_projector_hidden_layers\": 2,\n",
    "            \"ORT_weight\": 1,\n",
    "            \"MIT_weight\": 1,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        },\n",
    "        \"Autoformer\":{\n",
    "            \"n_layers\": 2, \"n_heads\": 2, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"factor\":3, \n",
    "            \"dropout\":0.05, \n",
    "            \"moving_avg_window_size\": 3,\n",
    "            \"ORT_weight\": 1,\n",
    "            \"MIT_weight\": 1,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:31:47.451760Z",
     "start_time": "2024-08-07T13:31:47.447281Z"
    }
   },
   "id": "eab173986d693e6b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "freeing copy memory @ 0x9bd9700\n",
      "freeing copy memory @ 0x9c9cd20\n",
      "freeing copy memory @ 0x9cfe860\n",
      "2024-08-07 16:31:54 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:31:54 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:31:54 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,217\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 001 - training loss: 0.7551\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 002 - training loss: 0.6179\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 003 - training loss: 0.5283\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 004 - training loss: 0.4306\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 005 - training loss: 0.3898\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 006 - training loss: 0.3365\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 007 - training loss: 0.2961\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 008 - training loss: 0.2964\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 009 - training loss: 0.2770\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 010 - training loss: 0.2742\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 011 - training loss: 0.2664\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 012 - training loss: 0.2665\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 013 - training loss: 0.2443\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 014 - training loss: 0.2568\n",
      "2024-08-07 16:31:55 [INFO]: Epoch 015 - training loss: 0.2415\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 016 - training loss: 0.2320\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 017 - training loss: 0.2380\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 018 - training loss: 0.2296\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 019 - training loss: 0.2208\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 020 - training loss: 0.2277\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 021 - training loss: 0.2146\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 022 - training loss: 0.2059\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 023 - training loss: 0.2198\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 024 - training loss: 0.2140\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 025 - training loss: 0.2146\n",
      "2024-08-07 16:31:56 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:31:56 [INFO]: Finished training. The best model is from epoch#22.\n",
      "2024-08-07 16:31:56 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:31:56 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:31:56 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 83,767\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 001 - training loss: 1.9004\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 002 - training loss: 1.8013\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 003 - training loss: 1.7193\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 004 - training loss: 1.6541\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 005 - training loss: 1.5830\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 006 - training loss: 1.5151\n",
      "2024-08-07 16:31:56 [INFO]: Epoch 007 - training loss: 1.4474\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 008 - training loss: 1.3785\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 009 - training loss: 1.3070\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 010 - training loss: 1.2483\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 011 - training loss: 1.1882\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 012 - training loss: 1.1360\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 013 - training loss: 1.0959\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 014 - training loss: 1.0886\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 015 - training loss: 1.0699\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 016 - training loss: 1.0693\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 017 - training loss: 1.0604\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 018 - training loss: 1.0398\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 019 - training loss: 1.0318\n",
      "2024-08-07 16:31:57 [INFO]: Epoch 020 - training loss: 1.0237\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 021 - training loss: 1.0172\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 022 - training loss: 1.0071\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 023 - training loss: 1.0025\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 024 - training loss: 1.0028\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 025 - training loss: 0.9952\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 026 - training loss: 0.9766\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 027 - training loss: 0.9665\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 028 - training loss: 0.9658\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 029 - training loss: 0.9690\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 030 - training loss: 0.9705\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 031 - training loss: 0.9606\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 032 - training loss: 0.9501\n",
      "2024-08-07 16:31:58 [INFO]: Epoch 033 - training loss: 0.9501\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 034 - training loss: 0.9428\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 035 - training loss: 0.9427\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 036 - training loss: 0.9343\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 037 - training loss: 0.9349\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 038 - training loss: 0.9236\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 039 - training loss: 0.9281\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 040 - training loss: 0.9271\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 041 - training loss: 0.9141\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 042 - training loss: 0.9296\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 043 - training loss: 0.9144\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 044 - training loss: 0.9181\n",
      "2024-08-07 16:31:59 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:31:59 [INFO]: Finished training. The best model is from epoch#41.\n",
      "2024-08-07 16:31:59 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:31:59 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:31:59 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 37,969\n",
      "2024-08-07 16:31:59 [INFO]: Epoch 001 - training loss: 1.8312\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 002 - training loss: 1.7513\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 003 - training loss: 1.6841\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 004 - training loss: 1.6203\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 005 - training loss: 1.5818\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 006 - training loss: 1.5414\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 007 - training loss: 1.5090\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 008 - training loss: 1.4936\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 009 - training loss: 1.4786\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 010 - training loss: 1.4741\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 011 - training loss: 1.4664\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 012 - training loss: 1.4489\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 013 - training loss: 1.4422\n",
      "2024-08-07 16:32:00 [INFO]: Epoch 014 - training loss: 1.4425\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 015 - training loss: 1.4245\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 016 - training loss: 1.4246\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 017 - training loss: 1.4165\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 018 - training loss: 1.4026\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 019 - training loss: 1.4026\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 020 - training loss: 1.3836\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 021 - training loss: 1.3904\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 022 - training loss: 1.3742\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 023 - training loss: 1.3858\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 024 - training loss: 1.3737\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 025 - training loss: 1.3700\n",
      "2024-08-07 16:32:01 [INFO]: Epoch 026 - training loss: 1.3664\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 027 - training loss: 1.3651\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 028 - training loss: 1.3560\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 029 - training loss: 1.3615\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 030 - training loss: 1.3538\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 031 - training loss: 1.3551\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 032 - training loss: 1.3422\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 033 - training loss: 1.3387\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 034 - training loss: 1.3360\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 035 - training loss: 1.3395\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 036 - training loss: 1.3298\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 037 - training loss: 1.3352\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 038 - training loss: 1.3267\n",
      "2024-08-07 16:32:02 [INFO]: Epoch 039 - training loss: 1.3304\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 040 - training loss: 1.3313\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 041 - training loss: 1.3255\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 042 - training loss: 1.3266\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 043 - training loss: 1.3206\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 044 - training loss: 1.3165\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 045 - training loss: 1.3214\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 046 - training loss: 1.3221\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 047 - training loss: 1.3096\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 048 - training loss: 1.3184\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 049 - training loss: 1.3178\n",
      "2024-08-07 16:32:03 [INFO]: Epoch 050 - training loss: 1.3199\n",
      "2024-08-07 16:32:03 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:03 [INFO]: Finished training. The best model is from epoch#47.\n"
     ]
    }
   ],
   "source": [
    "dict_of_imputed_dfs = tsi.run_imputation(missing = df_missing, \n",
    "                                         algorithms=algorithms, \n",
    "                                         params=params, \n",
    "                                         dimension_column=dimension_column,\n",
    "                                         spatial_x_column=spatial_x_column,\n",
    "                                         spatial_y_column=spatial_y_column, \n",
    "                                         header=header, \n",
    "                                         sep=sep, \n",
    "                                         is_multivariate=is_multivariate, \n",
    "                                         areaVStime=areaVStime, \n",
    "                                         preprocessing=preprocessing, \n",
    "                                         index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:03.857128Z",
     "start_time": "2024-08-07T13:31:47.452980Z"
    }
   },
   "id": "bf4f82d7f54c335f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...             0.344556             0.501164   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1              0.360435             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>0.344556</td>\n      <td>0.501164</td>\n      <td>0.360435</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df =dict_of_imputed_dfs['SVT']\n",
    "missing = imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:03.870668Z",
     "start_time": "2024-08-07T13:32:03.858303Z"
    }
   },
   "id": "43857f333cf8cfd9",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ea28603b29c274e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Missing value percentage': 3.2709532949456173,\n 'Mean absolute error': 0.16533025539126808,\n 'Mean square error': 0.05876296953865806,\n 'Root mean square error': 0.24241074550988465,\n 'Mean relative error': 0.209900412713316,\n 'Euclidean Distance': 9.804907861129784,\n 'r2 score': 0.9365066577097514}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_scaled = df_scaled.set_index(df_scaled.columns[0])\n",
    "new_df_scaled = new_df_scaled.iloc[:, 2:].T\n",
    "new_df_scaled.columns.name = ''\n",
    "new_df_scaled.index.name = 'Date'\n",
    "\n",
    "new_df_missing = df_missing.set_index(df_missing.columns[0])\n",
    "new_df_missing = new_df_missing.iloc[:, 2:].T\n",
    "new_df_missing.columns.name = ''\n",
    "new_df_missing.index.name = 'Date'\n",
    "\n",
    "new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "new_imputed_df = new_imputed_df.iloc[:, 2:].T\n",
    "new_imputed_df.columns.name = ''\n",
    "new_imputed_df.index.name = 'Date'\n",
    "\n",
    "tsi.compute_metrics(new_df_scaled, new_df_missing, new_imputed_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:03.884459Z",
     "start_time": "2024-08-07T13:32:03.871630Z"
    }
   },
   "id": "5e564d4917380bc0",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "851d34f0bc65c4b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\", \"NonStationary_Transformer\", \"Autoformer\"],\n",
    "    \"params\": { \n",
    "        \"SoftImpute\": { \"max_rank\": 5 },\n",
    "        \"IterativeSVD\": { \"rank\": 3 }, \n",
    "        \"SVT\": { \"tauScale\": 0.7}, \n",
    "        \"TimesNet\":{ \n",
    "            \"n_layers\": 2, \"top_k\": 3, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"n_kernels\":1, \"dropout\":0.05, \n",
    "            \"apply_nonstationary_norm\": False,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        },\n",
    "        \"NonStationary_Transformer\":{ \n",
    "            \"n_layers\": 2, \"n_heads\": 2, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"d_projector_hidden\":[64, 64], \n",
    "            \"dropout\":0.05, \n",
    "            \"n_projector_hidden_layers\": 2,\n",
    "            \"ORT_weight\": 1,\n",
    "            \"MIT_weight\": 1,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        },\n",
    "        \"Autoformer\":{\n",
    "            \"n_layers\": 2, \"n_heads\": 2, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"factor\":3, \n",
    "            \"dropout\":0.05, \n",
    "            \"moving_avg_window_size\": 3,\n",
    "            \"ORT_weight\": 1,\n",
    "            \"MIT_weight\": 1,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        }\n",
    "    },\n",
    "    'train_params': {\n",
    "        \"smooth\": False,\n",
    "        \"window\": 2,\n",
    "        \"order\": 1,\n",
    "        \"normalize\": False,\n",
    "        \"gap_type\": \"random\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 10,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5,\n",
    "        \"random_seed\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:03.891109Z",
     "start_time": "2024-08-07T13:32:03.885524Z"
    }
   },
   "id": "c2c8104cca230c33",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "freeing copy memory @ 0x2355f4a0\n",
      "freeing copy memory @ 0x235c0fe0\n",
      "freeing copy memory @ 0x235c0fe0\n",
      "2024-08-07 16:32:10 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:10 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:10 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,217\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 001 - training loss: 1.3111\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 002 - training loss: 1.1449\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 003 - training loss: 0.9706\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 004 - training loss: 0.8352\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 005 - training loss: 0.6925\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 006 - training loss: 0.6222\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 007 - training loss: 0.5281\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 008 - training loss: 0.4451\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 009 - training loss: 0.4040\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 010 - training loss: 0.3531\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 011 - training loss: 0.3286\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 012 - training loss: 0.3119\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 013 - training loss: 0.2871\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 014 - training loss: 0.2808\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 015 - training loss: 0.2732\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 016 - training loss: 0.2769\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 017 - training loss: 0.2726\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 018 - training loss: 0.2531\n",
      "2024-08-07 16:32:10 [INFO]: Epoch 019 - training loss: 0.2553\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 020 - training loss: 0.2571\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 021 - training loss: 0.2293\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 022 - training loss: 0.2444\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 023 - training loss: 0.2286\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 024 - training loss: 0.2343\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 025 - training loss: 0.2229\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 026 - training loss: 0.2212\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 027 - training loss: 0.2223\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 028 - training loss: 0.2276\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 029 - training loss: 0.2135\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 030 - training loss: 0.2159\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 031 - training loss: 0.2134\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 032 - training loss: 0.2105\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 033 - training loss: 0.2086\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 034 - training loss: 0.2052\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 035 - training loss: 0.2046\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 036 - training loss: 0.1960\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 037 - training loss: 0.1914\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 038 - training loss: 0.2045\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 039 - training loss: 0.1934\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 040 - training loss: 0.1882\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 041 - training loss: 0.1916\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 042 - training loss: 0.1886\n",
      "2024-08-07 16:32:11 [INFO]: Epoch 043 - training loss: 0.1926\n",
      "2024-08-07 16:32:11 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:11 [INFO]: Finished training. The best model is from epoch#40.\n",
      "2024-08-07 16:32:11 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:11 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:11 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 83,767\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 001 - training loss: 1.9431\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 002 - training loss: 1.7772\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 003 - training loss: 1.6521\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 004 - training loss: 1.5810\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 005 - training loss: 1.5197\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 006 - training loss: 1.4666\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 007 - training loss: 1.4163\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 008 - training loss: 1.3426\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 009 - training loss: 1.3082\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 010 - training loss: 1.2506\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 011 - training loss: 1.2096\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 012 - training loss: 1.1769\n",
      "2024-08-07 16:32:12 [INFO]: Epoch 013 - training loss: 1.1337\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 014 - training loss: 1.1134\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 015 - training loss: 1.0989\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 016 - training loss: 1.0889\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 017 - training loss: 1.0745\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 018 - training loss: 1.0637\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 019 - training loss: 1.0546\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 020 - training loss: 1.0405\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 021 - training loss: 1.0281\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 022 - training loss: 1.0111\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 023 - training loss: 1.0101\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 024 - training loss: 1.0037\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 025 - training loss: 0.9927\n",
      "2024-08-07 16:32:13 [INFO]: Epoch 026 - training loss: 0.9873\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 027 - training loss: 0.9747\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 028 - training loss: 0.9735\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 029 - training loss: 0.9745\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 030 - training loss: 0.9672\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 031 - training loss: 0.9589\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 032 - training loss: 0.9564\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 033 - training loss: 0.9487\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 034 - training loss: 0.9562\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 035 - training loss: 0.9481\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 036 - training loss: 0.9353\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 037 - training loss: 0.9399\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 038 - training loss: 0.9427\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 039 - training loss: 0.9304\n",
      "2024-08-07 16:32:14 [INFO]: Epoch 040 - training loss: 0.9321\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 041 - training loss: 0.9276\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 042 - training loss: 0.9315\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 043 - training loss: 0.9235\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 044 - training loss: 0.9216\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 045 - training loss: 0.9198\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 046 - training loss: 0.9172\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 047 - training loss: 0.9231\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 048 - training loss: 0.9136\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 049 - training loss: 0.9067\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 050 - training loss: 0.8998\n",
      "2024-08-07 16:32:15 [INFO]: Finished training. The best model is from epoch#50.\n",
      "2024-08-07 16:32:15 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:15 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:15 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 37,969\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 001 - training loss: 1.9157\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 002 - training loss: 1.8390\n",
      "2024-08-07 16:32:15 [INFO]: Epoch 003 - training loss: 1.7536\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 004 - training loss: 1.7001\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 005 - training loss: 1.6529\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 006 - training loss: 1.6247\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 007 - training loss: 1.5755\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 008 - training loss: 1.5393\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 009 - training loss: 1.5293\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 010 - training loss: 1.5026\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 011 - training loss: 1.4957\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 012 - training loss: 1.4812\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 013 - training loss: 1.4754\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 014 - training loss: 1.4713\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 015 - training loss: 1.4644\n",
      "2024-08-07 16:32:16 [INFO]: Epoch 016 - training loss: 1.4531\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 017 - training loss: 1.4454\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 018 - training loss: 1.4397\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 019 - training loss: 1.4253\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 020 - training loss: 1.4137\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 021 - training loss: 1.4124\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 022 - training loss: 1.3993\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 023 - training loss: 1.4076\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 024 - training loss: 1.3961\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 025 - training loss: 1.3865\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 026 - training loss: 1.3797\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 027 - training loss: 1.3753\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 028 - training loss: 1.3731\n",
      "2024-08-07 16:32:17 [INFO]: Epoch 029 - training loss: 1.3682\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 030 - training loss: 1.3669\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 031 - training loss: 1.3630\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 032 - training loss: 1.3562\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 033 - training loss: 1.3518\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 034 - training loss: 1.3544\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 035 - training loss: 1.3536\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 036 - training loss: 1.3445\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 037 - training loss: 1.3521\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 038 - training loss: 1.3446\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 039 - training loss: 1.3424\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 040 - training loss: 1.3415\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 041 - training loss: 1.3367\n",
      "2024-08-07 16:32:18 [INFO]: Epoch 042 - training loss: 1.3268\n",
      "2024-08-07 16:32:19 [INFO]: Epoch 043 - training loss: 1.3352\n",
      "2024-08-07 16:32:19 [INFO]: Epoch 044 - training loss: 1.3349\n",
      "2024-08-07 16:32:19 [INFO]: Epoch 045 - training loss: 1.3346\n",
      "2024-08-07 16:32:19 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:19 [INFO]: Finished training. The best model is from epoch#42.\n",
      "freeing copy memory @ 0x23b51fc0\n",
      "freeing copy memory @ 0x2343a3a0\n",
      "freeing copy memory @ 0x2343a3a0\n",
      "2024-08-07 16:32:27 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:27 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:27 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,217\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 001 - training loss: 0.8678\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 002 - training loss: 0.7353\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 003 - training loss: 0.6468\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 004 - training loss: 0.5580\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 005 - training loss: 0.5044\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 006 - training loss: 0.4188\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 007 - training loss: 0.3946\n",
      "2024-08-07 16:32:27 [INFO]: Epoch 008 - training loss: 0.3703\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 009 - training loss: 0.3332\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 010 - training loss: 0.3194\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 011 - training loss: 0.2902\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 012 - training loss: 0.3029\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 013 - training loss: 0.2899\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 014 - training loss: 0.2716\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 015 - training loss: 0.2643\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 016 - training loss: 0.2656\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 017 - training loss: 0.2561\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 018 - training loss: 0.2569\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 019 - training loss: 0.2538\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 020 - training loss: 0.2453\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 021 - training loss: 0.2422\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 022 - training loss: 0.2346\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 023 - training loss: 0.2290\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 024 - training loss: 0.2308\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 025 - training loss: 0.2296\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 026 - training loss: 0.2194\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 027 - training loss: 0.2124\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 028 - training loss: 0.2153\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 029 - training loss: 0.2152\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 030 - training loss: 0.2074\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 031 - training loss: 0.2113\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 032 - training loss: 0.2051\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 033 - training loss: 0.1979\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 034 - training loss: 0.2102\n",
      "2024-08-07 16:32:28 [INFO]: Epoch 035 - training loss: 0.2009\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 036 - training loss: 0.1948\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 037 - training loss: 0.1967\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 038 - training loss: 0.1934\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 039 - training loss: 0.1952\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 040 - training loss: 0.1871\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 041 - training loss: 0.1818\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 042 - training loss: 0.1806\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 043 - training loss: 0.1864\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 044 - training loss: 0.1658\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 045 - training loss: 0.1860\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 046 - training loss: 0.1811\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 047 - training loss: 0.1762\n",
      "2024-08-07 16:32:29 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:29 [INFO]: Finished training. The best model is from epoch#44.\n",
      "2024-08-07 16:32:29 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:29 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:29 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 83,767\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 001 - training loss: 1.5064\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 002 - training loss: 1.4419\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 003 - training loss: 1.3769\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 004 - training loss: 1.3208\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 005 - training loss: 1.2622\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 006 - training loss: 1.2092\n",
      "2024-08-07 16:32:29 [INFO]: Epoch 007 - training loss: 1.1640\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 008 - training loss: 1.1314\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 009 - training loss: 1.1173\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 010 - training loss: 1.1012\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 011 - training loss: 1.0982\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 012 - training loss: 1.0849\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 013 - training loss: 1.0693\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 014 - training loss: 1.0709\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 015 - training loss: 1.0426\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 016 - training loss: 1.0390\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 017 - training loss: 1.0204\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 018 - training loss: 1.0213\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 019 - training loss: 1.0055\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 020 - training loss: 1.0073\n",
      "2024-08-07 16:32:30 [INFO]: Epoch 021 - training loss: 0.9931\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 022 - training loss: 0.9904\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 023 - training loss: 0.9778\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 024 - training loss: 0.9752\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 025 - training loss: 0.9689\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 026 - training loss: 0.9627\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 027 - training loss: 0.9572\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 028 - training loss: 0.9472\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 029 - training loss: 0.9534\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 030 - training loss: 0.9447\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 031 - training loss: 0.9430\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 032 - training loss: 0.9331\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 033 - training loss: 0.9287\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 034 - training loss: 0.9261\n",
      "2024-08-07 16:32:31 [INFO]: Epoch 035 - training loss: 0.9271\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 036 - training loss: 0.9145\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 037 - training loss: 0.9143\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 038 - training loss: 0.9175\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 039 - training loss: 0.9219\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 040 - training loss: 0.9194\n",
      "2024-08-07 16:32:32 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:32 [INFO]: Finished training. The best model is from epoch#37.\n",
      "2024-08-07 16:32:32 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:32 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:32 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 37,969\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 001 - training loss: 1.9063\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 002 - training loss: 1.8401\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 003 - training loss: 1.7773\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 004 - training loss: 1.7195\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 005 - training loss: 1.6699\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 006 - training loss: 1.6129\n",
      "2024-08-07 16:32:32 [INFO]: Epoch 007 - training loss: 1.5667\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 008 - training loss: 1.5226\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 009 - training loss: 1.4945\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 010 - training loss: 1.4569\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 011 - training loss: 1.4488\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 012 - training loss: 1.4411\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 013 - training loss: 1.4399\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 014 - training loss: 1.4292\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 015 - training loss: 1.4214\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 016 - training loss: 1.4096\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 017 - training loss: 1.3984\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 018 - training loss: 1.3904\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 019 - training loss: 1.3864\n",
      "2024-08-07 16:32:33 [INFO]: Epoch 020 - training loss: 1.3943\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 021 - training loss: 1.3794\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 022 - training loss: 1.3734\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 023 - training loss: 1.3683\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 024 - training loss: 1.3653\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 025 - training loss: 1.3737\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 026 - training loss: 1.3615\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 027 - training loss: 1.3632\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 028 - training loss: 1.3587\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 029 - training loss: 1.3487\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 030 - training loss: 1.3649\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 031 - training loss: 1.3503\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 032 - training loss: 1.3439\n",
      "2024-08-07 16:32:34 [INFO]: Epoch 033 - training loss: 1.3560\n",
      "2024-08-07 16:32:35 [INFO]: Epoch 034 - training loss: 1.3499\n",
      "2024-08-07 16:32:35 [INFO]: Epoch 035 - training loss: 1.3447\n",
      "2024-08-07 16:32:35 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:35 [INFO]: Finished training. The best model is from epoch#32.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'SoftImpute': {'mae': 0.30782448665202633,\n  'mse': 0.17197610335894156,\n  'rmse': 0.41470001610675344,\n  'r2': 0.8141799558116444,\n  'euclidean_distance': 678.4492263506487},\n 'IterativeSVD': {'mae': 0.4175407534853295,\n  'mse': 0.2906180158891313,\n  'rmse': 0.5390899886745545,\n  'r2': 0.6859874627945346,\n  'euclidean_distance': 881.9512214715713},\n 'SVT': {'mae': 0.1660114985881344,\n  'mse': 0.05915575816129628,\n  'rmse': 0.24321956780098158,\n  'r2': 0.9360822499124485,\n  'euclidean_distance': 397.9072129224059},\n 'TimesNet': {'mae': 0.6123043350226947,\n  'mse': 0.6721779429354217,\n  'rmse': 0.8198645881701573,\n  'r2': 0.27371226223213585,\n  'euclidean_distance': 1341.2984662463773},\n 'NonStationary_Transformer': {'mae': 0.782855504615161,\n  'mse': 0.9218410690610824,\n  'rmse': 0.9601255485930381,\n  'r2': 0.003951451149883933,\n  'euclidean_distance': 1570.76539749821},\n 'Autoformer': {'mae': 0.749195769034823,\n  'mse': 1.3673523858357235,\n  'rmse': 1.169338439390292,\n  'r2': -0.4774231756300411,\n  'euclidean_distance': 1913.037686842518},\n 'Ensemble_Model': {'mae': 0.15416172550805748,\n  'mse': 0.04994376825238334,\n  'rmse': 0.22348102436758102,\n  'r2': 0.9460357977513837,\n  'euclidean_distance': 365.61495586536245}}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, metrics = tsi.train_ensemble(ground_truth = df_scaled, \n",
    "                                    algorithms=algorithms,\n",
    "                                    params=params, \n",
    "                                    train_params=train_params,\n",
    "                                    dimension_column=dimension_column, \n",
    "                                    spatial_x_column=spatial_x_column, \n",
    "                                    spatial_y_column=spatial_y_column,\n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    is_multivariate=is_multivariate, \n",
    "                                    areaVStime=areaVStime, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:36.638938Z",
     "start_time": "2024-08-07T13:32:03.892182Z"
    }
   },
   "id": "e8463e4866230439",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation with Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d95313e789bb4fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\", \"NonStationary_Transformer\", \"Autoformer\"],\n",
    "    \"params\": { \n",
    "                \"SoftImpute\": { \"max_rank\": 5 },\n",
    "        \"IterativeSVD\": { \"rank\": 3 }, \n",
    "        \"SVT\": { \"tauScale\": 0.7}, \n",
    "        \"TimesNet\":{ \n",
    "            \"n_layers\": 2, \"top_k\": 3, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"n_kernels\":1, \"dropout\":0.05, \n",
    "            \"apply_nonstationary_norm\": False,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        },\n",
    "        \"NonStationary_Transformer\":{ \n",
    "            \"n_layers\": 2, \"n_heads\": 2, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"d_projector_hidden\":[64, 64], \n",
    "            \"dropout\":0.05, \n",
    "            \"n_projector_hidden_layers\": 2,\n",
    "            \"ORT_weight\": 1,\n",
    "            \"MIT_weight\": 1,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        },\n",
    "        \"Autoformer\":{\n",
    "            \"n_layers\": 2, \"n_heads\": 2, \n",
    "            \"d_model\":56, \"d_ffn\":56, \n",
    "            \"factor\":3, \n",
    "            \"dropout\":0.05, \n",
    "            \"moving_avg_window_size\": 3,\n",
    "            \"ORT_weight\": 1,\n",
    "            \"MIT_weight\": 1,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\":50,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 3,\n",
    "            \"lr\": 0.0003\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:36.645034Z",
     "start_time": "2024-08-07T13:32:36.640488Z"
    }
   },
   "id": "7316c22364f48853",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "freeing copy memory @ 0x23a2ce60\n",
      "freeing copy memory @ 0x23a8e9c0\n",
      "freeing copy memory @ 0x23a8e9c0\n",
      "2024-08-07 16:32:42 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:42 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:42 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 13,217\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 001 - training loss: 0.3691\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 002 - training loss: 0.3623\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 003 - training loss: 0.3270\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 004 - training loss: 0.3099\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 005 - training loss: 0.2956\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 006 - training loss: 0.3024\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 007 - training loss: 0.2763\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 008 - training loss: 0.2802\n",
      "2024-08-07 16:32:42 [INFO]: Epoch 009 - training loss: 0.2554\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 010 - training loss: 0.2549\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 011 - training loss: 0.2474\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 012 - training loss: 0.2265\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 013 - training loss: 0.2324\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 014 - training loss: 0.2155\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 015 - training loss: 0.2226\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 016 - training loss: 0.2068\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 017 - training loss: 0.2075\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 018 - training loss: 0.2108\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 019 - training loss: 0.2181\n",
      "2024-08-07 16:32:43 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:43 [INFO]: Finished training. The best model is from epoch#16.\n",
      "2024-08-07 16:32:43 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:43 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:43 [INFO]: NonstationaryTransformer initialized with the given hyperparameters, the number of trainable parameters: 83,767\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 001 - training loss: 2.1036\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 002 - training loss: 1.9974\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 003 - training loss: 1.9186\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 004 - training loss: 1.8449\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 005 - training loss: 1.7843\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 006 - training loss: 1.7158\n",
      "2024-08-07 16:32:43 [INFO]: Epoch 007 - training loss: 1.6553\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 008 - training loss: 1.5846\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 009 - training loss: 1.5208\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 010 - training loss: 1.4531\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 011 - training loss: 1.3865\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 012 - training loss: 1.3245\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 013 - training loss: 1.2626\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 014 - training loss: 1.1956\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 015 - training loss: 1.1452\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 016 - training loss: 1.1258\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 017 - training loss: 1.1085\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 018 - training loss: 1.0885\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 019 - training loss: 1.0643\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 020 - training loss: 1.0537\n",
      "2024-08-07 16:32:44 [INFO]: Epoch 021 - training loss: 1.0394\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 022 - training loss: 1.0351\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 023 - training loss: 1.0186\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 024 - training loss: 1.0002\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 025 - training loss: 1.0014\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 026 - training loss: 0.9869\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 027 - training loss: 0.9818\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 028 - training loss: 0.9773\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 029 - training loss: 0.9739\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 030 - training loss: 0.9669\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 031 - training loss: 0.9652\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 032 - training loss: 0.9568\n",
      "2024-08-07 16:32:45 [INFO]: Epoch 033 - training loss: 0.9619\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 034 - training loss: 0.9604\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 035 - training loss: 0.9475\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 036 - training loss: 0.9424\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 037 - training loss: 0.9344\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 038 - training loss: 0.9356\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 039 - training loss: 0.9379\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 040 - training loss: 0.9301\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 041 - training loss: 0.9278\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 042 - training loss: 0.9216\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 043 - training loss: 0.9215\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 044 - training loss: 0.9269\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 045 - training loss: 0.9206\n",
      "2024-08-07 16:32:46 [INFO]: Epoch 046 - training loss: 0.9247\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 047 - training loss: 0.9179\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 048 - training loss: 0.9142\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 049 - training loss: 0.9164\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 050 - training loss: 0.8997\n",
      "2024-08-07 16:32:47 [INFO]: Finished training. The best model is from epoch#50.\n",
      "2024-08-07 16:32:47 [INFO]: No given device, using default device: cuda\n",
      "2024-08-07 16:32:47 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2024-08-07 16:32:47 [INFO]: Autoformer initialized with the given hyperparameters, the number of trainable parameters: 37,969\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 001 - training loss: 1.6486\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 002 - training loss: 1.5836\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 003 - training loss: 1.5406\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 004 - training loss: 1.5090\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 005 - training loss: 1.4946\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 006 - training loss: 1.4767\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 007 - training loss: 1.4645\n",
      "2024-08-07 16:32:47 [INFO]: Epoch 008 - training loss: 1.4698\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 009 - training loss: 1.4637\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 010 - training loss: 1.4440\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 011 - training loss: 1.4297\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 012 - training loss: 1.4409\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 013 - training loss: 1.4249\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 014 - training loss: 1.4248\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 015 - training loss: 1.4044\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 016 - training loss: 1.4071\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 017 - training loss: 1.4030\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 018 - training loss: 1.3952\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 019 - training loss: 1.3879\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 020 - training loss: 1.3898\n",
      "2024-08-07 16:32:48 [INFO]: Epoch 021 - training loss: 1.3829\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 022 - training loss: 1.3769\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 023 - training loss: 1.3795\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 024 - training loss: 1.3709\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 025 - training loss: 1.3610\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 026 - training loss: 1.3597\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 027 - training loss: 1.3612\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 028 - training loss: 1.3504\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 029 - training loss: 1.3487\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 030 - training loss: 1.3338\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 031 - training loss: 1.3402\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 032 - training loss: 1.3376\n",
      "2024-08-07 16:32:49 [INFO]: Epoch 033 - training loss: 1.3459\n",
      "2024-08-07 16:32:49 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2024-08-07 16:32:49 [INFO]: Finished training. The best model is from epoch#30.\n"
     ]
    }
   ],
   "source": [
    "model_imputed_df = tsi.run_imputation_ensemble(missing = df_missing, \n",
    "                                               algorithms=algorithms,\n",
    "                                               params=params, \n",
    "                                               model=model,\n",
    "                                               dimension_column=dimension_column,\n",
    "                                               spatial_x_column=spatial_x_column,\n",
    "                                               spatial_y_column=spatial_y_column,\n",
    "                                               header=header, \n",
    "                                               sep=sep, \n",
    "                                               is_multivariate=is_multivariate, \n",
    "                                               areaVStime=areaVStime, \n",
    "                                               preprocessing=preprocessing,\n",
    "                                               index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:50.010470Z",
     "start_time": "2024-08-07T13:32:36.646828Z"
    }
   },
   "id": "b2cf91b790d58cd6",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...            -0.143818            -0.358792   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1             -0.136402             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>-0.143818</td>\n      <td>-0.358792</td>\n      <td>-0.136402</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = model_imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "model_imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:50.024150Z",
     "start_time": "2024-08-07T13:32:50.012459Z"
    }
   },
   "id": "83aeb8e669ea512d",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform to the original"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e52770a423e0719"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:50.034352Z",
     "start_time": "2024-08-07T13:32:50.025088Z"
    }
   },
   "id": "15fea205eabacd2",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_imputed_df_orig = tsi.dataframe_inverse_scaler(df_input=model_imputed_df, \n",
    "                                                     scaler=scaler,\n",
    "                                                     dimension_column=dimension_column, \n",
    "                                                     spatial_x_column=spatial_x_column, \n",
    "                                                     spatial_y_column=spatial_y_column, \n",
    "                                                     header=header, \n",
    "                                                     sep=sep, \n",
    "                                                     preprocessing=preprocessing, \n",
    "                                                     index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:50.048249Z",
     "start_time": "2024-08-07T13:32:50.035391Z"
    }
   },
   "id": "1fbc24afc86b6ac6",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...            -0.143818            -0.358792   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1             -0.136402             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>-0.143818</td>\n      <td>-0.358792</td>\n      <td>-0.136402</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imputed_df_orig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:50.063899Z",
     "start_time": "2024-08-07T13:32:50.049284Z"
    }
   },
   "id": "169daf168fb7b85c",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T13:32:50.066671Z",
     "start_time": "2024-08-07T13:32:50.064994Z"
    }
   },
   "id": "530bf8561d812de5",
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
