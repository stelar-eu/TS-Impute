{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import stelarImputation as tsi\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:50.079812Z",
     "start_time": "2025-05-23T12:33:47.258713Z"
    }
   },
   "id": "7d06e22ee8dfb7bd",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scale Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8315eb0bd1dadb6c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_input = '../datasets/example_input_type1.csv'\n",
    "\n",
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:50.084087Z",
     "start_time": "2025-05-23T12:33:50.080947Z"
    }
   },
   "id": "4666cc372e6ee305",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...             0.129009             0.151553   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1              0.326270             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>0.129009</td>\n      <td>0.151553</td>\n      <td>0.326270</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled, scaler = tsi.dataframe_scaler(df_input=df_input, \n",
    "                                 dimension_column=dimension_column, \n",
    "                                 spatial_x_column=spatial_x_column, \n",
    "                                 spatial_y_column=spatial_y_column, \n",
    "                                 header=header, \n",
    "                                 sep=sep, \n",
    "                                 preprocessing=preprocessing, \n",
    "                                 index=index)\n",
    "\n",
    "df_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:50.129713Z",
     "start_time": "2025-05-23T12:33:50.084862Z"
    }
   },
   "id": "e8dc9c0849a30ec",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gap Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deb58fa26858ef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    'train_params': {\n",
    "        \"gap_type\": \"random\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 10,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5,\n",
    "        \"random_seed\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:50.134242Z",
     "start_time": "2025-05-23T12:33:50.130824Z"
    }
   },
   "id": "88fa2ad58b835c1e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 1636\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...                  NaN                  NaN   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1                   NaN             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missing = tsi.run_gap_generation(ground_truth=df_scaled, \n",
    "                                    train_params=train_params, \n",
    "                                    dimension_column=dimension_column, \n",
    "                                    spatial_x_column=spatial_x_column, \n",
    "                                    spatial_y_column=spatial_y_column, \n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "\n",
    "missing = df_missing.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "df_missing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:50.157131Z",
     "start_time": "2025-05-23T12:33:50.135580Z"
    }
   },
   "id": "658dfb14d4145728",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06dd72064d4e510"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": tsi.default_imputation_params()\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:50.160724Z",
     "start_time": "2025-05-23T12:33:50.158023Z"
    }
   },
   "id": "eab173986d693e6b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n",
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:33:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:33:56 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:33:56 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:33:56 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 001 - training loss (MSE): 0.9286\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 002 - training loss (MSE): 0.3693\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 003 - training loss (MSE): 0.3291\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 004 - training loss (MSE): 0.2408\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 005 - training loss (MSE): 0.1910\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 006 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 007 - training loss (MSE): 0.1611\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 008 - training loss (MSE): 0.1604\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 009 - training loss (MSE): 0.1511\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 010 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 011 - training loss (MSE): 0.1371\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 012 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 013 - training loss (MSE): 0.1310\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 014 - training loss (MSE): 0.1254\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 015 - training loss (MSE): 0.1198\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 016 - training loss (MSE): 0.1193\n",
      "2025-05-23 15:33:57 [INFO]: Epoch 017 - training loss (MSE): 0.1192\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 018 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 019 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 020 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 021 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 022 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 023 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 024 - training loss (MSE): 0.1003\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 025 - training loss (MSE): 0.0950\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 026 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 027 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 028 - training loss (MSE): 0.0832\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 029 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 030 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 031 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 032 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 033 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 035 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 036 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 038 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 039 - training loss (MSE): 0.0698\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 040 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 041 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:33:58 [INFO]: Epoch 042 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:33:59 [INFO]: Epoch 043 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:33:59 [INFO]: Epoch 044 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:33:59 [INFO]: Epoch 045 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:33:59 [INFO]: Epoch 046 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:33:59 [INFO]: Epoch 047 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:33:59 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:33:59 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    }
   ],
   "source": [
    "dict_of_imputed_dfs = tsi.run_imputation(missing = df_missing, \n",
    "                                         algorithms=algorithms, \n",
    "                                         params=params, \n",
    "                                         dimension_column=dimension_column,\n",
    "                                         spatial_x_column=spatial_x_column,\n",
    "                                         spatial_y_column=spatial_y_column, \n",
    "                                         header=header, \n",
    "                                         sep=sep, \n",
    "                                         is_multivariate=is_multivariate, \n",
    "                                         areaVStime=areaVStime, \n",
    "                                         preprocessing=preprocessing, \n",
    "                                         index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:59.285033Z",
     "start_time": "2025-05-23T12:33:50.161628Z"
    }
   },
   "id": "bf4f82d7f54c335f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...            -0.131713            -0.172581   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1              0.046228             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>-0.131713</td>\n      <td>-0.172581</td>\n      <td>0.046228</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df =dict_of_imputed_dfs['TimesNet']\n",
    "missing = imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:59.296448Z",
     "start_time": "2025-05-23T12:33:59.285998Z"
    }
   },
   "id": "43857f333cf8cfd9",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ea28603b29c274e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Missing value percentage': 3.2709532949456173,\n 'Mean absolute error': 0.5507465132903318,\n 'Mean square error': 0.5909495169889909,\n 'Root mean square error': 0.7687324092224751,\n 'Mean relative error': 0.699218181006729,\n 'Euclidean Distance': 31.093301686922693,\n 'r2 score': 0.361479512471624}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_scaled = df_scaled.set_index(df_scaled.columns[0])\n",
    "new_df_scaled = new_df_scaled.iloc[:, 2:].T\n",
    "new_df_scaled.columns.name = ''\n",
    "new_df_scaled.index.name = 'Date'\n",
    "\n",
    "new_df_missing = df_missing.set_index(df_missing.columns[0])\n",
    "new_df_missing = new_df_missing.iloc[:, 2:].T\n",
    "new_df_missing.columns.name = ''\n",
    "new_df_missing.index.name = 'Date'\n",
    "\n",
    "new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "new_imputed_df = new_imputed_df.iloc[:, 2:].T\n",
    "new_imputed_df.columns.name = ''\n",
    "new_imputed_df.index.name = 'Date'\n",
    "\n",
    "default_metrics = tsi.compute_metrics(new_df_scaled, new_df_missing, new_imputed_df)\n",
    "default_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:59.304793Z",
     "start_time": "2025-05-23T12:33:59.297303Z"
    }
   },
   "id": "5e564d4917380bc0",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "851d34f0bc65c4b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": tsi.default_imputation_params(),\n",
    "    'train_params': {\n",
    "        \"smooth\": False,\n",
    "        \"window\": 2,\n",
    "        \"order\": 1,\n",
    "        \"normalize\": False,\n",
    "        \"gap_type\": \"random\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 10,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5,\n",
    "        \"random_seed\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:33:59.308866Z",
     "start_time": "2025-05-23T12:33:59.305627Z"
    }
   },
   "id": "c2c8104cca230c33",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:05 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:34:05 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:34:05 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:34:05 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:06 [INFO]: Epoch 001 - training loss (MSE): 0.9286\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 002 - training loss (MSE): 0.3693\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 003 - training loss (MSE): 0.3291\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 004 - training loss (MSE): 0.2408\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 005 - training loss (MSE): 0.1910\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 006 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 007 - training loss (MSE): 0.1611\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 008 - training loss (MSE): 0.1604\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 009 - training loss (MSE): 0.1511\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 010 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 011 - training loss (MSE): 0.1371\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 012 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 013 - training loss (MSE): 0.1310\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 014 - training loss (MSE): 0.1254\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 015 - training loss (MSE): 0.1198\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 016 - training loss (MSE): 0.1193\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 017 - training loss (MSE): 0.1192\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 018 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 019 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 020 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 021 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 022 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 023 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:34:06 [INFO]: Epoch 024 - training loss (MSE): 0.1003\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 025 - training loss (MSE): 0.0950\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 026 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 027 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 028 - training loss (MSE): 0.0832\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 029 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 030 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 031 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 032 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 033 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 035 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 036 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 038 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 039 - training loss (MSE): 0.0698\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 040 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 041 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 042 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 043 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 044 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 045 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 046 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:34:07 [INFO]: Epoch 047 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:34:07 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:34:07 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:14 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:34:14 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:34:14 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:34:14 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:14 [INFO]: Epoch 001 - training loss (MSE): 0.9286\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 002 - training loss (MSE): 0.3693\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 003 - training loss (MSE): 0.3291\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 004 - training loss (MSE): 0.2408\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 005 - training loss (MSE): 0.1910\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 006 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 007 - training loss (MSE): 0.1611\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 008 - training loss (MSE): 0.1604\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 009 - training loss (MSE): 0.1511\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 010 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 011 - training loss (MSE): 0.1371\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 012 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 013 - training loss (MSE): 0.1310\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 014 - training loss (MSE): 0.1254\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 015 - training loss (MSE): 0.1198\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 016 - training loss (MSE): 0.1193\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 017 - training loss (MSE): 0.1192\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 018 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 019 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 020 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 021 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 022 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 023 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 024 - training loss (MSE): 0.1003\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 025 - training loss (MSE): 0.0950\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 026 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:34:15 [INFO]: Epoch 027 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 028 - training loss (MSE): 0.0832\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 029 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 030 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 031 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 032 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 033 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 035 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 036 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 038 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 039 - training loss (MSE): 0.0698\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 040 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 041 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 042 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 043 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 044 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 045 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 046 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:34:16 [INFO]: Epoch 047 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:34:16 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:34:16 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'SoftImpute': {'mae': 0.4162696587156214,\n  'mse': 0.29012308257206726,\n  'rmse': 0.5386307478895604,\n  'r2': 0.6865222378536902,\n  'euclidean_distance': 881.1999035473209},\n 'IterativeSVD': {'mae': 0.4181902965623029,\n  'mse': 0.29139891424249265,\n  'rmse': 0.5398137773737279,\n  'r2': 0.6851437027389565,\n  'euclidean_distance': 883.1353397834192},\n 'SVT': {'mae': 0.1645953861637029,\n  'mse': 0.05819230515515841,\n  'rmse': 0.24123081303009034,\n  'r2': 0.9371232601265262,\n  'euclidean_distance': 394.65361011722797},\n 'TimesNet': {'mae': 0.5507467455813096,\n  'mse': 0.5909496995924984,\n  'rmse': 0.7687325279916926,\n  'r2': 0.36147931516867826,\n  'euclidean_distance': 1257.646415794409},\n 'Ensemble_Model': {'mae': 0.14236293372091646,\n  'mse': 0.039103607400369535,\n  'rmse': 0.1977463208263798,\n  'r2': 0.9577485830115886,\n  'euclidean_distance': 323.5129808719573}}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, metrics = tsi.train_ensemble(ground_truth = df_scaled, \n",
    "                                    algorithms=algorithms,\n",
    "                                    params=params, \n",
    "                                    train_params=train_params,\n",
    "                                    dimension_column=dimension_column, \n",
    "                                    spatial_x_column=spatial_x_column, \n",
    "                                    spatial_y_column=spatial_y_column,\n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    is_multivariate=is_multivariate, \n",
    "                                    areaVStime=areaVStime, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:17.631315Z",
     "start_time": "2025-05-23T12:33:59.309713Z"
    }
   },
   "id": "e8463e4866230439",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation with Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d95313e789bb4fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": tsi.default_imputation_params()\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:17.635666Z",
     "start_time": "2025-05-23T12:34:17.632666Z"
    }
   },
   "id": "7316c22364f48853",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:22 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:34:22 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:34:22 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:34:22 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:23 [INFO]: Epoch 001 - training loss (MSE): 0.9286\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 002 - training loss (MSE): 0.3693\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 003 - training loss (MSE): 0.3291\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 004 - training loss (MSE): 0.2408\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 005 - training loss (MSE): 0.1910\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 006 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 007 - training loss (MSE): 0.1611\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 008 - training loss (MSE): 0.1604\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 009 - training loss (MSE): 0.1511\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 010 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 011 - training loss (MSE): 0.1371\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 012 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 013 - training loss (MSE): 0.1310\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 014 - training loss (MSE): 0.1254\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 015 - training loss (MSE): 0.1198\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 016 - training loss (MSE): 0.1193\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 017 - training loss (MSE): 0.1192\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 018 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 019 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 020 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 021 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:34:23 [INFO]: Epoch 022 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 023 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 024 - training loss (MSE): 0.1003\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 025 - training loss (MSE): 0.0950\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 026 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 027 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 028 - training loss (MSE): 0.0832\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 029 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 030 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 031 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 032 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 033 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 035 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 036 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 038 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 039 - training loss (MSE): 0.0698\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 040 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 041 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 042 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 043 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 044 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 045 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 046 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:34:24 [INFO]: Epoch 047 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:34:24 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:34:24 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    }
   ],
   "source": [
    "model_imputed_df = tsi.run_imputation_ensemble(missing = df_missing, \n",
    "                                               algorithms=algorithms,\n",
    "                                               params=params, \n",
    "                                               model=model,\n",
    "                                               dimension_column=dimension_column,\n",
    "                                               spatial_x_column=spatial_x_column,\n",
    "                                               spatial_y_column=spatial_y_column,\n",
    "                                               header=header, \n",
    "                                               sep=sep, \n",
    "                                               is_multivariate=is_multivariate, \n",
    "                                               areaVStime=areaVStime, \n",
    "                                               preprocessing=preprocessing,\n",
    "                                               index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:25.141215Z",
     "start_time": "2025-05-23T12:34:17.636403Z"
    }
   },
   "id": "b2cf91b790d58cd6",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...            -0.145976            -0.426676   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1             -0.087657             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>-0.145976</td>\n      <td>-0.426676</td>\n      <td>-0.087657</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = model_imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "model_imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:25.157018Z",
     "start_time": "2025-05-23T12:34:25.144469Z"
    }
   },
   "id": "83aeb8e669ea512d",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform to the original"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e52770a423e0719"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:25.165508Z",
     "start_time": "2025-05-23T12:34:25.157783Z"
    }
   },
   "id": "15fea205eabacd2",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_imputed_df_orig = tsi.dataframe_inverse_scaler(df_input=model_imputed_df, \n",
    "                                                     scaler=scaler,\n",
    "                                                     dimension_column=dimension_column, \n",
    "                                                     spatial_x_column=spatial_x_column, \n",
    "                                                     spatial_y_column=spatial_y_column, \n",
    "                                                     header=header, \n",
    "                                                     sep=sep, \n",
    "                                                     preprocessing=preprocessing, \n",
    "                                                     index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:25.180027Z",
     "start_time": "2025-05-23T12:34:25.166316Z"
    }
   },
   "id": "1fbc24afc86b6ac6",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...            -0.145976            -0.426676   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1             -0.087657             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>-0.145976</td>\n      <td>-0.426676</td>\n      <td>-0.087657</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imputed_df_orig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:25.192327Z",
     "start_time": "2025-05-23T12:34:25.180920Z"
    }
   },
   "id": "169daf168fb7b85c",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter tuning - Get params dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f633807d6fa8c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "params = tsi.default_hyperparameter_tuning_imputation_params()\n",
    "algorithm: str = 'TimesNet'\n",
    "dimension_column: str = 'Dimension'\n",
    "spatial_x_column: str = 'Spatial_X'\n",
    "spatial_y_column: str = 'Spatial_Y'\n",
    "is_multivariate: bool = False\n",
    "areaVStime: int = 0\n",
    "preprocessing: bool = True\n",
    "index: bool = False\n",
    "n_trials = 20"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:34:25.200821Z",
     "start_time": "2025-05-23T12:34:25.193356Z"
    }
   },
   "id": "e8b54c6cdeafeac9",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-23 15:34:25,212] A new study created in memory with name: no-name-61470eec-acf6-46a3-8f25-61b90997196f\n",
      "2025-05-23 15:34:25 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:34:25 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:34:25 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:34:25 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 690,305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:25 [INFO]: Epoch 001 - training loss (MSE): 0.7548\n",
      "2025-05-23 15:34:25 [INFO]: Epoch 002 - training loss (MSE): 0.3961\n",
      "2025-05-23 15:34:25 [INFO]: Epoch 003 - training loss (MSE): 0.2421\n",
      "2025-05-23 15:34:25 [INFO]: Epoch 004 - training loss (MSE): 0.1980\n",
      "2025-05-23 15:34:26 [INFO]: Epoch 005 - training loss (MSE): 0.1632\n",
      "2025-05-23 15:34:26 [INFO]: Epoch 006 - training loss (MSE): 0.1427\n",
      "2025-05-23 15:34:26 [INFO]: Epoch 007 - training loss (MSE): 0.1211\n",
      "2025-05-23 15:34:26 [INFO]: Epoch 008 - training loss (MSE): 0.1152\n",
      "2025-05-23 15:34:26 [INFO]: Epoch 009 - training loss (MSE): 0.1034\n",
      "2025-05-23 15:34:26 [INFO]: Epoch 010 - training loss (MSE): 0.0892\n",
      "2025-05-23 15:34:26 [INFO]: Epoch 011 - training loss (MSE): 0.0824\n",
      "2025-05-23 15:34:27 [INFO]: Epoch 012 - training loss (MSE): 0.0796\n",
      "2025-05-23 15:34:27 [INFO]: Epoch 013 - training loss (MSE): 0.0729\n",
      "2025-05-23 15:34:27 [INFO]: Epoch 014 - training loss (MSE): 0.0687\n",
      "2025-05-23 15:34:27 [INFO]: Epoch 015 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:34:27 [INFO]: Epoch 016 - training loss (MSE): 0.0611\n",
      "2025-05-23 15:34:27 [INFO]: Epoch 017 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:34:28 [INFO]: Epoch 018 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:34:28 [INFO]: Epoch 019 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:34:28 [INFO]: Epoch 020 - training loss (MSE): 0.0546\n",
      "2025-05-23 15:34:28 [INFO]: Epoch 021 - training loss (MSE): 0.0533\n",
      "2025-05-23 15:34:28 [INFO]: Epoch 022 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:34:28 [INFO]: Epoch 023 - training loss (MSE): 0.0496\n",
      "2025-05-23 15:34:28 [INFO]: Epoch 024 - training loss (MSE): 0.0469\n",
      "2025-05-23 15:34:29 [INFO]: Epoch 025 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:34:29 [INFO]: Epoch 026 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:34:29 [INFO]: Epoch 027 - training loss (MSE): 0.0451\n",
      "2025-05-23 15:34:29 [INFO]: Epoch 028 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:34:29 [INFO]: Epoch 029 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:34:29 [INFO]: Epoch 030 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:34:30 [INFO]: Epoch 031 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:34:30 [INFO]: Epoch 032 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:34:30 [INFO]: Epoch 033 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:34:30 [INFO]: Epoch 034 - training loss (MSE): 0.0412\n",
      "2025-05-23 15:34:30 [INFO]: Epoch 035 - training loss (MSE): 0.0348\n",
      "2025-05-23 15:34:30 [INFO]: Epoch 036 - training loss (MSE): 0.0386\n",
      "2025-05-23 15:34:30 [INFO]: Epoch 037 - training loss (MSE): 0.0371\n",
      "2025-05-23 15:34:31 [INFO]: Epoch 038 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:34:31 [INFO]: Epoch 039 - training loss (MSE): 0.0393\n",
      "2025-05-23 15:34:31 [INFO]: Epoch 040 - training loss (MSE): 0.0374\n",
      "2025-05-23 15:34:31 [INFO]: Epoch 041 - training loss (MSE): 0.0378\n",
      "2025-05-23 15:34:31 [INFO]: Epoch 042 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:34:31 [INFO]: Epoch 043 - training loss (MSE): 0.0381\n",
      "2025-05-23 15:34:32 [INFO]: Epoch 044 - training loss (MSE): 0.0365\n",
      "2025-05-23 15:34:32 [INFO]: Epoch 045 - training loss (MSE): 0.0347\n",
      "2025-05-23 15:34:32 [INFO]: Epoch 046 - training loss (MSE): 0.0362\n",
      "2025-05-23 15:34:32 [INFO]: Epoch 047 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:34:32 [INFO]: Epoch 048 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:34:32 [INFO]: Epoch 049 - training loss (MSE): 0.0330\n",
      "2025-05-23 15:34:32 [INFO]: Epoch 050 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:34:33 [INFO]: Epoch 051 - training loss (MSE): 0.0340\n",
      "2025-05-23 15:34:33 [INFO]: Epoch 052 - training loss (MSE): 0.0328\n",
      "2025-05-23 15:34:33 [INFO]: Epoch 053 - training loss (MSE): 0.0315\n",
      "2025-05-23 15:34:33 [INFO]: Epoch 054 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:34:33 [INFO]: Epoch 055 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:34:33 [INFO]: Epoch 056 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:34:33 [INFO]: Epoch 057 - training loss (MSE): 0.0344\n",
      "2025-05-23 15:34:34 [INFO]: Epoch 058 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:34:34 [INFO]: Epoch 059 - training loss (MSE): 0.0340\n",
      "2025-05-23 15:34:34 [INFO]: Epoch 060 - training loss (MSE): 0.0320\n",
      "2025-05-23 15:34:34 [INFO]: Epoch 061 - training loss (MSE): 0.0315\n",
      "2025-05-23 15:34:34 [INFO]: Epoch 062 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:34:34 [INFO]: Epoch 063 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:34:35 [INFO]: Epoch 064 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:34:35 [INFO]: Epoch 065 - training loss (MSE): 0.0322\n",
      "2025-05-23 15:34:35 [INFO]: Epoch 066 - training loss (MSE): 0.0312\n",
      "2025-05-23 15:34:35 [INFO]: Epoch 067 - training loss (MSE): 0.0319\n",
      "2025-05-23 15:34:35 [INFO]: Epoch 068 - training loss (MSE): 0.0309\n",
      "2025-05-23 15:34:35 [INFO]: Epoch 069 - training loss (MSE): 0.0343\n",
      "2025-05-23 15:34:35 [INFO]: Epoch 070 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:34:36 [INFO]: Epoch 071 - training loss (MSE): 0.0313\n",
      "2025-05-23 15:34:36 [INFO]: Epoch 072 - training loss (MSE): 0.0308\n",
      "2025-05-23 15:34:36 [INFO]: Epoch 073 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:34:36 [INFO]: Epoch 074 - training loss (MSE): 0.0290\n",
      "2025-05-23 15:34:36 [INFO]: Epoch 075 - training loss (MSE): 0.0286\n",
      "2025-05-23 15:34:36 [INFO]: Epoch 076 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:34:36 [INFO]: Epoch 077 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:34:37 [INFO]: Epoch 078 - training loss (MSE): 0.0295\n",
      "2025-05-23 15:34:37 [INFO]: Epoch 079 - training loss (MSE): 0.0299\n",
      "2025-05-23 15:34:37 [INFO]: Epoch 080 - training loss (MSE): 0.0292\n",
      "2025-05-23 15:34:37 [INFO]: Epoch 081 - training loss (MSE): 0.0289\n",
      "2025-05-23 15:34:37 [INFO]: Epoch 082 - training loss (MSE): 0.0293\n",
      "2025-05-23 15:34:37 [INFO]: Epoch 083 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:34:38 [INFO]: Epoch 084 - training loss (MSE): 0.0275\n",
      "2025-05-23 15:34:38 [INFO]: Epoch 085 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:34:38 [INFO]: Epoch 086 - training loss (MSE): 0.0303\n",
      "2025-05-23 15:34:38 [INFO]: Epoch 087 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:34:38 [INFO]: Epoch 088 - training loss (MSE): 0.0291\n",
      "2025-05-23 15:34:38 [INFO]: Epoch 089 - training loss (MSE): 0.0292\n",
      "2025-05-23 15:34:38 [INFO]: Epoch 090 - training loss (MSE): 0.0278\n",
      "2025-05-23 15:34:39 [INFO]: Epoch 091 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:34:39 [INFO]: Epoch 092 - training loss (MSE): 0.0280\n",
      "2025-05-23 15:34:39 [INFO]: Epoch 093 - training loss (MSE): 0.0298\n",
      "2025-05-23 15:34:39 [INFO]: Epoch 094 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:34:39 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:34:39 [INFO]: Finished training. The best model is from epoch#84.\n",
      "[I 2025-05-23 15:34:39,760] Trial 0 finished with value: 0.25373720576500797 and parameters: {'n_layers': 2, 'd_model': 128, 'd_ffn': 16, 'n_heads': 2, 'top_k': 3, 'n_kernels': 4, 'dropout': 0.4, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.1, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.005431095176981607, 'weight_decay': 0.0001106913243495668}. Best is trial 0 with value: 0.25373720576500797.\n",
      "2025-05-23 15:34:39 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:34:39 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:34:39 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:34:39 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 677,585\n",
      "2025-05-23 15:34:39 [INFO]: Epoch 001 - training loss (MSE): 0.5991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:40 [INFO]: Epoch 002 - training loss (MSE): 0.3987\n",
      "2025-05-23 15:34:40 [INFO]: Epoch 003 - training loss (MSE): 0.3297\n",
      "2025-05-23 15:34:40 [INFO]: Epoch 004 - training loss (MSE): 0.2795\n",
      "2025-05-23 15:34:40 [INFO]: Epoch 005 - training loss (MSE): 0.2680\n",
      "2025-05-23 15:34:40 [INFO]: Epoch 006 - training loss (MSE): 0.2344\n",
      "2025-05-23 15:34:40 [INFO]: Epoch 007 - training loss (MSE): 0.2125\n",
      "2025-05-23 15:34:41 [INFO]: Epoch 008 - training loss (MSE): 0.1994\n",
      "2025-05-23 15:34:41 [INFO]: Epoch 009 - training loss (MSE): 0.1760\n",
      "2025-05-23 15:34:41 [INFO]: Epoch 010 - training loss (MSE): 0.1657\n",
      "2025-05-23 15:34:41 [INFO]: Epoch 011 - training loss (MSE): 0.1500\n",
      "2025-05-23 15:34:41 [INFO]: Epoch 012 - training loss (MSE): 0.1414\n",
      "2025-05-23 15:34:41 [INFO]: Epoch 013 - training loss (MSE): 0.1246\n",
      "2025-05-23 15:34:41 [INFO]: Epoch 014 - training loss (MSE): 0.1218\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 015 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 016 - training loss (MSE): 0.1046\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 017 - training loss (MSE): 0.1018\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 018 - training loss (MSE): 0.0957\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 019 - training loss (MSE): 0.0922\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 020 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 021 - training loss (MSE): 0.0806\n",
      "2025-05-23 15:34:42 [INFO]: Epoch 022 - training loss (MSE): 0.0766\n",
      "2025-05-23 15:34:43 [INFO]: Epoch 023 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:34:43 [INFO]: Epoch 024 - training loss (MSE): 0.0709\n",
      "2025-05-23 15:34:43 [INFO]: Epoch 025 - training loss (MSE): 0.0686\n",
      "2025-05-23 15:34:43 [INFO]: Epoch 026 - training loss (MSE): 0.0658\n",
      "2025-05-23 15:34:43 [INFO]: Epoch 027 - training loss (MSE): 0.0652\n",
      "2025-05-23 15:34:43 [INFO]: Epoch 028 - training loss (MSE): 0.0667\n",
      "2025-05-23 15:34:44 [INFO]: Epoch 029 - training loss (MSE): 0.0657\n",
      "2025-05-23 15:34:44 [INFO]: Epoch 030 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:34:44 [INFO]: Epoch 031 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:34:44 [INFO]: Epoch 032 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:34:44 [INFO]: Epoch 033 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:34:44 [INFO]: Epoch 034 - training loss (MSE): 0.0588\n",
      "2025-05-23 15:34:44 [INFO]: Epoch 035 - training loss (MSE): 0.0532\n",
      "2025-05-23 15:34:45 [INFO]: Epoch 036 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:34:45 [INFO]: Epoch 037 - training loss (MSE): 0.0511\n",
      "2025-05-23 15:34:45 [INFO]: Epoch 038 - training loss (MSE): 0.0492\n",
      "2025-05-23 15:34:45 [INFO]: Epoch 039 - training loss (MSE): 0.0542\n",
      "2025-05-23 15:34:45 [INFO]: Epoch 040 - training loss (MSE): 0.0490\n",
      "2025-05-23 15:34:45 [INFO]: Epoch 041 - training loss (MSE): 0.0536\n",
      "2025-05-23 15:34:45 [INFO]: Epoch 042 - training loss (MSE): 0.0522\n",
      "2025-05-23 15:34:46 [INFO]: Epoch 043 - training loss (MSE): 0.0492\n",
      "2025-05-23 15:34:46 [INFO]: Epoch 044 - training loss (MSE): 0.0509\n",
      "2025-05-23 15:34:46 [INFO]: Epoch 045 - training loss (MSE): 0.0469\n",
      "2025-05-23 15:34:46 [INFO]: Epoch 046 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:34:46 [INFO]: Epoch 047 - training loss (MSE): 0.0491\n",
      "2025-05-23 15:34:46 [INFO]: Epoch 048 - training loss (MSE): 0.0480\n",
      "2025-05-23 15:34:47 [INFO]: Epoch 049 - training loss (MSE): 0.0463\n",
      "2025-05-23 15:34:47 [INFO]: Epoch 050 - training loss (MSE): 0.0479\n",
      "2025-05-23 15:34:47 [INFO]: Epoch 051 - training loss (MSE): 0.0460\n",
      "2025-05-23 15:34:47 [INFO]: Epoch 052 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:34:47 [INFO]: Epoch 053 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:34:47 [INFO]: Epoch 054 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:34:47 [INFO]: Epoch 055 - training loss (MSE): 0.0462\n",
      "2025-05-23 15:34:48 [INFO]: Epoch 056 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:34:48 [INFO]: Epoch 057 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:34:48 [INFO]: Epoch 058 - training loss (MSE): 0.0434\n",
      "2025-05-23 15:34:48 [INFO]: Epoch 059 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:34:48 [INFO]: Epoch 060 - training loss (MSE): 0.0404\n",
      "2025-05-23 15:34:48 [INFO]: Epoch 061 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:34:48 [INFO]: Epoch 062 - training loss (MSE): 0.0401\n",
      "2025-05-23 15:34:49 [INFO]: Epoch 063 - training loss (MSE): 0.0384\n",
      "2025-05-23 15:34:49 [INFO]: Epoch 064 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:34:49 [INFO]: Epoch 065 - training loss (MSE): 0.0402\n",
      "2025-05-23 15:34:49 [INFO]: Epoch 066 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:34:49 [INFO]: Epoch 067 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:34:49 [INFO]: Epoch 068 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:34:49 [INFO]: Epoch 069 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:34:50 [INFO]: Epoch 070 - training loss (MSE): 0.0416\n",
      "2025-05-23 15:34:50 [INFO]: Epoch 071 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:34:50 [INFO]: Epoch 072 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:34:50 [INFO]: Epoch 073 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:34:50 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:34:50 [INFO]: Finished training. The best model is from epoch#63.\n",
      "[I 2025-05-23 15:34:50,692] Trial 1 finished with value: 0.2375157177107524 and parameters: {'n_layers': 1, 'd_model': 128, 'd_ffn': 16, 'n_heads': 2, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0008505207524153307, 'weight_decay': 0.0003414234544289426}. Best is trial 1 with value: 0.2375157177107524.\n",
      "2025-05-23 15:34:50 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:34:50 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:34:50 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:34:50 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 2,029,809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:34:50 [INFO]: Epoch 001 - training loss (MSE): 0.4885\n",
      "2025-05-23 15:34:51 [INFO]: Epoch 002 - training loss (MSE): 0.2531\n",
      "2025-05-23 15:34:51 [INFO]: Epoch 003 - training loss (MSE): 0.1862\n",
      "2025-05-23 15:34:51 [INFO]: Epoch 004 - training loss (MSE): 0.1549\n",
      "2025-05-23 15:34:51 [INFO]: Epoch 005 - training loss (MSE): 0.1274\n",
      "2025-05-23 15:34:51 [INFO]: Epoch 006 - training loss (MSE): 0.1165\n",
      "2025-05-23 15:34:52 [INFO]: Epoch 007 - training loss (MSE): 0.0980\n",
      "2025-05-23 15:34:52 [INFO]: Epoch 008 - training loss (MSE): 0.0881\n",
      "2025-05-23 15:34:52 [INFO]: Epoch 009 - training loss (MSE): 0.0855\n",
      "2025-05-23 15:34:52 [INFO]: Epoch 010 - training loss (MSE): 0.0835\n",
      "2025-05-23 15:34:52 [INFO]: Epoch 011 - training loss (MSE): 0.0828\n",
      "2025-05-23 15:34:53 [INFO]: Epoch 012 - training loss (MSE): 0.0726\n",
      "2025-05-23 15:34:53 [INFO]: Epoch 013 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:34:53 [INFO]: Epoch 014 - training loss (MSE): 0.0693\n",
      "2025-05-23 15:34:53 [INFO]: Epoch 015 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:34:53 [INFO]: Epoch 016 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:34:54 [INFO]: Epoch 017 - training loss (MSE): 0.0647\n",
      "2025-05-23 15:34:54 [INFO]: Epoch 018 - training loss (MSE): 0.0621\n",
      "2025-05-23 15:34:54 [INFO]: Epoch 019 - training loss (MSE): 0.0608\n",
      "2025-05-23 15:34:54 [INFO]: Epoch 020 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:34:54 [INFO]: Epoch 021 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:34:54 [INFO]: Epoch 022 - training loss (MSE): 0.0568\n",
      "2025-05-23 15:34:55 [INFO]: Epoch 023 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:34:55 [INFO]: Epoch 024 - training loss (MSE): 0.0538\n",
      "2025-05-23 15:34:55 [INFO]: Epoch 025 - training loss (MSE): 0.0511\n",
      "2025-05-23 15:34:55 [INFO]: Epoch 026 - training loss (MSE): 0.0541\n",
      "2025-05-23 15:34:55 [INFO]: Epoch 027 - training loss (MSE): 0.0527\n",
      "2025-05-23 15:34:56 [INFO]: Epoch 028 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:34:56 [INFO]: Epoch 029 - training loss (MSE): 0.0504\n",
      "2025-05-23 15:34:56 [INFO]: Epoch 030 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:34:56 [INFO]: Epoch 031 - training loss (MSE): 0.0468\n",
      "2025-05-23 15:34:56 [INFO]: Epoch 032 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:34:56 [INFO]: Epoch 033 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:34:57 [INFO]: Epoch 034 - training loss (MSE): 0.0455\n",
      "2025-05-23 15:34:57 [INFO]: Epoch 035 - training loss (MSE): 0.0506\n",
      "2025-05-23 15:34:57 [INFO]: Epoch 036 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:34:57 [INFO]: Epoch 037 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:34:57 [INFO]: Epoch 038 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:34:58 [INFO]: Epoch 039 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:34:58 [INFO]: Epoch 040 - training loss (MSE): 0.0446\n",
      "2025-05-23 15:34:58 [INFO]: Epoch 041 - training loss (MSE): 0.0446\n",
      "2025-05-23 15:34:58 [INFO]: Epoch 042 - training loss (MSE): 0.0494\n",
      "2025-05-23 15:34:58 [INFO]: Epoch 043 - training loss (MSE): 0.0421\n",
      "2025-05-23 15:34:59 [INFO]: Epoch 044 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:34:59 [INFO]: Epoch 045 - training loss (MSE): 0.0432\n",
      "2025-05-23 15:34:59 [INFO]: Epoch 046 - training loss (MSE): 0.0450\n",
      "2025-05-23 15:34:59 [INFO]: Epoch 047 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:34:59 [INFO]: Epoch 048 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:34:59 [INFO]: Epoch 049 - training loss (MSE): 0.0412\n",
      "2025-05-23 15:35:00 [INFO]: Epoch 050 - training loss (MSE): 0.0409\n",
      "2025-05-23 15:35:00 [INFO]: Epoch 051 - training loss (MSE): 0.0421\n",
      "2025-05-23 15:35:00 [INFO]: Epoch 052 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:35:00 [INFO]: Epoch 053 - training loss (MSE): 0.0396\n",
      "2025-05-23 15:35:00 [INFO]: Epoch 054 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:35:01 [INFO]: Epoch 055 - training loss (MSE): 0.0429\n",
      "2025-05-23 15:35:01 [INFO]: Epoch 056 - training loss (MSE): 0.0393\n",
      "2025-05-23 15:35:01 [INFO]: Epoch 057 - training loss (MSE): 0.0395\n",
      "2025-05-23 15:35:01 [INFO]: Epoch 058 - training loss (MSE): 0.0418\n",
      "2025-05-23 15:35:01 [INFO]: Epoch 059 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:35:02 [INFO]: Epoch 060 - training loss (MSE): 0.0407\n",
      "2025-05-23 15:35:02 [INFO]: Epoch 061 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:35:02 [INFO]: Epoch 062 - training loss (MSE): 0.0384\n",
      "2025-05-23 15:35:02 [INFO]: Epoch 063 - training loss (MSE): 0.0409\n",
      "2025-05-23 15:35:02 [INFO]: Epoch 064 - training loss (MSE): 0.0364\n",
      "2025-05-23 15:35:03 [INFO]: Epoch 065 - training loss (MSE): 0.0402\n",
      "2025-05-23 15:35:03 [INFO]: Epoch 066 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:35:03 [INFO]: Epoch 067 - training loss (MSE): 0.0382\n",
      "2025-05-23 15:35:03 [INFO]: Epoch 068 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:35:03 [INFO]: Epoch 069 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:35:03 [INFO]: Epoch 070 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:35:04 [INFO]: Epoch 071 - training loss (MSE): 0.0370\n",
      "2025-05-23 15:35:04 [INFO]: Epoch 072 - training loss (MSE): 0.0390\n",
      "2025-05-23 15:35:04 [INFO]: Epoch 073 - training loss (MSE): 0.0381\n",
      "2025-05-23 15:35:04 [INFO]: Epoch 074 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:35:04 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:35:04 [INFO]: Finished training. The best model is from epoch#64.\n",
      "[I 2025-05-23 15:35:04,887] Trial 2 finished with value: 0.24379356992735043 and parameters: {'n_layers': 3, 'd_model': 16, 'd_ffn': 128, 'n_heads': 1, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.003964743616689867, 'weight_decay': 0.0004659747070765304}. Best is trial 1 with value: 0.2375157177107524.\n",
      "2025-05-23 15:35:04 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:04 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:04 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:04 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,377,793\n",
      "2025-05-23 15:35:05 [INFO]: Epoch 001 - training loss (MSE): 1.7119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:05 [INFO]: Epoch 002 - training loss (MSE): 1.4442\n",
      "2025-05-23 15:35:05 [INFO]: Epoch 003 - training loss (MSE): 1.2114\n",
      "2025-05-23 15:35:05 [INFO]: Epoch 004 - training loss (MSE): 1.0332\n",
      "2025-05-23 15:35:05 [INFO]: Epoch 005 - training loss (MSE): 0.8529\n",
      "2025-05-23 15:35:05 [INFO]: Epoch 006 - training loss (MSE): 0.6720\n",
      "2025-05-23 15:35:05 [INFO]: Epoch 007 - training loss (MSE): 0.5716\n",
      "2025-05-23 15:35:06 [INFO]: Epoch 008 - training loss (MSE): 0.4794\n",
      "2025-05-23 15:35:06 [INFO]: Epoch 009 - training loss (MSE): 0.4137\n",
      "2025-05-23 15:35:06 [INFO]: Epoch 010 - training loss (MSE): 0.3786\n",
      "2025-05-23 15:35:06 [INFO]: Epoch 011 - training loss (MSE): 0.3586\n",
      "2025-05-23 15:35:06 [INFO]: Epoch 012 - training loss (MSE): 0.3542\n",
      "2025-05-23 15:35:06 [INFO]: Epoch 013 - training loss (MSE): 0.3496\n",
      "2025-05-23 15:35:06 [INFO]: Epoch 014 - training loss (MSE): 0.3382\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 015 - training loss (MSE): 0.3284\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 016 - training loss (MSE): 0.3063\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 017 - training loss (MSE): 0.2986\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 018 - training loss (MSE): 0.2904\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 019 - training loss (MSE): 0.2838\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 020 - training loss (MSE): 0.2761\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 021 - training loss (MSE): 0.2787\n",
      "2025-05-23 15:35:07 [INFO]: Epoch 022 - training loss (MSE): 0.2708\n",
      "2025-05-23 15:35:08 [INFO]: Epoch 023 - training loss (MSE): 0.2620\n",
      "2025-05-23 15:35:08 [INFO]: Epoch 024 - training loss (MSE): 0.2665\n",
      "2025-05-23 15:35:08 [INFO]: Epoch 025 - training loss (MSE): 0.2552\n",
      "2025-05-23 15:35:08 [INFO]: Epoch 026 - training loss (MSE): 0.2531\n",
      "2025-05-23 15:35:08 [INFO]: Epoch 027 - training loss (MSE): 0.2403\n",
      "2025-05-23 15:35:08 [INFO]: Epoch 028 - training loss (MSE): 0.2377\n",
      "2025-05-23 15:35:08 [INFO]: Epoch 029 - training loss (MSE): 0.2421\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 030 - training loss (MSE): 0.2289\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 031 - training loss (MSE): 0.2258\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 032 - training loss (MSE): 0.2287\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 033 - training loss (MSE): 0.2163\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 034 - training loss (MSE): 0.2112\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 035 - training loss (MSE): 0.2078\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 036 - training loss (MSE): 0.2100\n",
      "2025-05-23 15:35:09 [INFO]: Epoch 037 - training loss (MSE): 0.1985\n",
      "2025-05-23 15:35:10 [INFO]: Epoch 038 - training loss (MSE): 0.1948\n",
      "2025-05-23 15:35:10 [INFO]: Epoch 039 - training loss (MSE): 0.1958\n",
      "2025-05-23 15:35:10 [INFO]: Epoch 040 - training loss (MSE): 0.1927\n",
      "2025-05-23 15:35:10 [INFO]: Epoch 041 - training loss (MSE): 0.1872\n",
      "2025-05-23 15:35:10 [INFO]: Epoch 042 - training loss (MSE): 0.1841\n",
      "2025-05-23 15:35:10 [INFO]: Epoch 043 - training loss (MSE): 0.1840\n",
      "2025-05-23 15:35:10 [INFO]: Epoch 044 - training loss (MSE): 0.1756\n",
      "2025-05-23 15:35:11 [INFO]: Epoch 045 - training loss (MSE): 0.1753\n",
      "2025-05-23 15:35:11 [INFO]: Epoch 046 - training loss (MSE): 0.1679\n",
      "2025-05-23 15:35:11 [INFO]: Epoch 047 - training loss (MSE): 0.1668\n",
      "2025-05-23 15:35:11 [INFO]: Epoch 048 - training loss (MSE): 0.1627\n",
      "2025-05-23 15:35:11 [INFO]: Epoch 049 - training loss (MSE): 0.1601\n",
      "2025-05-23 15:35:11 [INFO]: Epoch 050 - training loss (MSE): 0.1631\n",
      "2025-05-23 15:35:11 [INFO]: Epoch 051 - training loss (MSE): 0.1480\n",
      "2025-05-23 15:35:12 [INFO]: Epoch 052 - training loss (MSE): 0.1479\n",
      "2025-05-23 15:35:12 [INFO]: Epoch 053 - training loss (MSE): 0.1482\n",
      "2025-05-23 15:35:12 [INFO]: Epoch 054 - training loss (MSE): 0.1459\n",
      "2025-05-23 15:35:12 [INFO]: Epoch 055 - training loss (MSE): 0.1437\n",
      "2025-05-23 15:35:12 [INFO]: Epoch 056 - training loss (MSE): 0.1391\n",
      "2025-05-23 15:35:12 [INFO]: Epoch 057 - training loss (MSE): 0.1344\n",
      "2025-05-23 15:35:12 [INFO]: Epoch 058 - training loss (MSE): 0.1295\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 059 - training loss (MSE): 0.1289\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 060 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 061 - training loss (MSE): 0.1282\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 062 - training loss (MSE): 0.1271\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 063 - training loss (MSE): 0.1251\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 064 - training loss (MSE): 0.1201\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 065 - training loss (MSE): 0.1144\n",
      "2025-05-23 15:35:13 [INFO]: Epoch 066 - training loss (MSE): 0.1110\n",
      "2025-05-23 15:35:14 [INFO]: Epoch 067 - training loss (MSE): 0.1163\n",
      "2025-05-23 15:35:14 [INFO]: Epoch 068 - training loss (MSE): 0.1132\n",
      "2025-05-23 15:35:14 [INFO]: Epoch 069 - training loss (MSE): 0.1089\n",
      "2025-05-23 15:35:14 [INFO]: Epoch 070 - training loss (MSE): 0.1096\n",
      "2025-05-23 15:35:14 [INFO]: Epoch 071 - training loss (MSE): 0.1056\n",
      "2025-05-23 15:35:14 [INFO]: Epoch 072 - training loss (MSE): 0.1032\n",
      "2025-05-23 15:35:14 [INFO]: Epoch 073 - training loss (MSE): 0.1078\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 074 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 075 - training loss (MSE): 0.1054\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 076 - training loss (MSE): 0.1028\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 077 - training loss (MSE): 0.1020\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 078 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 079 - training loss (MSE): 0.0980\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 080 - training loss (MSE): 0.0962\n",
      "2025-05-23 15:35:15 [INFO]: Epoch 081 - training loss (MSE): 0.0945\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 082 - training loss (MSE): 0.0927\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 083 - training loss (MSE): 0.0958\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 084 - training loss (MSE): 0.0905\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 085 - training loss (MSE): 0.0891\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 086 - training loss (MSE): 0.0896\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 087 - training loss (MSE): 0.0929\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 088 - training loss (MSE): 0.0861\n",
      "2025-05-23 15:35:16 [INFO]: Epoch 089 - training loss (MSE): 0.0924\n",
      "2025-05-23 15:35:17 [INFO]: Epoch 090 - training loss (MSE): 0.0886\n",
      "2025-05-23 15:35:17 [INFO]: Epoch 091 - training loss (MSE): 0.0842\n",
      "2025-05-23 15:35:17 [INFO]: Epoch 092 - training loss (MSE): 0.0849\n",
      "2025-05-23 15:35:17 [INFO]: Epoch 093 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:35:17 [INFO]: Epoch 094 - training loss (MSE): 0.0824\n",
      "2025-05-23 15:35:17 [INFO]: Epoch 095 - training loss (MSE): 0.0798\n",
      "2025-05-23 15:35:17 [INFO]: Epoch 096 - training loss (MSE): 0.0818\n",
      "2025-05-23 15:35:18 [INFO]: Epoch 097 - training loss (MSE): 0.0790\n",
      "2025-05-23 15:35:18 [INFO]: Epoch 098 - training loss (MSE): 0.0811\n",
      "2025-05-23 15:35:18 [INFO]: Epoch 099 - training loss (MSE): 0.0818\n",
      "2025-05-23 15:35:18 [INFO]: Epoch 100 - training loss (MSE): 0.0795\n",
      "2025-05-23 15:35:18 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:35:18,576] Trial 3 finished with value: 0.3153886965406908 and parameters: {'n_layers': 2, 'd_model': 64, 'd_ffn': 64, 'n_heads': 1, 'top_k': 3, 'n_kernels': 4, 'dropout': 0.4, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.00014632223035577693, 'weight_decay': 0.000331603591492567}. Best is trial 1 with value: 0.2375157177107524.\n",
      "2025-05-23 15:35:18 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:18 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:18 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:18 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 431,201\n",
      "2025-05-23 15:35:18 [INFO]: Epoch 001 - training loss (MSE): 1.6065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:18 [INFO]: Epoch 002 - training loss (MSE): 0.5707\n",
      "2025-05-23 15:35:18 [INFO]: Epoch 003 - training loss (MSE): 0.4591\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 004 - training loss (MSE): 0.4421\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 005 - training loss (MSE): 0.3163\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 006 - training loss (MSE): 0.2502\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 007 - training loss (MSE): 0.2623\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 008 - training loss (MSE): 0.2432\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 009 - training loss (MSE): 0.2096\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 010 - training loss (MSE): 0.1904\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 011 - training loss (MSE): 0.1840\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 012 - training loss (MSE): 0.1746\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 013 - training loss (MSE): 0.1558\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 014 - training loss (MSE): 0.1517\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 015 - training loss (MSE): 0.1430\n",
      "2025-05-23 15:35:19 [INFO]: Epoch 016 - training loss (MSE): 0.1392\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 017 - training loss (MSE): 0.1289\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 018 - training loss (MSE): 0.1203\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 019 - training loss (MSE): 0.1171\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 020 - training loss (MSE): 0.1126\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 021 - training loss (MSE): 0.1094\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 022 - training loss (MSE): 0.1074\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 023 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 024 - training loss (MSE): 0.1030\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 025 - training loss (MSE): 0.0954\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 026 - training loss (MSE): 0.1005\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 027 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:35:20 [INFO]: Epoch 028 - training loss (MSE): 0.0957\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 029 - training loss (MSE): 0.0990\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 030 - training loss (MSE): 0.0928\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 031 - training loss (MSE): 0.1012\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 032 - training loss (MSE): 0.0945\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 033 - training loss (MSE): 0.0923\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 034 - training loss (MSE): 0.0889\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 035 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 036 - training loss (MSE): 0.0919\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 037 - training loss (MSE): 0.0839\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 038 - training loss (MSE): 0.0882\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 039 - training loss (MSE): 0.0844\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 040 - training loss (MSE): 0.0874\n",
      "2025-05-23 15:35:21 [INFO]: Epoch 041 - training loss (MSE): 0.0827\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 042 - training loss (MSE): 0.0827\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 043 - training loss (MSE): 0.0834\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 044 - training loss (MSE): 0.0842\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 045 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 046 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 047 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 048 - training loss (MSE): 0.0801\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 049 - training loss (MSE): 0.0761\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 050 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 051 - training loss (MSE): 0.0713\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 052 - training loss (MSE): 0.0707\n",
      "2025-05-23 15:35:22 [INFO]: Epoch 053 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 054 - training loss (MSE): 0.0743\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 055 - training loss (MSE): 0.0754\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 056 - training loss (MSE): 0.0692\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 057 - training loss (MSE): 0.0696\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 058 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 059 - training loss (MSE): 0.0694\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 060 - training loss (MSE): 0.0673\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 061 - training loss (MSE): 0.0711\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 062 - training loss (MSE): 0.0666\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 063 - training loss (MSE): 0.0670\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 064 - training loss (MSE): 0.0657\n",
      "2025-05-23 15:35:23 [INFO]: Epoch 065 - training loss (MSE): 0.0661\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 066 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 067 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 068 - training loss (MSE): 0.0664\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 069 - training loss (MSE): 0.0658\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 070 - training loss (MSE): 0.0638\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 071 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 072 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 073 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 074 - training loss (MSE): 0.0603\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 075 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 076 - training loss (MSE): 0.0611\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 077 - training loss (MSE): 0.0612\n",
      "2025-05-23 15:35:24 [INFO]: Epoch 078 - training loss (MSE): 0.0579\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 079 - training loss (MSE): 0.0587\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 080 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 081 - training loss (MSE): 0.0570\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 082 - training loss (MSE): 0.0603\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 083 - training loss (MSE): 0.0590\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 084 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 085 - training loss (MSE): 0.0565\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 086 - training loss (MSE): 0.0553\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 087 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 088 - training loss (MSE): 0.0528\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 089 - training loss (MSE): 0.0586\n",
      "2025-05-23 15:35:25 [INFO]: Epoch 090 - training loss (MSE): 0.0582\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 091 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 092 - training loss (MSE): 0.0561\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 093 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 094 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 095 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 096 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 097 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 098 - training loss (MSE): 0.0534\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 099 - training loss (MSE): 0.0515\n",
      "2025-05-23 15:35:26 [INFO]: Epoch 100 - training loss (MSE): 0.0539\n",
      "2025-05-23 15:35:26 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:35:26,961] Trial 4 finished with value: 0.2517844566290508 and parameters: {'n_layers': 3, 'd_model': 32, 'd_ffn': 64, 'n_heads': 1, 'top_k': 3, 'n_kernels': 3, 'dropout': 0.5, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0017597216454587137, 'weight_decay': 0.0009307823237220275}. Best is trial 1 with value: 0.2375157177107524.\n",
      "2025-05-23 15:35:26 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:26 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:27 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:27 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 258,881\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 001 - training loss (MSE): 0.7176\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 002 - training loss (MSE): 0.4686\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 003 - training loss (MSE): 0.3322\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 004 - training loss (MSE): 0.2884\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 005 - training loss (MSE): 0.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:27 [INFO]: Epoch 006 - training loss (MSE): 0.2648\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 007 - training loss (MSE): 0.2491\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 008 - training loss (MSE): 0.2238\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 009 - training loss (MSE): 0.2134\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 010 - training loss (MSE): 0.1875\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 011 - training loss (MSE): 0.1773\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 012 - training loss (MSE): 0.1709\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 013 - training loss (MSE): 0.1652\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 014 - training loss (MSE): 0.1494\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 015 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 016 - training loss (MSE): 0.1418\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 017 - training loss (MSE): 0.1335\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 018 - training loss (MSE): 0.1316\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 019 - training loss (MSE): 0.1188\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 020 - training loss (MSE): 0.1122\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 021 - training loss (MSE): 0.1137\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 022 - training loss (MSE): 0.1035\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 023 - training loss (MSE): 0.0988\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 024 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 025 - training loss (MSE): 0.0996\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 026 - training loss (MSE): 0.0942\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 027 - training loss (MSE): 0.0890\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 028 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 029 - training loss (MSE): 0.0883\n",
      "2025-05-23 15:35:27 [INFO]: Epoch 030 - training loss (MSE): 0.0828\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 031 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 032 - training loss (MSE): 0.0861\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 033 - training loss (MSE): 0.0776\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 034 - training loss (MSE): 0.0763\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 035 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 036 - training loss (MSE): 0.0789\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 037 - training loss (MSE): 0.0764\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 038 - training loss (MSE): 0.0749\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 039 - training loss (MSE): 0.0726\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 040 - training loss (MSE): 0.0686\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 041 - training loss (MSE): 0.0697\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 042 - training loss (MSE): 0.0695\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 043 - training loss (MSE): 0.0695\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 044 - training loss (MSE): 0.0669\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 045 - training loss (MSE): 0.0672\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 046 - training loss (MSE): 0.0651\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 047 - training loss (MSE): 0.0638\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 048 - training loss (MSE): 0.0649\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 049 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 050 - training loss (MSE): 0.0599\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 051 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 052 - training loss (MSE): 0.0611\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 053 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 054 - training loss (MSE): 0.0602\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 055 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 056 - training loss (MSE): 0.0584\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 057 - training loss (MSE): 0.0537\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 058 - training loss (MSE): 0.0565\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 059 - training loss (MSE): 0.0586\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 060 - training loss (MSE): 0.0566\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 061 - training loss (MSE): 0.0561\n",
      "2025-05-23 15:35:28 [INFO]: Epoch 062 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 063 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 064 - training loss (MSE): 0.0559\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 065 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 066 - training loss (MSE): 0.0522\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 067 - training loss (MSE): 0.0548\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 068 - training loss (MSE): 0.0536\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 069 - training loss (MSE): 0.0525\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 070 - training loss (MSE): 0.0540\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 071 - training loss (MSE): 0.0526\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 072 - training loss (MSE): 0.0508\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 073 - training loss (MSE): 0.0536\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 074 - training loss (MSE): 0.0522\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 075 - training loss (MSE): 0.0486\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 076 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 077 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 078 - training loss (MSE): 0.0477\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 079 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 080 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 081 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 082 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 083 - training loss (MSE): 0.0456\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 084 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 085 - training loss (MSE): 0.0480\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 086 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 087 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 088 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 089 - training loss (MSE): 0.0453\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 090 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 091 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 092 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 093 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:35:29 [INFO]: Epoch 094 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 095 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 096 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 097 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 098 - training loss (MSE): 0.0445\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 099 - training loss (MSE): 0.0451\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 100 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:35:30 [INFO]: Finished training. The best model is from epoch#95.\n",
      "[I 2025-05-23 15:35:30,295] Trial 5 finished with value: 0.25813495810722314 and parameters: {'n_layers': 3, 'd_model': 32, 'd_ffn': 16, 'n_heads': 2, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0010035341501560358, 'weight_decay': 0.000815755185102926}. Best is trial 1 with value: 0.2375157177107524.\n",
      "2025-05-23 15:35:30 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:30 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:30 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:30 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,033,857\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 001 - training loss (MSE): 1.7772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:30 [INFO]: Epoch 002 - training loss (MSE): 1.6666\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 003 - training loss (MSE): 1.5384\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 004 - training loss (MSE): 1.4215\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 005 - training loss (MSE): 1.3596\n",
      "2025-05-23 15:35:30 [INFO]: Epoch 006 - training loss (MSE): 1.2266\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 007 - training loss (MSE): 1.1303\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 008 - training loss (MSE): 1.0428\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 009 - training loss (MSE): 0.9492\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 010 - training loss (MSE): 0.8988\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 011 - training loss (MSE): 0.8174\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 012 - training loss (MSE): 0.7346\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 013 - training loss (MSE): 0.6295\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 014 - training loss (MSE): 0.5925\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 015 - training loss (MSE): 0.5477\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 016 - training loss (MSE): 0.4883\n",
      "2025-05-23 15:35:31 [INFO]: Epoch 017 - training loss (MSE): 0.4422\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 018 - training loss (MSE): 0.3871\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 019 - training loss (MSE): 0.3739\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 020 - training loss (MSE): 0.3384\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 021 - training loss (MSE): 0.3255\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 022 - training loss (MSE): 0.3052\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 023 - training loss (MSE): 0.3034\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 024 - training loss (MSE): 0.2889\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 025 - training loss (MSE): 0.2844\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 026 - training loss (MSE): 0.2736\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 027 - training loss (MSE): 0.2713\n",
      "2025-05-23 15:35:32 [INFO]: Epoch 028 - training loss (MSE): 0.2695\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 029 - training loss (MSE): 0.2653\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 030 - training loss (MSE): 0.2592\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 031 - training loss (MSE): 0.2544\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 032 - training loss (MSE): 0.2561\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 033 - training loss (MSE): 0.2452\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 034 - training loss (MSE): 0.2456\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 035 - training loss (MSE): 0.2484\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 036 - training loss (MSE): 0.2391\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 037 - training loss (MSE): 0.2432\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 038 - training loss (MSE): 0.2425\n",
      "2025-05-23 15:35:33 [INFO]: Epoch 039 - training loss (MSE): 0.2293\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 040 - training loss (MSE): 0.2273\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 041 - training loss (MSE): 0.2301\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 042 - training loss (MSE): 0.2290\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 043 - training loss (MSE): 0.2300\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 044 - training loss (MSE): 0.2183\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 045 - training loss (MSE): 0.2138\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 046 - training loss (MSE): 0.2138\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 047 - training loss (MSE): 0.2090\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 048 - training loss (MSE): 0.2153\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 049 - training loss (MSE): 0.2027\n",
      "2025-05-23 15:35:34 [INFO]: Epoch 050 - training loss (MSE): 0.2077\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 051 - training loss (MSE): 0.2071\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 052 - training loss (MSE): 0.2094\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 053 - training loss (MSE): 0.2071\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 054 - training loss (MSE): 0.2012\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 055 - training loss (MSE): 0.1970\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 056 - training loss (MSE): 0.2021\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 057 - training loss (MSE): 0.1919\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 058 - training loss (MSE): 0.1992\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 059 - training loss (MSE): 0.1936\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 060 - training loss (MSE): 0.1889\n",
      "2025-05-23 15:35:35 [INFO]: Epoch 061 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 062 - training loss (MSE): 0.1824\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 063 - training loss (MSE): 0.1827\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 064 - training loss (MSE): 0.1836\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 065 - training loss (MSE): 0.1782\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 066 - training loss (MSE): 0.1742\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 067 - training loss (MSE): 0.1820\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 068 - training loss (MSE): 0.1761\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 069 - training loss (MSE): 0.1762\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 070 - training loss (MSE): 0.1671\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 071 - training loss (MSE): 0.1697\n",
      "2025-05-23 15:35:36 [INFO]: Epoch 072 - training loss (MSE): 0.1671\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 073 - training loss (MSE): 0.1676\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 074 - training loss (MSE): 0.1648\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 075 - training loss (MSE): 0.1715\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 076 - training loss (MSE): 0.1660\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 077 - training loss (MSE): 0.1617\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 078 - training loss (MSE): 0.1583\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 079 - training loss (MSE): 0.1629\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 080 - training loss (MSE): 0.1609\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 081 - training loss (MSE): 0.1573\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 082 - training loss (MSE): 0.1549\n",
      "2025-05-23 15:35:37 [INFO]: Epoch 083 - training loss (MSE): 0.1581\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 084 - training loss (MSE): 0.1525\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 085 - training loss (MSE): 0.1485\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 086 - training loss (MSE): 0.1497\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 087 - training loss (MSE): 0.1438\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 088 - training loss (MSE): 0.1450\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 089 - training loss (MSE): 0.1450\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 090 - training loss (MSE): 0.1441\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 091 - training loss (MSE): 0.1416\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 092 - training loss (MSE): 0.1452\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 093 - training loss (MSE): 0.1389\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 094 - training loss (MSE): 0.1374\n",
      "2025-05-23 15:35:38 [INFO]: Epoch 095 - training loss (MSE): 0.1384\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 096 - training loss (MSE): 0.1377\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 097 - training loss (MSE): 0.1358\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 098 - training loss (MSE): 0.1317\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 099 - training loss (MSE): 0.1347\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 100 - training loss (MSE): 0.1328\n",
      "2025-05-23 15:35:39 [INFO]: Finished training. The best model is from epoch#98.\n",
      "[I 2025-05-23 15:35:39,606] Trial 6 finished with value: 0.33889591923680673 and parameters: {'n_layers': 3, 'd_model': 64, 'd_ffn': 32, 'n_heads': 3, 'top_k': 2, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 7.411200761456233e-05, 'weight_decay': 0.0009705778096597715}. Best is trial 1 with value: 0.2375157177107524.\n",
      "2025-05-23 15:35:39 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:39 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:39 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:39 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 689,153\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 001 - training loss (MSE): 0.9247\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 002 - training loss (MSE): 0.3808\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 003 - training loss (MSE): 0.2453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:39 [INFO]: Epoch 004 - training loss (MSE): 0.1743\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 005 - training loss (MSE): 0.1621\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 006 - training loss (MSE): 0.1306\n",
      "2025-05-23 15:35:39 [INFO]: Epoch 007 - training loss (MSE): 0.1113\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 008 - training loss (MSE): 0.1010\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 009 - training loss (MSE): 0.0941\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 010 - training loss (MSE): 0.0956\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 011 - training loss (MSE): 0.0811\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 012 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 013 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 014 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 015 - training loss (MSE): 0.0654\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 016 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 017 - training loss (MSE): 0.0628\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 018 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 019 - training loss (MSE): 0.0541\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 020 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 021 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 022 - training loss (MSE): 0.0447\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 023 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 024 - training loss (MSE): 0.0436\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 025 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 026 - training loss (MSE): 0.0379\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 027 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 028 - training loss (MSE): 0.0366\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 029 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 030 - training loss (MSE): 0.0358\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 031 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:35:40 [INFO]: Epoch 032 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 033 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 034 - training loss (MSE): 0.0324\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 035 - training loss (MSE): 0.0332\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 036 - training loss (MSE): 0.0299\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 037 - training loss (MSE): 0.0314\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 038 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 039 - training loss (MSE): 0.0324\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 040 - training loss (MSE): 0.0304\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 041 - training loss (MSE): 0.0281\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 042 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 043 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 044 - training loss (MSE): 0.0287\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 045 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 046 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 047 - training loss (MSE): 0.0291\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 048 - training loss (MSE): 0.0298\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 049 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 050 - training loss (MSE): 0.0316\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 051 - training loss (MSE): 0.0274\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 052 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 053 - training loss (MSE): 0.0292\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 054 - training loss (MSE): 0.0294\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 055 - training loss (MSE): 0.0289\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 056 - training loss (MSE): 0.0274\n",
      "2025-05-23 15:35:41 [INFO]: Epoch 057 - training loss (MSE): 0.0277\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 058 - training loss (MSE): 0.0290\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 059 - training loss (MSE): 0.0266\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 060 - training loss (MSE): 0.0265\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 061 - training loss (MSE): 0.0267\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 062 - training loss (MSE): 0.0270\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 063 - training loss (MSE): 0.0272\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 064 - training loss (MSE): 0.0255\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 065 - training loss (MSE): 0.0290\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 066 - training loss (MSE): 0.0259\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 067 - training loss (MSE): 0.0268\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 068 - training loss (MSE): 0.0269\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 069 - training loss (MSE): 0.0272\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 070 - training loss (MSE): 0.0277\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 071 - training loss (MSE): 0.0268\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 072 - training loss (MSE): 0.0267\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 073 - training loss (MSE): 0.0266\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 074 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:35:42 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:35:42 [INFO]: Finished training. The best model is from epoch#64.\n",
      "[I 2025-05-23 15:35:42,782] Trial 7 finished with value: 0.23652139200056946 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0038565644906092124, 'weight_decay': 0.00023577029727229654}. Best is trial 7 with value: 0.23652139200056946.\n",
      "2025-05-23 15:35:42 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:42 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:42 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:42 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,353,153\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 001 - training loss (MSE): 1.0275\n",
      "2025-05-23 15:35:42 [INFO]: Epoch 002 - training loss (MSE): 0.8612\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 003 - training loss (MSE): 0.7230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:43 [INFO]: Epoch 004 - training loss (MSE): 0.5795\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 005 - training loss (MSE): 0.4570\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 006 - training loss (MSE): 0.3789\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 007 - training loss (MSE): 0.3223\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 008 - training loss (MSE): 0.3208\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 009 - training loss (MSE): 0.3252\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 010 - training loss (MSE): 0.3166\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 011 - training loss (MSE): 0.3054\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 012 - training loss (MSE): 0.2786\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 013 - training loss (MSE): 0.2731\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 014 - training loss (MSE): 0.2587\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 015 - training loss (MSE): 0.2577\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 016 - training loss (MSE): 0.2497\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 017 - training loss (MSE): 0.2456\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 018 - training loss (MSE): 0.2412\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 019 - training loss (MSE): 0.2324\n",
      "2025-05-23 15:35:43 [INFO]: Epoch 020 - training loss (MSE): 0.2211\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 021 - training loss (MSE): 0.2230\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 022 - training loss (MSE): 0.2155\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 023 - training loss (MSE): 0.2140\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 024 - training loss (MSE): 0.1995\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 025 - training loss (MSE): 0.1906\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 026 - training loss (MSE): 0.1908\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 027 - training loss (MSE): 0.1881\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 028 - training loss (MSE): 0.1808\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 029 - training loss (MSE): 0.1726\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 030 - training loss (MSE): 0.1703\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 031 - training loss (MSE): 0.1639\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 032 - training loss (MSE): 0.1590\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 033 - training loss (MSE): 0.1588\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 034 - training loss (MSE): 0.1503\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 035 - training loss (MSE): 0.1484\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 036 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 037 - training loss (MSE): 0.1420\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 038 - training loss (MSE): 0.1380\n",
      "2025-05-23 15:35:44 [INFO]: Epoch 039 - training loss (MSE): 0.1415\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 040 - training loss (MSE): 0.1328\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 041 - training loss (MSE): 0.1312\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 042 - training loss (MSE): 0.1280\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 043 - training loss (MSE): 0.1316\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 044 - training loss (MSE): 0.1236\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 045 - training loss (MSE): 0.1212\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 046 - training loss (MSE): 0.1149\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 047 - training loss (MSE): 0.1115\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 048 - training loss (MSE): 0.1092\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 049 - training loss (MSE): 0.1134\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 050 - training loss (MSE): 0.1097\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 051 - training loss (MSE): 0.1068\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 052 - training loss (MSE): 0.0994\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 053 - training loss (MSE): 0.1080\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 054 - training loss (MSE): 0.1041\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 055 - training loss (MSE): 0.1020\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 056 - training loss (MSE): 0.0986\n",
      "2025-05-23 15:35:45 [INFO]: Epoch 057 - training loss (MSE): 0.0962\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 058 - training loss (MSE): 0.0961\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 059 - training loss (MSE): 0.0912\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 060 - training loss (MSE): 0.0953\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 061 - training loss (MSE): 0.0912\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 062 - training loss (MSE): 0.0918\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 063 - training loss (MSE): 0.0900\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 064 - training loss (MSE): 0.0887\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 065 - training loss (MSE): 0.0859\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 066 - training loss (MSE): 0.0846\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 067 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 068 - training loss (MSE): 0.0821\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 069 - training loss (MSE): 0.0804\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 070 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 071 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 072 - training loss (MSE): 0.0770\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 073 - training loss (MSE): 0.0771\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 074 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:35:46 [INFO]: Epoch 075 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 076 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 077 - training loss (MSE): 0.0747\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 078 - training loss (MSE): 0.0763\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 079 - training loss (MSE): 0.0715\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 080 - training loss (MSE): 0.0701\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 081 - training loss (MSE): 0.0712\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 082 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 083 - training loss (MSE): 0.0732\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 084 - training loss (MSE): 0.0669\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 085 - training loss (MSE): 0.0696\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 086 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 087 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 088 - training loss (MSE): 0.0673\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 089 - training loss (MSE): 0.0675\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 090 - training loss (MSE): 0.0666\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 091 - training loss (MSE): 0.0681\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 092 - training loss (MSE): 0.0662\n",
      "2025-05-23 15:35:47 [INFO]: Epoch 093 - training loss (MSE): 0.0667\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 094 - training loss (MSE): 0.0666\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 095 - training loss (MSE): 0.0672\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 096 - training loss (MSE): 0.0697\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 097 - training loss (MSE): 0.0631\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 098 - training loss (MSE): 0.0626\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 099 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 100 - training loss (MSE): 0.0624\n",
      "2025-05-23 15:35:48 [INFO]: Finished training. The best model is from epoch#100.\n",
      "[I 2025-05-23 15:35:48,473] Trial 8 finished with value: 0.2513832391978127 and parameters: {'n_layers': 2, 'd_model': 64, 'd_ffn': 32, 'n_heads': 1, 'top_k': 1, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.1, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0002128524987103289, 'weight_decay': 0.0007940117135494467}. Best is trial 7 with value: 0.23652139200056946.\n",
      "2025-05-23 15:35:48 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:48 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:48 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:48 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 36,569\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 001 - training loss (MSE): 0.5229\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 002 - training loss (MSE): 0.2811\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 003 - training loss (MSE): 0.2065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:48 [INFO]: Epoch 004 - training loss (MSE): 0.1635\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 005 - training loss (MSE): 0.1304\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 006 - training loss (MSE): 0.1199\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 007 - training loss (MSE): 0.1030\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 008 - training loss (MSE): 0.0861\n",
      "2025-05-23 15:35:48 [INFO]: Epoch 009 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 010 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 011 - training loss (MSE): 0.0601\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 012 - training loss (MSE): 0.0587\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 013 - training loss (MSE): 0.0514\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 014 - training loss (MSE): 0.0505\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 015 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 016 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 017 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 018 - training loss (MSE): 0.0418\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 019 - training loss (MSE): 0.0407\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 020 - training loss (MSE): 0.0361\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 021 - training loss (MSE): 0.0362\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 022 - training loss (MSE): 0.0359\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 023 - training loss (MSE): 0.0354\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 024 - training loss (MSE): 0.0350\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 025 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 026 - training loss (MSE): 0.0359\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 027 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 028 - training loss (MSE): 0.0323\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 029 - training loss (MSE): 0.0301\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 030 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 031 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:35:49 [INFO]: Epoch 032 - training loss (MSE): 0.0331\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 033 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 034 - training loss (MSE): 0.0322\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 035 - training loss (MSE): 0.0312\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 036 - training loss (MSE): 0.0323\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 037 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 038 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 039 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 040 - training loss (MSE): 0.0310\n",
      "2025-05-23 15:35:50 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:35:50 [INFO]: Finished training. The best model is from epoch#30.\n",
      "[I 2025-05-23 15:35:50,478] Trial 9 finished with value: 0.30940735395611757 and parameters: {'n_layers': 1, 'd_model': 64, 'd_ffn': 8, 'n_heads': 3, 'top_k': 3, 'n_kernels': 3, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.007691181798461813, 'weight_decay': 0.0006470205285361298}. Best is trial 7 with value: 0.23652139200056946.\n",
      "2025-05-23 15:35:50 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:50 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:50 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:50 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 172,673\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 001 - training loss (MSE): 1.7583\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 002 - training loss (MSE): 1.1200\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 003 - training loss (MSE): 0.5898\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 004 - training loss (MSE): 0.3335\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 005 - training loss (MSE): 0.2543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:50 [INFO]: Epoch 006 - training loss (MSE): 0.2499\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 007 - training loss (MSE): 0.2499\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 008 - training loss (MSE): 0.2540\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 009 - training loss (MSE): 0.2520\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 010 - training loss (MSE): 0.2518\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 011 - training loss (MSE): 0.2534\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 012 - training loss (MSE): 0.2479\n",
      "2025-05-23 15:35:50 [INFO]: Epoch 013 - training loss (MSE): 0.2366\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 014 - training loss (MSE): 0.2324\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 015 - training loss (MSE): 0.2200\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 016 - training loss (MSE): 0.2080\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 017 - training loss (MSE): 0.1974\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 018 - training loss (MSE): 0.1819\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 019 - training loss (MSE): 0.1780\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 020 - training loss (MSE): 0.1746\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 021 - training loss (MSE): 0.1545\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 022 - training loss (MSE): 0.1513\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 023 - training loss (MSE): 0.1367\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 024 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 025 - training loss (MSE): 0.1282\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 026 - training loss (MSE): 0.1302\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 027 - training loss (MSE): 0.1187\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 028 - training loss (MSE): 0.1149\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 029 - training loss (MSE): 0.1068\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 030 - training loss (MSE): 0.1099\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 031 - training loss (MSE): 0.0999\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 032 - training loss (MSE): 0.1013\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 033 - training loss (MSE): 0.0940\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 034 - training loss (MSE): 0.0896\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 035 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 036 - training loss (MSE): 0.0893\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 037 - training loss (MSE): 0.0875\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 038 - training loss (MSE): 0.0847\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 039 - training loss (MSE): 0.0840\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 040 - training loss (MSE): 0.0829\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 041 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 042 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:35:51 [INFO]: Epoch 043 - training loss (MSE): 0.0840\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 044 - training loss (MSE): 0.0823\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 045 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 046 - training loss (MSE): 0.0738\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 047 - training loss (MSE): 0.0728\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 048 - training loss (MSE): 0.0749\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 049 - training loss (MSE): 0.0719\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 050 - training loss (MSE): 0.0739\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 051 - training loss (MSE): 0.0697\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 052 - training loss (MSE): 0.0671\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 053 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 054 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 055 - training loss (MSE): 0.0719\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 056 - training loss (MSE): 0.0700\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 057 - training loss (MSE): 0.0644\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 058 - training loss (MSE): 0.0672\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 059 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 060 - training loss (MSE): 0.0670\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 061 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 062 - training loss (MSE): 0.0642\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 063 - training loss (MSE): 0.0663\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 064 - training loss (MSE): 0.0641\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 065 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 066 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 067 - training loss (MSE): 0.0731\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 068 - training loss (MSE): 0.0649\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 069 - training loss (MSE): 0.0643\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 070 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 071 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 072 - training loss (MSE): 0.0622\n",
      "2025-05-23 15:35:52 [INFO]: Epoch 073 - training loss (MSE): 0.0635\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 074 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 075 - training loss (MSE): 0.0609\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 076 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 077 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 078 - training loss (MSE): 0.0609\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 079 - training loss (MSE): 0.0598\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 080 - training loss (MSE): 0.0608\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 081 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 082 - training loss (MSE): 0.0607\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 083 - training loss (MSE): 0.0576\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 084 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 085 - training loss (MSE): 0.0607\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 086 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 087 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 088 - training loss (MSE): 0.0588\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 089 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 090 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 091 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 092 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 093 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 094 - training loss (MSE): 0.0566\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 095 - training loss (MSE): 0.0587\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 096 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 097 - training loss (MSE): 0.0537\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 098 - training loss (MSE): 0.0539\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 099 - training loss (MSE): 0.0564\n",
      "2025-05-23 15:35:53 [INFO]: Epoch 100 - training loss (MSE): 0.0548\n",
      "2025-05-23 15:35:53 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:35:54,001] Trial 10 finished with value: 0.27148234572566654 and parameters: {'n_layers': 2, 'd_model': 8, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0022922125037629845, 'weight_decay': 6.245289685592381e-05}. Best is trial 7 with value: 0.23652139200056946.\n",
      "2025-05-23 15:35:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:35:54 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:35:54 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:35:54 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 677,585\n",
      "2025-05-23 15:35:54 [INFO]: Epoch 001 - training loss (MSE): 0.6369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:35:54 [INFO]: Epoch 002 - training loss (MSE): 0.4173\n",
      "2025-05-23 15:35:54 [INFO]: Epoch 003 - training loss (MSE): 0.3887\n",
      "2025-05-23 15:35:54 [INFO]: Epoch 004 - training loss (MSE): 0.3141\n",
      "2025-05-23 15:35:54 [INFO]: Epoch 005 - training loss (MSE): 0.2890\n",
      "2025-05-23 15:35:54 [INFO]: Epoch 006 - training loss (MSE): 0.2720\n",
      "2025-05-23 15:35:55 [INFO]: Epoch 007 - training loss (MSE): 0.2512\n",
      "2025-05-23 15:35:55 [INFO]: Epoch 008 - training loss (MSE): 0.2292\n",
      "2025-05-23 15:35:55 [INFO]: Epoch 009 - training loss (MSE): 0.2062\n",
      "2025-05-23 15:35:55 [INFO]: Epoch 010 - training loss (MSE): 0.1981\n",
      "2025-05-23 15:35:55 [INFO]: Epoch 011 - training loss (MSE): 0.1780\n",
      "2025-05-23 15:35:55 [INFO]: Epoch 012 - training loss (MSE): 0.1706\n",
      "2025-05-23 15:35:55 [INFO]: Epoch 013 - training loss (MSE): 0.1524\n",
      "2025-05-23 15:35:56 [INFO]: Epoch 014 - training loss (MSE): 0.1503\n",
      "2025-05-23 15:35:56 [INFO]: Epoch 015 - training loss (MSE): 0.1375\n",
      "2025-05-23 15:35:56 [INFO]: Epoch 016 - training loss (MSE): 0.1309\n",
      "2025-05-23 15:35:56 [INFO]: Epoch 017 - training loss (MSE): 0.1273\n",
      "2025-05-23 15:35:56 [INFO]: Epoch 018 - training loss (MSE): 0.1197\n",
      "2025-05-23 15:35:56 [INFO]: Epoch 019 - training loss (MSE): 0.1145\n",
      "2025-05-23 15:35:56 [INFO]: Epoch 020 - training loss (MSE): 0.1062\n",
      "2025-05-23 15:35:57 [INFO]: Epoch 021 - training loss (MSE): 0.0997\n",
      "2025-05-23 15:35:57 [INFO]: Epoch 022 - training loss (MSE): 0.0953\n",
      "2025-05-23 15:35:57 [INFO]: Epoch 023 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:35:57 [INFO]: Epoch 024 - training loss (MSE): 0.0878\n",
      "2025-05-23 15:35:57 [INFO]: Epoch 025 - training loss (MSE): 0.0847\n",
      "2025-05-23 15:35:57 [INFO]: Epoch 026 - training loss (MSE): 0.0816\n",
      "2025-05-23 15:35:58 [INFO]: Epoch 027 - training loss (MSE): 0.0797\n",
      "2025-05-23 15:35:58 [INFO]: Epoch 028 - training loss (MSE): 0.0809\n",
      "2025-05-23 15:35:58 [INFO]: Epoch 029 - training loss (MSE): 0.0798\n",
      "2025-05-23 15:35:58 [INFO]: Epoch 030 - training loss (MSE): 0.0752\n",
      "2025-05-23 15:35:58 [INFO]: Epoch 031 - training loss (MSE): 0.0704\n",
      "2025-05-23 15:35:58 [INFO]: Epoch 032 - training loss (MSE): 0.0707\n",
      "2025-05-23 15:35:58 [INFO]: Epoch 033 - training loss (MSE): 0.0681\n",
      "2025-05-23 15:35:59 [INFO]: Epoch 034 - training loss (MSE): 0.0692\n",
      "2025-05-23 15:35:59 [INFO]: Epoch 035 - training loss (MSE): 0.0631\n",
      "2025-05-23 15:35:59 [INFO]: Epoch 036 - training loss (MSE): 0.0661\n",
      "2025-05-23 15:35:59 [INFO]: Epoch 037 - training loss (MSE): 0.0613\n",
      "2025-05-23 15:35:59 [INFO]: Epoch 038 - training loss (MSE): 0.0585\n",
      "2025-05-23 15:35:59 [INFO]: Epoch 039 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:35:59 [INFO]: Epoch 040 - training loss (MSE): 0.0588\n",
      "2025-05-23 15:36:00 [INFO]: Epoch 041 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:36:00 [INFO]: Epoch 042 - training loss (MSE): 0.0612\n",
      "2025-05-23 15:36:00 [INFO]: Epoch 043 - training loss (MSE): 0.0582\n",
      "2025-05-23 15:36:00 [INFO]: Epoch 044 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:36:00 [INFO]: Epoch 045 - training loss (MSE): 0.0550\n",
      "2025-05-23 15:36:00 [INFO]: Epoch 046 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:36:00 [INFO]: Epoch 047 - training loss (MSE): 0.0569\n",
      "2025-05-23 15:36:01 [INFO]: Epoch 048 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:36:01 [INFO]: Epoch 049 - training loss (MSE): 0.0516\n",
      "2025-05-23 15:36:01 [INFO]: Epoch 050 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:36:01 [INFO]: Epoch 051 - training loss (MSE): 0.0525\n",
      "2025-05-23 15:36:01 [INFO]: Epoch 052 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:36:01 [INFO]: Epoch 053 - training loss (MSE): 0.0520\n",
      "2025-05-23 15:36:01 [INFO]: Epoch 054 - training loss (MSE): 0.0519\n",
      "2025-05-23 15:36:02 [INFO]: Epoch 055 - training loss (MSE): 0.0524\n",
      "2025-05-23 15:36:02 [INFO]: Epoch 056 - training loss (MSE): 0.0488\n",
      "2025-05-23 15:36:02 [INFO]: Epoch 057 - training loss (MSE): 0.0505\n",
      "2025-05-23 15:36:02 [INFO]: Epoch 058 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:36:02 [INFO]: Epoch 059 - training loss (MSE): 0.0474\n",
      "2025-05-23 15:36:02 [INFO]: Epoch 060 - training loss (MSE): 0.0468\n",
      "2025-05-23 15:36:02 [INFO]: Epoch 061 - training loss (MSE): 0.0467\n",
      "2025-05-23 15:36:03 [INFO]: Epoch 062 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:36:03 [INFO]: Epoch 063 - training loss (MSE): 0.0437\n",
      "2025-05-23 15:36:03 [INFO]: Epoch 064 - training loss (MSE): 0.0478\n",
      "2025-05-23 15:36:03 [INFO]: Epoch 065 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:36:03 [INFO]: Epoch 066 - training loss (MSE): 0.0455\n",
      "2025-05-23 15:36:03 [INFO]: Epoch 067 - training loss (MSE): 0.0457\n",
      "2025-05-23 15:36:03 [INFO]: Epoch 068 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:36:04 [INFO]: Epoch 069 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:36:04 [INFO]: Epoch 070 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:36:04 [INFO]: Epoch 071 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:36:04 [INFO]: Epoch 072 - training loss (MSE): 0.0459\n",
      "2025-05-23 15:36:04 [INFO]: Epoch 073 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:36:04 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:36:04 [INFO]: Finished training. The best model is from epoch#63.\n",
      "[I 2025-05-23 15:36:04,803] Trial 11 finished with value: 0.24093627097405948 and parameters: {'n_layers': 1, 'd_model': 128, 'd_ffn': 16, 'n_heads': 2, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0005597413939486375, 'weight_decay': 0.0002858718430285778}. Best is trial 7 with value: 0.23652139200056946.\n",
      "2025-05-23 15:36:04 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:04 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:04 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:04 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 339,625\n",
      "2025-05-23 15:36:05 [INFO]: Epoch 001 - training loss (MSE): 0.4977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:05 [INFO]: Epoch 002 - training loss (MSE): 0.4054\n",
      "2025-05-23 15:36:05 [INFO]: Epoch 003 - training loss (MSE): 0.3669\n",
      "2025-05-23 15:36:05 [INFO]: Epoch 004 - training loss (MSE): 0.3398\n",
      "2025-05-23 15:36:05 [INFO]: Epoch 005 - training loss (MSE): 0.3162\n",
      "2025-05-23 15:36:05 [INFO]: Epoch 006 - training loss (MSE): 0.2808\n",
      "2025-05-23 15:36:05 [INFO]: Epoch 007 - training loss (MSE): 0.2731\n",
      "2025-05-23 15:36:06 [INFO]: Epoch 008 - training loss (MSE): 0.2506\n",
      "2025-05-23 15:36:06 [INFO]: Epoch 009 - training loss (MSE): 0.2382\n",
      "2025-05-23 15:36:06 [INFO]: Epoch 010 - training loss (MSE): 0.2289\n",
      "2025-05-23 15:36:06 [INFO]: Epoch 011 - training loss (MSE): 0.2123\n",
      "2025-05-23 15:36:06 [INFO]: Epoch 012 - training loss (MSE): 0.2059\n",
      "2025-05-23 15:36:06 [INFO]: Epoch 013 - training loss (MSE): 0.1926\n",
      "2025-05-23 15:36:06 [INFO]: Epoch 014 - training loss (MSE): 0.1873\n",
      "2025-05-23 15:36:07 [INFO]: Epoch 015 - training loss (MSE): 0.1715\n",
      "2025-05-23 15:36:07 [INFO]: Epoch 016 - training loss (MSE): 0.1575\n",
      "2025-05-23 15:36:07 [INFO]: Epoch 017 - training loss (MSE): 0.1553\n",
      "2025-05-23 15:36:07 [INFO]: Epoch 018 - training loss (MSE): 0.1444\n",
      "2025-05-23 15:36:07 [INFO]: Epoch 019 - training loss (MSE): 0.1344\n",
      "2025-05-23 15:36:07 [INFO]: Epoch 020 - training loss (MSE): 0.1283\n",
      "2025-05-23 15:36:07 [INFO]: Epoch 021 - training loss (MSE): 0.1196\n",
      "2025-05-23 15:36:08 [INFO]: Epoch 022 - training loss (MSE): 0.1116\n",
      "2025-05-23 15:36:08 [INFO]: Epoch 023 - training loss (MSE): 0.1145\n",
      "2025-05-23 15:36:08 [INFO]: Epoch 024 - training loss (MSE): 0.1089\n",
      "2025-05-23 15:36:08 [INFO]: Epoch 025 - training loss (MSE): 0.0982\n",
      "2025-05-23 15:36:08 [INFO]: Epoch 026 - training loss (MSE): 0.0990\n",
      "2025-05-23 15:36:08 [INFO]: Epoch 027 - training loss (MSE): 0.0926\n",
      "2025-05-23 15:36:08 [INFO]: Epoch 028 - training loss (MSE): 0.0938\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 029 - training loss (MSE): 0.0858\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 030 - training loss (MSE): 0.0871\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 031 - training loss (MSE): 0.0865\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 032 - training loss (MSE): 0.0876\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 033 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 035 - training loss (MSE): 0.0771\n",
      "2025-05-23 15:36:09 [INFO]: Epoch 036 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:36:10 [INFO]: Epoch 037 - training loss (MSE): 0.0811\n",
      "2025-05-23 15:36:10 [INFO]: Epoch 038 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:36:10 [INFO]: Epoch 039 - training loss (MSE): 0.0705\n",
      "2025-05-23 15:36:10 [INFO]: Epoch 040 - training loss (MSE): 0.0704\n",
      "2025-05-23 15:36:10 [INFO]: Epoch 041 - training loss (MSE): 0.0710\n",
      "2025-05-23 15:36:10 [INFO]: Epoch 042 - training loss (MSE): 0.0685\n",
      "2025-05-23 15:36:11 [INFO]: Epoch 043 - training loss (MSE): 0.0719\n",
      "2025-05-23 15:36:11 [INFO]: Epoch 044 - training loss (MSE): 0.0736\n",
      "2025-05-23 15:36:11 [INFO]: Epoch 045 - training loss (MSE): 0.0712\n",
      "2025-05-23 15:36:11 [INFO]: Epoch 046 - training loss (MSE): 0.0711\n",
      "2025-05-23 15:36:11 [INFO]: Epoch 047 - training loss (MSE): 0.0728\n",
      "2025-05-23 15:36:11 [INFO]: Epoch 048 - training loss (MSE): 0.0678\n",
      "2025-05-23 15:36:11 [INFO]: Epoch 049 - training loss (MSE): 0.0639\n",
      "2025-05-23 15:36:12 [INFO]: Epoch 050 - training loss (MSE): 0.0631\n",
      "2025-05-23 15:36:12 [INFO]: Epoch 051 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:36:12 [INFO]: Epoch 052 - training loss (MSE): 0.0650\n",
      "2025-05-23 15:36:12 [INFO]: Epoch 053 - training loss (MSE): 0.0628\n",
      "2025-05-23 15:36:12 [INFO]: Epoch 054 - training loss (MSE): 0.0634\n",
      "2025-05-23 15:36:12 [INFO]: Epoch 055 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:36:12 [INFO]: Epoch 056 - training loss (MSE): 0.0657\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 057 - training loss (MSE): 0.0646\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 058 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 059 - training loss (MSE): 0.0653\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 060 - training loss (MSE): 0.0596\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 061 - training loss (MSE): 0.0612\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 062 - training loss (MSE): 0.0593\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 063 - training loss (MSE): 0.0623\n",
      "2025-05-23 15:36:13 [INFO]: Epoch 064 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:36:14 [INFO]: Epoch 065 - training loss (MSE): 0.0586\n",
      "2025-05-23 15:36:14 [INFO]: Epoch 066 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:36:14 [INFO]: Epoch 067 - training loss (MSE): 0.0607\n",
      "2025-05-23 15:36:14 [INFO]: Epoch 068 - training loss (MSE): 0.0558\n",
      "2025-05-23 15:36:14 [INFO]: Epoch 069 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:36:14 [INFO]: Epoch 070 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:36:14 [INFO]: Epoch 071 - training loss (MSE): 0.0600\n",
      "2025-05-23 15:36:15 [INFO]: Epoch 072 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:36:15 [INFO]: Epoch 073 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:36:15 [INFO]: Epoch 074 - training loss (MSE): 0.0568\n",
      "2025-05-23 15:36:15 [INFO]: Epoch 075 - training loss (MSE): 0.0577\n",
      "2025-05-23 15:36:15 [INFO]: Epoch 076 - training loss (MSE): 0.0564\n",
      "2025-05-23 15:36:15 [INFO]: Epoch 077 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:36:15 [INFO]: Epoch 078 - training loss (MSE): 0.0554\n",
      "2025-05-23 15:36:16 [INFO]: Epoch 079 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:36:16 [INFO]: Epoch 080 - training loss (MSE): 0.0573\n",
      "2025-05-23 15:36:16 [INFO]: Epoch 081 - training loss (MSE): 0.0590\n",
      "2025-05-23 15:36:16 [INFO]: Epoch 082 - training loss (MSE): 0.0599\n",
      "2025-05-23 15:36:16 [INFO]: Epoch 083 - training loss (MSE): 0.0561\n",
      "2025-05-23 15:36:16 [INFO]: Epoch 084 - training loss (MSE): 0.0559\n",
      "2025-05-23 15:36:16 [INFO]: Epoch 085 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 086 - training loss (MSE): 0.0580\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 087 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 088 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:36:17 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:36:17 [INFO]: Finished training. The best model is from epoch#78.\n",
      "[I 2025-05-23 15:36:17,458] Trial 12 finished with value: 0.27364734372318245 and parameters: {'n_layers': 1, 'd_model': 128, 'd_ffn': 8, 'n_heads': 3, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0004127289522954379, 'weight_decay': 0.00026744327270321225}. Best is trial 7 with value: 0.23652139200056946.\n",
      "2025-05-23 15:36:17 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:17 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:17 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:17 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,737\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 001 - training loss (MSE): 1.0762\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 002 - training loss (MSE): 0.4315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:17 [INFO]: Epoch 003 - training loss (MSE): 0.3429\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 004 - training loss (MSE): 0.3151\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 005 - training loss (MSE): 0.2428\n",
      "2025-05-23 15:36:17 [INFO]: Epoch 006 - training loss (MSE): 0.2308\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 007 - training loss (MSE): 0.2081\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 008 - training loss (MSE): 0.1810\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 009 - training loss (MSE): 0.1724\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 010 - training loss (MSE): 0.1517\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 011 - training loss (MSE): 0.1469\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 012 - training loss (MSE): 0.1338\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 013 - training loss (MSE): 0.1281\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 014 - training loss (MSE): 0.1146\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 015 - training loss (MSE): 0.1083\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 016 - training loss (MSE): 0.1016\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 017 - training loss (MSE): 0.0955\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 018 - training loss (MSE): 0.0926\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 019 - training loss (MSE): 0.0833\n",
      "2025-05-23 15:36:18 [INFO]: Epoch 020 - training loss (MSE): 0.0883\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 021 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 022 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 023 - training loss (MSE): 0.0806\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 024 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 025 - training loss (MSE): 0.0743\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 026 - training loss (MSE): 0.0687\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 027 - training loss (MSE): 0.0703\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 028 - training loss (MSE): 0.0706\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 029 - training loss (MSE): 0.0654\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 030 - training loss (MSE): 0.0639\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 031 - training loss (MSE): 0.0620\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 032 - training loss (MSE): 0.0613\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 033 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:36:19 [INFO]: Epoch 034 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 035 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 036 - training loss (MSE): 0.0574\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 037 - training loss (MSE): 0.0599\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 038 - training loss (MSE): 0.0584\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 039 - training loss (MSE): 0.0577\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 040 - training loss (MSE): 0.0585\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 041 - training loss (MSE): 0.0580\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 042 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 043 - training loss (MSE): 0.0565\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 044 - training loss (MSE): 0.0543\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 045 - training loss (MSE): 0.0528\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 046 - training loss (MSE): 0.0529\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 047 - training loss (MSE): 0.0487\n",
      "2025-05-23 15:36:20 [INFO]: Epoch 048 - training loss (MSE): 0.0513\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 049 - training loss (MSE): 0.0492\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 050 - training loss (MSE): 0.0550\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 051 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 052 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 053 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 054 - training loss (MSE): 0.0483\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 055 - training loss (MSE): 0.0480\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 056 - training loss (MSE): 0.0494\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 057 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 058 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 059 - training loss (MSE): 0.0459\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 060 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 061 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:36:21 [INFO]: Epoch 062 - training loss (MSE): 0.0457\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 063 - training loss (MSE): 0.0449\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 064 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 065 - training loss (MSE): 0.0422\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 066 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 067 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 068 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 069 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 070 - training loss (MSE): 0.0450\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 071 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 072 - training loss (MSE): 0.0403\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 073 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 074 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 075 - training loss (MSE): 0.0414\n",
      "2025-05-23 15:36:22 [INFO]: Epoch 076 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 077 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 078 - training loss (MSE): 0.0407\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 079 - training loss (MSE): 0.0390\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 080 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 081 - training loss (MSE): 0.0359\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 082 - training loss (MSE): 0.0373\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 083 - training loss (MSE): 0.0371\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 084 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 085 - training loss (MSE): 0.0348\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 086 - training loss (MSE): 0.0368\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 087 - training loss (MSE): 0.0384\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 088 - training loss (MSE): 0.0379\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 089 - training loss (MSE): 0.0344\n",
      "2025-05-23 15:36:23 [INFO]: Epoch 090 - training loss (MSE): 0.0374\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 091 - training loss (MSE): 0.0332\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 092 - training loss (MSE): 0.0346\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 093 - training loss (MSE): 0.0350\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 094 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 095 - training loss (MSE): 0.0341\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 096 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 097 - training loss (MSE): 0.0353\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 098 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 099 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 100 - training loss (MSE): 0.0328\n",
      "2025-05-23 15:36:24 [INFO]: Finished training. The best model is from epoch#100.\n",
      "[I 2025-05-23 15:36:24,846] Trial 13 finished with value: 0.2360340310541408 and parameters: {'n_layers': 1, 'd_model': 32, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 5, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0012427183476582465, 'weight_decay': 0.0005072807145484922}. Best is trial 13 with value: 0.2360340310541408.\n",
      "2025-05-23 15:36:24 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:24 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:24 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:24 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 287,457\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 001 - training loss (MSE): 0.8872\n",
      "2025-05-23 15:36:24 [INFO]: Epoch 002 - training loss (MSE): 0.4284\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 003 - training loss (MSE): 0.2586\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 004 - training loss (MSE): 0.2776\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 005 - training loss (MSE): 0.2127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:25 [INFO]: Epoch 006 - training loss (MSE): 0.1963\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 007 - training loss (MSE): 0.1796\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 008 - training loss (MSE): 0.1570\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 009 - training loss (MSE): 0.1401\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 010 - training loss (MSE): 0.1380\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 011 - training loss (MSE): 0.1121\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 012 - training loss (MSE): 0.1069\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 013 - training loss (MSE): 0.0973\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 014 - training loss (MSE): 0.0912\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 015 - training loss (MSE): 0.0969\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 016 - training loss (MSE): 0.0838\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 017 - training loss (MSE): 0.0853\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 018 - training loss (MSE): 0.0786\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 019 - training loss (MSE): 0.0751\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 020 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 021 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 022 - training loss (MSE): 0.0679\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 023 - training loss (MSE): 0.0699\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 024 - training loss (MSE): 0.0645\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 025 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 026 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 027 - training loss (MSE): 0.0629\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 028 - training loss (MSE): 0.0585\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 029 - training loss (MSE): 0.0582\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 030 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 031 - training loss (MSE): 0.0532\n",
      "2025-05-23 15:36:25 [INFO]: Epoch 032 - training loss (MSE): 0.0546\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 033 - training loss (MSE): 0.0525\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 034 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 035 - training loss (MSE): 0.0515\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 036 - training loss (MSE): 0.0455\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 037 - training loss (MSE): 0.0481\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 038 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 039 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 040 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 041 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 042 - training loss (MSE): 0.0460\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 043 - training loss (MSE): 0.0412\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 044 - training loss (MSE): 0.0426\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 045 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 046 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 047 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 048 - training loss (MSE): 0.0403\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 049 - training loss (MSE): 0.0399\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 050 - training loss (MSE): 0.0375\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 051 - training loss (MSE): 0.0370\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 052 - training loss (MSE): 0.0380\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 053 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 054 - training loss (MSE): 0.0375\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 055 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 056 - training loss (MSE): 0.0362\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 057 - training loss (MSE): 0.0357\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 058 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 059 - training loss (MSE): 0.0352\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 060 - training loss (MSE): 0.0360\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 061 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:36:26 [INFO]: Epoch 062 - training loss (MSE): 0.0343\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 063 - training loss (MSE): 0.0354\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 064 - training loss (MSE): 0.0351\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 065 - training loss (MSE): 0.0347\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 066 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 067 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 068 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 069 - training loss (MSE): 0.0328\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 070 - training loss (MSE): 0.0345\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 071 - training loss (MSE): 0.0340\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 072 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 073 - training loss (MSE): 0.0313\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 074 - training loss (MSE): 0.0329\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 075 - training loss (MSE): 0.0346\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 076 - training loss (MSE): 0.0316\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 077 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 078 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 079 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 080 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 081 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 082 - training loss (MSE): 0.0331\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 083 - training loss (MSE): 0.0324\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 084 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 085 - training loss (MSE): 0.0346\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 086 - training loss (MSE): 0.0313\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 087 - training loss (MSE): 0.0323\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 088 - training loss (MSE): 0.0321\n",
      "2025-05-23 15:36:27 [INFO]: Epoch 089 - training loss (MSE): 0.0318\n",
      "2025-05-23 15:36:27 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:36:27 [INFO]: Finished training. The best model is from epoch#79.\n",
      "[I 2025-05-23 15:36:28,007] Trial 14 finished with value: 0.27746988812981843 and parameters: {'n_layers': 1, 'd_model': 32, 'd_ffn': 128, 'n_heads': 3, 'top_k': 1, 'n_kernels': 3, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.002365300835186382, 'weight_decay': 0.0005401266377398488}. Best is trial 13 with value: 0.2360340310541408.\n",
      "2025-05-23 15:36:28 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:28 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:28 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:28 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,377,793\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 001 - training loss (MSE): 0.6846\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 002 - training loss (MSE): 0.3880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:28 [INFO]: Epoch 003 - training loss (MSE): 0.2990\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 004 - training loss (MSE): 0.2497\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 005 - training loss (MSE): 0.2141\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 006 - training loss (MSE): 0.1874\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 007 - training loss (MSE): 0.1794\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 008 - training loss (MSE): 0.1536\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 009 - training loss (MSE): 0.1418\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 010 - training loss (MSE): 0.1325\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 011 - training loss (MSE): 0.1168\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 012 - training loss (MSE): 0.1030\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 013 - training loss (MSE): 0.0986\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 014 - training loss (MSE): 0.0960\n",
      "2025-05-23 15:36:28 [INFO]: Epoch 015 - training loss (MSE): 0.0884\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 016 - training loss (MSE): 0.0900\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 017 - training loss (MSE): 0.0837\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 018 - training loss (MSE): 0.0768\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 019 - training loss (MSE): 0.0734\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 020 - training loss (MSE): 0.0750\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 021 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 022 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 023 - training loss (MSE): 0.0686\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 024 - training loss (MSE): 0.0645\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 025 - training loss (MSE): 0.0653\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 026 - training loss (MSE): 0.0655\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 027 - training loss (MSE): 0.0627\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 028 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 029 - training loss (MSE): 0.0601\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 030 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 031 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:36:29 [INFO]: Epoch 032 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 033 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 034 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 035 - training loss (MSE): 0.0502\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 036 - training loss (MSE): 0.0524\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 037 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 038 - training loss (MSE): 0.0499\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 039 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 040 - training loss (MSE): 0.0508\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 041 - training loss (MSE): 0.0477\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 042 - training loss (MSE): 0.0488\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 043 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 044 - training loss (MSE): 0.0451\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 045 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 046 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 047 - training loss (MSE): 0.0434\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 048 - training loss (MSE): 0.0402\n",
      "2025-05-23 15:36:30 [INFO]: Epoch 049 - training loss (MSE): 0.0420\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 050 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 051 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 052 - training loss (MSE): 0.0419\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 053 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 054 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 055 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 056 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 057 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 058 - training loss (MSE): 0.0375\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 059 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 060 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 061 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 062 - training loss (MSE): 0.0367\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 063 - training loss (MSE): 0.0361\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 064 - training loss (MSE): 0.0369\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 065 - training loss (MSE): 0.0374\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 066 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:36:31 [INFO]: Epoch 067 - training loss (MSE): 0.0360\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 068 - training loss (MSE): 0.0351\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 069 - training loss (MSE): 0.0361\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 070 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 071 - training loss (MSE): 0.0348\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 072 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 073 - training loss (MSE): 0.0332\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 074 - training loss (MSE): 0.0343\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 075 - training loss (MSE): 0.0352\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 076 - training loss (MSE): 0.0353\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 077 - training loss (MSE): 0.0320\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 078 - training loss (MSE): 0.0338\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 079 - training loss (MSE): 0.0336\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 080 - training loss (MSE): 0.0338\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 081 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 082 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 083 - training loss (MSE): 0.0317\n",
      "2025-05-23 15:36:32 [INFO]: Epoch 084 - training loss (MSE): 0.0325\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 085 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 086 - training loss (MSE): 0.0331\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 087 - training loss (MSE): 0.0336\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 088 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 089 - training loss (MSE): 0.0318\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 090 - training loss (MSE): 0.0315\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 091 - training loss (MSE): 0.0304\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 092 - training loss (MSE): 0.0333\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 093 - training loss (MSE): 0.0297\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 094 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 095 - training loss (MSE): 0.0321\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 096 - training loss (MSE): 0.0312\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 097 - training loss (MSE): 0.0318\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 098 - training loss (MSE): 0.0320\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 099 - training loss (MSE): 0.0325\n",
      "2025-05-23 15:36:33 [INFO]: Epoch 100 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:36:33 [INFO]: Finished training. The best model is from epoch#93.\n",
      "[I 2025-05-23 15:36:34,019] Trial 15 finished with value: 0.2404351765385075 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0013185610521158162, 'weight_decay': 0.0004898357806420887}. Best is trial 13 with value: 0.2360340310541408.\n",
      "2025-05-23 15:36:34 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:34 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:34 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:34 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,897\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 001 - training loss (MSE): 0.6073\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 002 - training loss (MSE): 0.2817\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 003 - training loss (MSE): 0.1957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:34 [INFO]: Epoch 004 - training loss (MSE): 0.1483\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 005 - training loss (MSE): 0.1280\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 006 - training loss (MSE): 0.1032\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 007 - training loss (MSE): 0.0918\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 008 - training loss (MSE): 0.0843\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 009 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 010 - training loss (MSE): 0.0728\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 011 - training loss (MSE): 0.0655\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 012 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 013 - training loss (MSE): 0.0590\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 014 - training loss (MSE): 0.0518\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 015 - training loss (MSE): 0.0497\n",
      "2025-05-23 15:36:34 [INFO]: Epoch 016 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 017 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 018 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 019 - training loss (MSE): 0.0381\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 020 - training loss (MSE): 0.0357\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 021 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 022 - training loss (MSE): 0.0306\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 023 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 024 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 025 - training loss (MSE): 0.0266\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 026 - training loss (MSE): 0.0265\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 027 - training loss (MSE): 0.0269\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 028 - training loss (MSE): 0.0260\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 029 - training loss (MSE): 0.0254\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 030 - training loss (MSE): 0.0241\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 031 - training loss (MSE): 0.0247\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 032 - training loss (MSE): 0.0220\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 033 - training loss (MSE): 0.0249\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 034 - training loss (MSE): 0.0231\n",
      "2025-05-23 15:36:35 [INFO]: Epoch 035 - training loss (MSE): 0.0250\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 036 - training loss (MSE): 0.0234\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 037 - training loss (MSE): 0.0232\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 038 - training loss (MSE): 0.0230\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 039 - training loss (MSE): 0.0240\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 040 - training loss (MSE): 0.0241\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 041 - training loss (MSE): 0.0232\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 042 - training loss (MSE): 0.0231\n",
      "2025-05-23 15:36:36 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:36:36 [INFO]: Finished training. The best model is from epoch#32.\n",
      "[I 2025-05-23 15:36:36,516] Trial 16 finished with value: 0.2228476616622515 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 5, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.003618468418728484, 'weight_decay': 0.00015422186712091382}. Best is trial 16 with value: 0.2228476616622515.\n",
      "2025-05-23 15:36:36 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:36 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:36 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:36 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 676,689\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 001 - training loss (MSE): 0.7073\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 002 - training loss (MSE): 0.5352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:36 [INFO]: Epoch 003 - training loss (MSE): 0.4075\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 004 - training loss (MSE): 0.3076\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 005 - training loss (MSE): 0.2270\n",
      "2025-05-23 15:36:36 [INFO]: Epoch 006 - training loss (MSE): 0.2023\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 007 - training loss (MSE): 0.1947\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 008 - training loss (MSE): 0.1939\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 009 - training loss (MSE): 0.1961\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 010 - training loss (MSE): 0.1817\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 011 - training loss (MSE): 0.1833\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 012 - training loss (MSE): 0.1687\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 013 - training loss (MSE): 0.1667\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 014 - training loss (MSE): 0.1582\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 015 - training loss (MSE): 0.1530\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 016 - training loss (MSE): 0.1476\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 017 - training loss (MSE): 0.1466\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 018 - training loss (MSE): 0.1402\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 019 - training loss (MSE): 0.1323\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 020 - training loss (MSE): 0.1299\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 021 - training loss (MSE): 0.1239\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 022 - training loss (MSE): 0.1197\n",
      "2025-05-23 15:36:37 [INFO]: Epoch 023 - training loss (MSE): 0.1257\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 024 - training loss (MSE): 0.1145\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 025 - training loss (MSE): 0.1187\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 026 - training loss (MSE): 0.1117\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 027 - training loss (MSE): 0.1129\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 028 - training loss (MSE): 0.1043\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 029 - training loss (MSE): 0.1079\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 030 - training loss (MSE): 0.1026\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 031 - training loss (MSE): 0.1077\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 032 - training loss (MSE): 0.0970\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 033 - training loss (MSE): 0.0963\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 034 - training loss (MSE): 0.0973\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 035 - training loss (MSE): 0.0971\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 036 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 037 - training loss (MSE): 0.0881\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 038 - training loss (MSE): 0.0831\n",
      "2025-05-23 15:36:38 [INFO]: Epoch 039 - training loss (MSE): 0.0877\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 040 - training loss (MSE): 0.0879\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 041 - training loss (MSE): 0.0840\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 042 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 043 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 044 - training loss (MSE): 0.0802\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 045 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 046 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 047 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 048 - training loss (MSE): 0.0683\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 049 - training loss (MSE): 0.0658\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 050 - training loss (MSE): 0.0710\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 051 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 052 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 053 - training loss (MSE): 0.0680\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 054 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 055 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:36:39 [INFO]: Epoch 056 - training loss (MSE): 0.0646\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 057 - training loss (MSE): 0.0622\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 058 - training loss (MSE): 0.0635\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 059 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 060 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 061 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 062 - training loss (MSE): 0.0584\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 063 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 064 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 065 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 066 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 067 - training loss (MSE): 0.0609\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 068 - training loss (MSE): 0.0596\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 069 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 070 - training loss (MSE): 0.0574\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 071 - training loss (MSE): 0.0554\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 072 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:36:40 [INFO]: Epoch 073 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 074 - training loss (MSE): 0.0516\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 075 - training loss (MSE): 0.0517\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 076 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 077 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 078 - training loss (MSE): 0.0509\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 079 - training loss (MSE): 0.0499\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 080 - training loss (MSE): 0.0524\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 081 - training loss (MSE): 0.0509\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 082 - training loss (MSE): 0.0501\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 083 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 084 - training loss (MSE): 0.0483\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 085 - training loss (MSE): 0.0516\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 086 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 087 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 088 - training loss (MSE): 0.0497\n",
      "2025-05-23 15:36:41 [INFO]: Epoch 089 - training loss (MSE): 0.0496\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 090 - training loss (MSE): 0.0503\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 091 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 092 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 093 - training loss (MSE): 0.0469\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 094 - training loss (MSE): 0.0444\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 095 - training loss (MSE): 0.0426\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 096 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 097 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 098 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 099 - training loss (MSE): 0.0421\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 100 - training loss (MSE): 0.0456\n",
      "2025-05-23 15:36:42 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:36:42,754] Trial 17 finished with value: 0.2707434676044463 and parameters: {'n_layers': 1, 'd_model': 16, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 5, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.00030073823758577705, 'weight_decay': 0.0006527843667838277}. Best is trial 16 with value: 0.2228476616622515.\n",
      "2025-05-23 15:36:42 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:42 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:42 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:42 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 338,665\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 001 - training loss (MSE): 0.4981\n",
      "2025-05-23 15:36:42 [INFO]: Epoch 002 - training loss (MSE): 0.2581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:42 [INFO]: Epoch 003 - training loss (MSE): 0.2283\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 004 - training loss (MSE): 0.1629\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 005 - training loss (MSE): 0.1165\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 006 - training loss (MSE): 0.1050\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 007 - training loss (MSE): 0.0807\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 008 - training loss (MSE): 0.0691\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 009 - training loss (MSE): 0.0673\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 010 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 011 - training loss (MSE): 0.0622\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 012 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 013 - training loss (MSE): 0.0523\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 014 - training loss (MSE): 0.0534\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 015 - training loss (MSE): 0.0499\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 016 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 017 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 018 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 019 - training loss (MSE): 0.0467\n",
      "2025-05-23 15:36:43 [INFO]: Epoch 020 - training loss (MSE): 0.0459\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 021 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 022 - training loss (MSE): 0.0446\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 023 - training loss (MSE): 0.0428\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 024 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 025 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 026 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 027 - training loss (MSE): 0.0447\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 028 - training loss (MSE): 0.0387\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 029 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 030 - training loss (MSE): 0.0364\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 031 - training loss (MSE): 0.0388\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 032 - training loss (MSE): 0.0396\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 033 - training loss (MSE): 0.0380\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 034 - training loss (MSE): 0.0367\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 035 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 036 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:36:44 [INFO]: Epoch 037 - training loss (MSE): 0.0329\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 038 - training loss (MSE): 0.0304\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 039 - training loss (MSE): 0.0325\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 040 - training loss (MSE): 0.0310\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 041 - training loss (MSE): 0.0285\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 042 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 043 - training loss (MSE): 0.0309\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 044 - training loss (MSE): 0.0310\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 045 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 046 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 047 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 048 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 049 - training loss (MSE): 0.0285\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 050 - training loss (MSE): 0.0318\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 051 - training loss (MSE): 0.0295\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 052 - training loss (MSE): 0.0333\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 053 - training loss (MSE): 0.0287\n",
      "2025-05-23 15:36:45 [INFO]: Epoch 054 - training loss (MSE): 0.0287\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 055 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 056 - training loss (MSE): 0.0304\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 057 - training loss (MSE): 0.0291\n",
      "2025-05-23 15:36:46 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:36:46 [INFO]: Finished training. The best model is from epoch#47.\n",
      "[I 2025-05-23 15:36:46,279] Trial 18 finished with value: 0.23608377775213404 and parameters: {'n_layers': 1, 'd_model': 8, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 5, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.008215759484316036, 'weight_decay': 0.00016067793405038876}. Best is trial 16 with value: 0.2228476616622515.\n",
      "2025-05-23 15:36:46 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:46 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:46 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:46 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,897\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 001 - training loss (MSE): 0.7743\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 002 - training loss (MSE): 0.4275\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 003 - training loss (MSE): 0.3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:46 [INFO]: Epoch 004 - training loss (MSE): 0.2833\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 005 - training loss (MSE): 0.2467\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 006 - training loss (MSE): 0.1983\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 007 - training loss (MSE): 0.1977\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 008 - training loss (MSE): 0.1720\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 009 - training loss (MSE): 0.1497\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 010 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:36:46 [INFO]: Epoch 011 - training loss (MSE): 0.1359\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 012 - training loss (MSE): 0.1220\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 013 - training loss (MSE): 0.1156\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 014 - training loss (MSE): 0.1024\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 015 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 016 - training loss (MSE): 0.0956\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 017 - training loss (MSE): 0.0939\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 018 - training loss (MSE): 0.0910\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 019 - training loss (MSE): 0.0829\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 020 - training loss (MSE): 0.0809\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 021 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 022 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 023 - training loss (MSE): 0.0773\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 024 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 025 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 026 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 027 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 028 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:36:47 [INFO]: Epoch 029 - training loss (MSE): 0.0689\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 030 - training loss (MSE): 0.0633\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 031 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 032 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 033 - training loss (MSE): 0.0656\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 034 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 035 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 036 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 037 - training loss (MSE): 0.0598\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 038 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 039 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 040 - training loss (MSE): 0.0627\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 041 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 042 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 043 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 044 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 045 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 046 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 047 - training loss (MSE): 0.0538\n",
      "2025-05-23 15:36:48 [INFO]: Epoch 048 - training loss (MSE): 0.0514\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 049 - training loss (MSE): 0.0504\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 050 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 051 - training loss (MSE): 0.0520\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 052 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 053 - training loss (MSE): 0.0479\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 054 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 055 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 056 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 057 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 058 - training loss (MSE): 0.0457\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 059 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 060 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 061 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 062 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 063 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 064 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:36:49 [INFO]: Epoch 065 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 066 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 067 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 068 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 069 - training loss (MSE): 0.0429\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 070 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 071 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 072 - training loss (MSE): 0.0408\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 073 - training loss (MSE): 0.0419\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 074 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 075 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 076 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 077 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 078 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 079 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 080 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 081 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 082 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:36:50 [INFO]: Epoch 083 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 084 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 085 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 086 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 087 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 088 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 089 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 090 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 091 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 092 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 093 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 094 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 095 - training loss (MSE): 0.0416\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 096 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:36:51 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:36:51 [INFO]: Finished training. The best model is from epoch#86.\n",
      "[I 2025-05-23 15:36:51,800] Trial 19 finished with value: 0.17497435732336525 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 5, 'dropout': 0.5, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0023829279010544025, 'weight_decay': 0.0003987175700559951}. Best is trial 19 with value: 0.17497435732336525.\n"
     ]
    }
   ],
   "source": [
    "best_params, logs = tsi.perform_hyperparameter_tuning(missing_df=df_missing,\n",
    "                                                      groundTruth_df=df_scaled,\n",
    "                                                      algorithm=algorithm,\n",
    "                                                      params=params, \n",
    "                                                      n_trials=n_trials,\n",
    "                                                      dimension_column=dimension_column,\n",
    "                                                      spatial_x_column=spatial_x_column,\n",
    "                                                      spatial_y_column=spatial_y_column,\n",
    "                                                      is_multivariate=is_multivariate, \n",
    "                                                      areaVStime=areaVStime,\n",
    "                                                      preprocessing=preprocessing, \n",
    "                                                      index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:51.802424Z",
     "start_time": "2025-05-23T12:34:25.201761Z"
    }
   },
   "id": "da9301f0e11a47ef",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'TimesNet': {'n_layers': 2,\n  'd_model': 32,\n  'd_ffn': 64,\n  'n_heads': 3,\n  'top_k': 1,\n  'n_kernels': 5,\n  'dropout': 0.5,\n  'epochs': 100,\n  'batch_size': 32,\n  'attn_dropout': 0,\n  'apply_nonstationary_norm': True,\n  'num_workers': 0,\n  'patience': 10,\n  'lr': 0.0023829279010544025,\n  'weight_decay': 0.0003987175700559951,\n  'device': ['cuda']}}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:51.807026Z",
     "start_time": "2025-05-23T12:36:51.803366Z"
    }
   },
   "id": "ad7422675d76dda0",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'TimesNet': {'trial_number_0': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 128,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 3,\n     'n_kernels': 4,\n     'dropout': 0.4,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.1,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.005431095176981607,\n     'weight_decay': 0.0001106913243495668,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3669679136636915,\n    'Mean square error': 0.25373720576500797,\n    'Root mean square error': 0.5037233424857418,\n    'Mean relative error': 0.4658960717641372,\n    'Euclidean Distance': 20.374348299554352,\n    'r2 score': 0.7258371490771874,\n    'Execution Time': 14.542778253555298}},\n  'trial_number_1': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 128,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0008505207524153307,\n     'weight_decay': 0.0003414234544289426,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3578203303976919,\n    'Mean square error': 0.2375157177107524,\n    'Root mean square error': 0.4873558430046288,\n    'Mean relative error': 0.4542824593716643,\n    'Euclidean Distance': 19.712323916139137,\n    'r2 score': 0.7433644541397484,\n    'Execution Time': 10.927663087844849}},\n  'trial_number_2': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 16,\n     'd_ffn': 128,\n     'n_heads': 1,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.003964743616689867,\n     'weight_decay': 0.0004659747070765304,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.36450131428810595,\n    'Mean square error': 0.24379356992735043,\n    'Root mean square error': 0.4937545644622948,\n    'Mean relative error': 0.4627645201572728,\n    'Euclidean Distance': 19.971136182028943,\n    'r2 score': 0.7365812397657057,\n    'Execution Time': 14.189138650894165}},\n  'trial_number_3': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 64,\n     'd_ffn': 64,\n     'n_heads': 1,\n     'top_k': 3,\n     'n_kernels': 4,\n     'dropout': 0.4,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.00014632223035577693,\n     'weight_decay': 0.000331603591492567,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.4180257325063564,\n    'Mean square error': 0.3153886965406908,\n    'Root mean square error': 0.5615947796594006,\n    'Mean relative error': 0.5307181893006658,\n    'Euclidean Distance': 22.715103071317344,\n    'r2 score': 0.6592227618660484,\n    'Execution Time': 13.684444665908813}},\n  'trial_number_4': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 1,\n     'top_k': 3,\n     'n_kernels': 3,\n     'dropout': 0.5,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0017597216454587137,\n     'weight_decay': 0.0009307823237220275,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.35286878595068244,\n    'Mean square error': 0.2517844566290508,\n    'Root mean square error': 0.501781283657582,\n    'Mean relative error': 0.4479960647820234,\n    'Euclidean Distance': 20.29579688125419,\n    'r2 score': 0.7279470929801202,\n    'Execution Time': 8.380699634552002}},\n  'trial_number_5': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 32,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0010035341501560358,\n     'weight_decay': 0.000815755185102926,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.37748235424635895,\n    'Mean square error': 0.25813495810722314,\n    'Root mean square error': 0.5080698358564727,\n    'Mean relative error': 0.4792450223995095,\n    'Euclidean Distance': 20.550153076398658,\n    'r2 score': 0.7210853811361833,\n    'Execution Time': 3.3280277252197266}},\n  'trial_number_6': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 64,\n     'd_ffn': 32,\n     'n_heads': 3,\n     'top_k': 2,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 7.411200761456233e-05,\n     'weight_decay': 0.0009705778096597715,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.4386238939048915,\n    'Mean square error': 0.33889591923680673,\n    'Root mean square error': 0.5821476782027106,\n    'Mean relative error': 0.5568692562572609,\n    'Euclidean Distance': 23.546416370042728,\n    'r2 score': 0.633823226263007,\n    'Execution Time': 9.307136535644531}},\n  'trial_number_7': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0038565644906092124,\n     'weight_decay': 0.00023577029727229654,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3481021446752341,\n    'Mean square error': 0.23652139200056946,\n    'Root mean square error': 0.4863346502158462,\n    'Mean relative error': 0.44194442003856665,\n    'Euclidean Distance': 19.671019224049672,\n    'r2 score': 0.7444388222862237,\n    'Execution Time': 3.1718037128448486}},\n  'trial_number_8': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 64,\n     'd_ffn': 32,\n     'n_heads': 1,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.1,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0002128524987103289,\n     'weight_decay': 0.0007940117135494467,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3762232184229656,\n    'Mean square error': 0.2513832391978127,\n    'Root mean square error': 0.5013813311221437,\n    'Mean relative error': 0.4776464454883028,\n    'Euclidean Distance': 20.279619802343973,\n    'r2 score': 0.7283806080984748,\n    'Execution Time': 5.686073064804077}},\n  'trial_number_9': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 64,\n     'd_ffn': 8,\n     'n_heads': 3,\n     'top_k': 3,\n     'n_kernels': 3,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.007691181798461813,\n     'weight_decay': 0.0006470205285361298,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.4054716906796316,\n    'Mean square error': 0.30940735395611757,\n    'Root mean square error': 0.5562439698155096,\n    'Mean relative error': 0.5147797964492572,\n    'Euclidean Distance': 22.498676207106246,\n    'r2 score': 0.6656855978162929,\n    'Execution Time': 2.001021146774292}},\n  'trial_number_10': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 8,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0022922125037629845,\n     'weight_decay': 6.245289685592381e-05,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3847431532051987,\n    'Mean square error': 0.27148234572566654,\n    'Root mean square error': 0.5210396776884334,\n    'Mean relative error': 0.48846320629744217,\n    'Euclidean Distance': 21.07475071281249,\n    'r2 score': 0.7066635393301639,\n    'Execution Time': 3.514573097229004}},\n  'trial_number_11': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 128,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0005597413939486375,\n     'weight_decay': 0.0002858718430285778,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.36267531940269226,\n    'Mean square error': 0.24093627097405948,\n    'Root mean square error': 0.49085259597363795,\n    'Mean relative error': 0.46044626885382156,\n    'Euclidean Distance': 19.853758820776523,\n    'r2 score': 0.7396685490336201,\n    'Execution Time': 10.792407035827637}},\n  'trial_number_12': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 128,\n     'd_ffn': 8,\n     'n_heads': 3,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0004127289522954379,\n     'weight_decay': 0.00026744327270321225,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3920563241500039,\n    'Mean square error': 0.27364734372318245,\n    'Root mean square error': 0.5231131270797765,\n    'Mean relative error': 0.49774788075660176,\n    'Euclidean Distance': 21.158616550500813,\n    'r2 score': 0.7043242606995366,\n    'Execution Time': 12.646118879318237}},\n  'trial_number_13': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 32,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0012427183476582465,\n     'weight_decay': 0.0005072807145484922,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.35662433136863547,\n    'Mean square error': 0.2360340310541408,\n    'Root mean square error': 0.48583333670523354,\n    'Mean relative error': 0.45276403983490404,\n    'Euclidean Distance': 19.650742347417175,\n    'r2 score': 0.7449654154048736,\n    'Execution Time': 7.377415657043457}},\n  'trial_number_14': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 32,\n     'd_ffn': 128,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 3,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.002365300835186382,\n     'weight_decay': 0.0005401266377398488,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.37004609608662975,\n    'Mean square error': 0.27746988812981843,\n    'Root mean square error': 0.5267541059449071,\n    'Mean relative error': 0.4698040785560733,\n    'Euclidean Distance': 21.305885031614697,\n    'r2 score': 0.7001940044797494,\n    'Execution Time': 3.1511788368225098}},\n  'trial_number_15': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0013185610521158162,\n     'weight_decay': 0.0004898357806420887,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.35555747475080307,\n    'Mean square error': 0.2404351765385075,\n    'Root mean square error': 0.4903418975964704,\n    'Mean relative error': 0.45140957725418035,\n    'Euclidean Distance': 19.83310234978377,\n    'r2 score': 0.7402099811764479,\n    'Execution Time': 6.000720262527466}},\n  'trial_number_16': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.003618468418728484,\n     'weight_decay': 0.00015422186712091382,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.34327428804448235,\n    'Mean square error': 0.2228476616622515,\n    'Root mean square error': 0.47206743338452345,\n    'Mean relative error': 0.435815057346195,\n    'Euclidean Distance': 19.09394601645882,\n    'r2 score': 0.759213277143959,\n    'Execution Time': 2.487222671508789}},\n  'trial_number_17': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 16,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.00030073823758577705,\n     'weight_decay': 0.0006527843667838277,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3857040987523514,\n    'Mean square error': 0.2707434676044463,\n    'Root mean square error': 0.5203301525036256,\n    'Mean relative error': 0.48968320602746757,\n    'Euclidean Distance': 21.046052195147535,\n    'r2 score': 0.7074618965580189,\n    'Execution Time': 6.228282928466797}},\n  'trial_number_18': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 8,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.008215759484316036,\n     'weight_decay': 0.00016067793405038876,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3517178625663072,\n    'Mean square error': 0.23608377775213404,\n    'Root mean square error': 0.485884531295383,\n    'Mean relative error': 0.44653487249867485,\n    'Euclidean Distance': 19.652813040440073,\n    'r2 score': 0.7449116641368848,\n    'Execution Time': 3.5152108669281006}},\n  'trial_number_19': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.5,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0023829279010544025,\n     'weight_decay': 0.0003987175700559951,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.30304744827791186,\n    'Mean square error': 0.17497435732336525,\n    'Root mean square error': 0.41829936328348055,\n    'Mean relative error': 0.3847437622031916,\n    'Euclidean Distance': 16.919162171367283,\n    'r2 score': 0.810940344765252,\n    'Execution Time': 5.511841297149658}},\n  'best_trial': {'number': 19,\n   'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.5,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0023829279010544025,\n     'weight_decay': 0.0003987175700559951,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.30304744827791186,\n    'Mean square error': 0.17497435732336525,\n    'Root mean square error': 0.41829936328348055,\n    'Mean relative error': 0.3847437622031916,\n    'Euclidean Distance': 16.919162171367283,\n    'r2 score': 0.810940344765252,\n    'Execution Time': 5.511841297149658}}}}"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:51.828404Z",
     "start_time": "2025-05-23T12:36:51.808047Z"
    }
   },
   "id": "fbd2cbbc64458d17",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation - with tuned parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3bd21c41affc66"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'dimension_column' : 'Dimension',\n",
    "    'spatial_x_column': 'Spatial_X',\n",
    "    'spatial_y_column' : 'Spatial_Y',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': True,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"TimesNet\"],\n",
    "    \"params\": best_params\n",
    "}\n",
    "\n",
    "dimension_column = parameters['dimension_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "spatial_x_column = parameters['spatial_x_column']\n",
    "spatial_y_column = parameters['spatial_y_column']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:51.837857Z",
     "start_time": "2025-05-23T12:36:51.829465Z"
    }
   },
   "id": "bc57837f1a322fbd",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:51 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:36:51 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:36:51 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:36:51 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,897\n",
      "2025-05-23 15:36:51 [INFO]: Epoch 001 - training loss (MSE): 0.7743\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 002 - training loss (MSE): 0.4275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:36:52 [INFO]: Epoch 003 - training loss (MSE): 0.3300\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 004 - training loss (MSE): 0.2833\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 005 - training loss (MSE): 0.2467\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 006 - training loss (MSE): 0.1983\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 007 - training loss (MSE): 0.1977\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 008 - training loss (MSE): 0.1720\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 009 - training loss (MSE): 0.1497\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 010 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 011 - training loss (MSE): 0.1359\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 012 - training loss (MSE): 0.1220\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 013 - training loss (MSE): 0.1156\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 014 - training loss (MSE): 0.1024\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 015 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 016 - training loss (MSE): 0.0956\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 017 - training loss (MSE): 0.0939\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 018 - training loss (MSE): 0.0910\n",
      "2025-05-23 15:36:52 [INFO]: Epoch 019 - training loss (MSE): 0.0829\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 020 - training loss (MSE): 0.0809\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 021 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 022 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 023 - training loss (MSE): 0.0773\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 024 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 025 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 026 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 027 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 028 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 029 - training loss (MSE): 0.0689\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 030 - training loss (MSE): 0.0633\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 031 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 032 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 033 - training loss (MSE): 0.0656\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 034 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 035 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 036 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 037 - training loss (MSE): 0.0598\n",
      "2025-05-23 15:36:53 [INFO]: Epoch 038 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 039 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 040 - training loss (MSE): 0.0627\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 041 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 042 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 043 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 044 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 045 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 046 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 047 - training loss (MSE): 0.0538\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 048 - training loss (MSE): 0.0514\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 049 - training loss (MSE): 0.0504\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 050 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 051 - training loss (MSE): 0.0520\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 052 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 053 - training loss (MSE): 0.0479\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 054 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 055 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:36:54 [INFO]: Epoch 056 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 057 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 058 - training loss (MSE): 0.0457\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 059 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 060 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 061 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 062 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 063 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 064 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 065 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 066 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 067 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 068 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 069 - training loss (MSE): 0.0429\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 070 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 071 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 072 - training loss (MSE): 0.0408\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 073 - training loss (MSE): 0.0419\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 074 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:36:55 [INFO]: Epoch 075 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 076 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 077 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 078 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 079 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 080 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 081 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 082 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 083 - training loss (MSE): 0.0401\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 084 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 085 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 086 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 087 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 088 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 089 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 090 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 091 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 092 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:36:56 [INFO]: Epoch 093 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:36:57 [INFO]: Epoch 094 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:36:57 [INFO]: Epoch 095 - training loss (MSE): 0.0416\n",
      "2025-05-23 15:36:57 [INFO]: Epoch 096 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:36:57 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:36:57 [INFO]: Finished training. The best model is from epoch#86.\n"
     ]
    }
   ],
   "source": [
    "dict_of_imputed_dfs = tsi.run_imputation(missing = df_missing, \n",
    "                                         algorithms=algorithms, \n",
    "                                         params=params, \n",
    "                                         dimension_column=dimension_column,\n",
    "                                         spatial_x_column=spatial_x_column,\n",
    "                                         spatial_y_column=spatial_y_column, \n",
    "                                         header=header, \n",
    "                                         sep=sep, \n",
    "                                         is_multivariate=is_multivariate, \n",
    "                                         areaVStime=areaVStime, \n",
    "                                         preprocessing=preprocessing, \n",
    "                                         index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:57.278982Z",
     "start_time": "2025-05-23T12:36:51.838973Z"
    }
   },
   "id": "c7666d0427c16c08",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "                            Dimension  Spatial_X  Spatial_Y  \\\n0                 3i Group PLC_035999         96          0   \n1                Admiral Group_036346         97          1   \n2           Anglo American PLC_035918         98          2   \n3              Antofagasta PLC_028149         99          3   \n4                Ashtead Group_028090        100          4   \n..                                ...        ...        ...   \n91                Unilever PLC_035922        187         91   \n92  United Utilities Group PLC_036341        188         92   \n93          Vodafone Group PLC_035943        189         93   \n94               Whitbread PLC_035895        190         94   \n95                     WPP PLC_035947        191         95   \n\n    2017-01-02 00:00:00  2017-01-03 00:00:00  2017-01-04 00:00:00  \\\n0             -2.152305            -2.053810            -1.992251   \n1             -1.347635            -1.189826            -1.325091   \n2             -1.294240            -1.286110            -1.365373   \n3             -2.295047            -2.136391            -2.120526   \n4             -1.199292            -1.136860            -1.155222   \n..                  ...                  ...                  ...   \n91            -2.869013            -2.931364            -2.886828   \n92             0.733771             0.710001             0.638689   \n93            -0.007477             0.100982             0.241340   \n94            -0.898946            -0.881773            -0.713477   \n95             1.592327             1.624480             1.578037   \n\n    2017-01-05 00:00:00  2017-01-06 00:00:00  2017-01-09 00:00:00  \\\n0             -1.924536            -1.912224            -1.887600   \n1             -1.336363            -1.313819            -1.336363   \n2             -1.355211            -1.395858            -1.316596   \n3             -2.035909            -2.094083            -1.988312   \n4             -1.122170            -1.155222            -1.188275   \n..                  ...                  ...                  ...   \n91            -2.845854            -2.863669            -2.683742   \n92             0.724263             0.695738             0.705247   \n93             0.400838             0.481651             0.337039   \n94            -0.469618            -0.332233            -0.593265   \n95             1.642343             1.678069             1.767384   \n\n    2017-01-10 00:00:00  ...  2018-12-18 00:00:00  2018-12-19 00:00:00  \\\n0             -2.004563  ...            -1.356960            -1.273239   \n1             -1.877424  ...             0.246545             0.260844   \n2             -0.979221  ...             0.951131             1.116160   \n3             -1.787348  ...            -1.290226            -1.120993   \n4             -1.122170  ...            -0.920183            -0.918347   \n..                  ...  ...                  ...                  ...   \n91            -2.717589  ...             0.399962             0.417777   \n92             0.633935  ...            -0.762826            -0.705777   \n93             0.434864  ...            -1.793430            -1.725377   \n94             0.045575  ...             1.405687             1.467510   \n95             1.813828  ...            -1.785209            -1.708041   \n\n    2018-12-20 00:00:00  2018-12-21 00:00:00  2018-12-24 00:00:00  \\\n0             -1.297863            -1.098411            -1.509627   \n1              0.323943             0.343178             0.405175   \n2              0.913735             1.110469             1.060066   \n3             -1.427728            -1.300803            -1.271187   \n4             -1.098299            -1.120334            -0.967925   \n..                  ...                  ...                  ...   \n91             0.378584             0.373240             0.237849   \n92            -0.606891            -0.519415            -0.878827   \n93            -1.704111            -1.841067            -1.936340   \n94             1.481249             1.453772             1.656414   \n95            -1.798785            -1.805216            -1.858805   \n\n    2018-12-25 00:00:00  2018-12-26 00:00:00  2018-12-27 00:00:00  \\\n0             -1.509627            -1.509627            -1.310175   \n1              0.405175             0.405175             0.433355   \n2              1.060066             1.060066             0.958448   \n3             -1.271187            -1.271187            -1.406574   \n4             -0.967925            -0.967925            -1.175421   \n..                  ...                  ...                  ...   \n91             0.237849             0.237849            -0.063217   \n92            -0.878827            -0.878827            -1.002434   \n93            -1.936340            -1.936340            -2.063088   \n94             1.656414             1.656414             1.501856   \n95            -1.858805            -1.858805            -1.929542   \n\n    2018-12-28 00:00:00  2018-12-31 00:00:00  \n0             -1.027002            -1.297863  \n1              0.861694             1.132225  \n2              1.121038             1.095023  \n3             -1.173878            -1.150609  \n4             -0.978943            -0.989960  \n..                  ...                  ...  \n91             0.097114             0.038326  \n92            -0.802761            -0.833187  \n93            -1.932938            -2.004393  \n94             1.766322             1.859057  \n95            -1.848087            -1.870951  \n\n[96 rows x 524 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dimension</th>\n      <th>Spatial_X</th>\n      <th>Spatial_Y</th>\n      <th>2017-01-02 00:00:00</th>\n      <th>2017-01-03 00:00:00</th>\n      <th>2017-01-04 00:00:00</th>\n      <th>2017-01-05 00:00:00</th>\n      <th>2017-01-06 00:00:00</th>\n      <th>2017-01-09 00:00:00</th>\n      <th>2017-01-10 00:00:00</th>\n      <th>...</th>\n      <th>2018-12-18 00:00:00</th>\n      <th>2018-12-19 00:00:00</th>\n      <th>2018-12-20 00:00:00</th>\n      <th>2018-12-21 00:00:00</th>\n      <th>2018-12-24 00:00:00</th>\n      <th>2018-12-25 00:00:00</th>\n      <th>2018-12-26 00:00:00</th>\n      <th>2018-12-27 00:00:00</th>\n      <th>2018-12-28 00:00:00</th>\n      <th>2018-12-31 00:00:00</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3i Group PLC_035999</td>\n      <td>96</td>\n      <td>0</td>\n      <td>-2.152305</td>\n      <td>-2.053810</td>\n      <td>-1.992251</td>\n      <td>-1.924536</td>\n      <td>-1.912224</td>\n      <td>-1.887600</td>\n      <td>-2.004563</td>\n      <td>...</td>\n      <td>-1.356960</td>\n      <td>-1.273239</td>\n      <td>-1.297863</td>\n      <td>-1.098411</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.509627</td>\n      <td>-1.310175</td>\n      <td>-1.027002</td>\n      <td>-1.297863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Admiral Group_036346</td>\n      <td>97</td>\n      <td>1</td>\n      <td>-1.347635</td>\n      <td>-1.189826</td>\n      <td>-1.325091</td>\n      <td>-1.336363</td>\n      <td>-1.313819</td>\n      <td>-1.336363</td>\n      <td>-1.877424</td>\n      <td>...</td>\n      <td>0.246545</td>\n      <td>0.260844</td>\n      <td>0.323943</td>\n      <td>0.343178</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.405175</td>\n      <td>0.433355</td>\n      <td>0.861694</td>\n      <td>1.132225</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Anglo American PLC_035918</td>\n      <td>98</td>\n      <td>2</td>\n      <td>-1.294240</td>\n      <td>-1.286110</td>\n      <td>-1.365373</td>\n      <td>-1.355211</td>\n      <td>-1.395858</td>\n      <td>-1.316596</td>\n      <td>-0.979221</td>\n      <td>...</td>\n      <td>0.951131</td>\n      <td>1.116160</td>\n      <td>0.913735</td>\n      <td>1.110469</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>1.060066</td>\n      <td>0.958448</td>\n      <td>1.121038</td>\n      <td>1.095023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Antofagasta PLC_028149</td>\n      <td>99</td>\n      <td>3</td>\n      <td>-2.295047</td>\n      <td>-2.136391</td>\n      <td>-2.120526</td>\n      <td>-2.035909</td>\n      <td>-2.094083</td>\n      <td>-1.988312</td>\n      <td>-1.787348</td>\n      <td>...</td>\n      <td>-1.290226</td>\n      <td>-1.120993</td>\n      <td>-1.427728</td>\n      <td>-1.300803</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.271187</td>\n      <td>-1.406574</td>\n      <td>-1.173878</td>\n      <td>-1.150609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Ashtead Group_028090</td>\n      <td>100</td>\n      <td>4</td>\n      <td>-1.199292</td>\n      <td>-1.136860</td>\n      <td>-1.155222</td>\n      <td>-1.122170</td>\n      <td>-1.155222</td>\n      <td>-1.188275</td>\n      <td>-1.122170</td>\n      <td>...</td>\n      <td>-0.920183</td>\n      <td>-0.918347</td>\n      <td>-1.098299</td>\n      <td>-1.120334</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-0.967925</td>\n      <td>-1.175421</td>\n      <td>-0.978943</td>\n      <td>-0.989960</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Unilever PLC_035922</td>\n      <td>187</td>\n      <td>91</td>\n      <td>-2.869013</td>\n      <td>-2.931364</td>\n      <td>-2.886828</td>\n      <td>-2.845854</td>\n      <td>-2.863669</td>\n      <td>-2.683742</td>\n      <td>-2.717589</td>\n      <td>...</td>\n      <td>0.399962</td>\n      <td>0.417777</td>\n      <td>0.378584</td>\n      <td>0.373240</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>0.237849</td>\n      <td>-0.063217</td>\n      <td>0.097114</td>\n      <td>0.038326</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>United Utilities Group PLC_036341</td>\n      <td>188</td>\n      <td>92</td>\n      <td>0.733771</td>\n      <td>0.710001</td>\n      <td>0.638689</td>\n      <td>0.724263</td>\n      <td>0.695738</td>\n      <td>0.705247</td>\n      <td>0.633935</td>\n      <td>...</td>\n      <td>-0.762826</td>\n      <td>-0.705777</td>\n      <td>-0.606891</td>\n      <td>-0.519415</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-0.878827</td>\n      <td>-1.002434</td>\n      <td>-0.802761</td>\n      <td>-0.833187</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Vodafone Group PLC_035943</td>\n      <td>189</td>\n      <td>93</td>\n      <td>-0.007477</td>\n      <td>0.100982</td>\n      <td>0.241340</td>\n      <td>0.400838</td>\n      <td>0.481651</td>\n      <td>0.337039</td>\n      <td>0.434864</td>\n      <td>...</td>\n      <td>-1.793430</td>\n      <td>-1.725377</td>\n      <td>-1.704111</td>\n      <td>-1.841067</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-1.936340</td>\n      <td>-2.063088</td>\n      <td>-1.932938</td>\n      <td>-2.004393</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Whitbread PLC_035895</td>\n      <td>190</td>\n      <td>94</td>\n      <td>-0.898946</td>\n      <td>-0.881773</td>\n      <td>-0.713477</td>\n      <td>-0.469618</td>\n      <td>-0.332233</td>\n      <td>-0.593265</td>\n      <td>0.045575</td>\n      <td>...</td>\n      <td>1.405687</td>\n      <td>1.467510</td>\n      <td>1.481249</td>\n      <td>1.453772</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.656414</td>\n      <td>1.501856</td>\n      <td>1.766322</td>\n      <td>1.859057</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>WPP PLC_035947</td>\n      <td>191</td>\n      <td>95</td>\n      <td>1.592327</td>\n      <td>1.624480</td>\n      <td>1.578037</td>\n      <td>1.642343</td>\n      <td>1.678069</td>\n      <td>1.767384</td>\n      <td>1.813828</td>\n      <td>...</td>\n      <td>-1.785209</td>\n      <td>-1.708041</td>\n      <td>-1.798785</td>\n      <td>-1.805216</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.858805</td>\n      <td>-1.929542</td>\n      <td>-1.848087</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>96 rows × 524 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df =dict_of_imputed_dfs['TimesNet']\n",
    "missing = imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:57.290880Z",
     "start_time": "2025-05-23T12:36:57.279845Z"
    }
   },
   "id": "816733b7367c5cd5",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7a302b7a6326493"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Missing value percentage': 3.2709532949456173,\n 'Mean absolute error': 0.3030422662438388,\n 'Mean square error': 0.174970485363991,\n 'Root mean square error': 0.4182947350421602,\n 'Mean relative error': 0.3847371831829871,\n 'Euclidean Distance': 16.918974970591137,\n 'r2 score': 0.8109445284143053}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_scaled = df_scaled.set_index(df_scaled.columns[0])\n",
    "new_df_scaled = new_df_scaled.iloc[:, 2:].T\n",
    "new_df_scaled.columns.name = ''\n",
    "new_df_scaled.index.name = 'Date'\n",
    "\n",
    "new_df_missing = df_missing.set_index(df_missing.columns[0])\n",
    "new_df_missing = new_df_missing.iloc[:, 2:].T\n",
    "new_df_missing.columns.name = ''\n",
    "new_df_missing.index.name = 'Date'\n",
    "\n",
    "new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "new_imputed_df = new_imputed_df.iloc[:, 2:].T\n",
    "new_imputed_df.columns.name = ''\n",
    "new_imputed_df.index.name = 'Date'\n",
    "\n",
    "hyperTuned_metrics = tsi.compute_metrics(new_df_scaled, new_df_missing, new_imputed_df)\n",
    "hyperTuned_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:57.314438Z",
     "start_time": "2025-05-23T12:36:57.291789Z"
    }
   },
   "id": "313f936cb3f58ec5",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Missing value percentage': 3.2709532949456173,\n 'Mean absolute error': 0.5507465132903318,\n 'Mean square error': 0.5909495169889909,\n 'Root mean square error': 0.7687324092224751,\n 'Mean relative error': 0.699218181006729,\n 'Euclidean Distance': 31.093301686922693,\n 'r2 score': 0.361479512471624}"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:57.321229Z",
     "start_time": "2025-05-23T12:36:57.315237Z"
    }
   },
   "id": "de87e346c9eb2399",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:36:57.324328Z",
     "start_time": "2025-05-23T12:36:57.322456Z"
    }
   },
   "id": "2cf48c2fa4bf9c76",
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
