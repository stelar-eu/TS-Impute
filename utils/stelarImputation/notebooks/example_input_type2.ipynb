{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import stelarImputation as tsi\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:42.839109Z",
     "start_time": "2025-05-23T12:40:42.835652Z"
    }
   },
   "id": "7d06e22ee8dfb7bd",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scale Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c525b35375bb479d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_input = '../datasets/example_input_type2.csv'\n",
    "\n",
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:42.925721Z",
     "start_time": "2025-05-23T12:40:42.922571Z"
    }
   },
   "id": "97227f022732d820",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0    2017-01-02                -2.152305                 -1.347635   \n1    2017-01-03                -2.053810                 -1.189826   \n2    2017-01-04                -1.992251                 -1.325091   \n3    2017-01-05                -1.924536                 -1.336363   \n4    2017-01-06                -1.912224                 -1.313819   \n..          ...                      ...                       ...   \n516  2018-12-25                -1.509627                  0.405175   \n517  2018-12-26                -1.509627                  0.405175   \n518  2018-12-27                -1.310175                  0.433355   \n519  2018-12-28                -1.027002                  0.861694   \n520  2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                   1.753102  ...                          -0.465855   \n4                   1.890973  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-2.152305</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-2.053810</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-1.992251</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-1.924536</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>1.753102</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-1.912224</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>1.890973</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled, scaler = tsi.dataframe_scaler(df_input=df_input, \n",
    "                                 time_column=time_column, \n",
    "                                 header=header, \n",
    "                                 sep=sep, \n",
    "                                 preprocessing=preprocessing, \n",
    "                                 index=index)\n",
    "\n",
    "df_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:42.956397Z",
     "start_time": "2025-05-23T12:40:42.926851Z"
    }
   },
   "id": "b5f33d6fe30d0cc0",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gap Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deb58fa26858ef5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    'train_params': {\n",
    "        \"gap_type\": \"random\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 10,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5,\n",
    "        \"random_seed\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:42.960629Z",
     "start_time": "2025-05-23T12:40:42.957670Z"
    }
   },
   "id": "88fa2ad58b835c1e",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 1636\n"
     ]
    },
    {
     "data": {
      "text/plain": "           time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0    2017-01-02                -2.152305                 -1.347635   \n1    2017-01-03                -2.053810                 -1.189826   \n2    2017-01-04                -1.992251                 -1.325091   \n3    2017-01-05                -1.924536                 -1.336363   \n4    2017-01-06                -1.912224                 -1.313819   \n..          ...                      ...                       ...   \n516  2018-12-25                -1.509627                  0.405175   \n517  2018-12-26                -1.509627                  0.405175   \n518  2018-12-27                -1.310175                  0.433355   \n519  2018-12-28                -1.027002                  0.861694   \n520  2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                        NaN  ...                          -0.465855   \n4                        NaN  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-2.152305</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-2.053810</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-1.992251</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-1.924536</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-1.912224</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_missing = tsi.run_gap_generation(ground_truth=df_scaled, \n",
    "                                    train_params=train_params, \n",
    "                                    time_column=time_column, \n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "\n",
    "missing = df_missing.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "df_missing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:42.981566Z",
     "start_time": "2025-05-23T12:40:42.961652Z"
    }
   },
   "id": "658dfb14d4145728",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06dd72064d4e510"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": tsi.default_imputation_params()\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:42.986041Z",
     "start_time": "2025-05-23T12:40:42.983020Z"
    }
   },
   "id": "eab173986d693e6b",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:40:50 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:40:50 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:40:50 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:40:50 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:40:50 [INFO]: Epoch 001 - training loss (MSE): 0.9286\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 002 - training loss (MSE): 0.3693\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 003 - training loss (MSE): 0.3291\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 004 - training loss (MSE): 0.2408\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 005 - training loss (MSE): 0.1910\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 006 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 007 - training loss (MSE): 0.1611\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 008 - training loss (MSE): 0.1604\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 009 - training loss (MSE): 0.1511\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 010 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:40:50 [INFO]: Epoch 011 - training loss (MSE): 0.1371\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 012 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 013 - training loss (MSE): 0.1310\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 014 - training loss (MSE): 0.1254\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 015 - training loss (MSE): 0.1198\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 016 - training loss (MSE): 0.1193\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 017 - training loss (MSE): 0.1192\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 018 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 019 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 020 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 021 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 022 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 023 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 024 - training loss (MSE): 0.1003\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 025 - training loss (MSE): 0.0950\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 026 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 027 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 028 - training loss (MSE): 0.0832\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 029 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 030 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 031 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 032 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 033 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 035 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 036 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:40:51 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 038 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 039 - training loss (MSE): 0.0698\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 040 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 041 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 042 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 043 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 044 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 045 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 046 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:40:52 [INFO]: Epoch 047 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:40:52 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:40:52 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    }
   ],
   "source": [
    "dict_of_imputed_dfs = tsi.run_imputation(missing = df_missing, \n",
    "                                         algorithms=algorithms, \n",
    "                                         params=params, \n",
    "                                         time_column=time_column,\n",
    "                                         header=header, \n",
    "                                         sep=sep, \n",
    "                                         is_multivariate=is_multivariate, \n",
    "                                         areaVStime=areaVStime, \n",
    "                                         preprocessing=preprocessing, \n",
    "                                         index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:52.478036Z",
     "start_time": "2025-05-23T12:40:42.987055Z"
    }
   },
   "id": "bf4f82d7f54c335f",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "           time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0    2017-01-02                -2.152305                 -1.347635   \n1    2017-01-03                -2.053810                 -1.189826   \n2    2017-01-04                -1.992251                 -1.325091   \n3    2017-01-05                -1.924536                 -1.336363   \n4    2017-01-06                -1.912224                 -1.313819   \n..          ...                      ...                       ...   \n516  2018-12-25                -1.509627                  0.405175   \n517  2018-12-26                -1.509627                  0.405175   \n518  2018-12-27                -1.310175                  0.433355   \n519  2018-12-28                -1.027002                  0.861694   \n520  2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                   1.670609  ...                          -0.465855   \n4                  -0.202516  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-2.152305</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-2.053810</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-1.992251</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-1.924536</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>1.670609</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-1.912224</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>-0.202516</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df =dict_of_imputed_dfs['TimesNet']\n",
    "missing = imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:52.490304Z",
     "start_time": "2025-05-23T12:40:52.479264Z"
    }
   },
   "id": "43857f333cf8cfd9",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af408aef05bc4476"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Missing value percentage': 3.2709532949456173,\n 'Mean absolute error': 0.5507467259643313,\n 'Mean square error': 0.5909499795177864,\n 'Root mean square error': 0.768732710061037,\n 'Mean relative error': 0.6992184510139345,\n 'Euclidean Distance': 31.093313855089477,\n 'r2 score': 0.3614790127096249}"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_scaled = df_scaled.set_index(df_scaled.columns[0])\n",
    "\n",
    "new_df_missing = df_missing.set_index(df_missing.columns[0])\n",
    "\n",
    "new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "\n",
    "default_metrics = tsi.compute_metrics(new_df_scaled, new_df_missing, new_imputed_df)\n",
    "default_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:52.504363Z",
     "start_time": "2025-05-23T12:40:52.491260Z"
    }
   },
   "id": "ab95f5224b7a076a",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "851d34f0bc65c4b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": tsi.default_imputation_params(),\n",
    "    'train_params': {\n",
    "        \"smooth\": False,\n",
    "        \"window\": 2,\n",
    "        \"order\": 1,\n",
    "        \"normalize\": False,\n",
    "        \"gap_type\": \"no_overlap\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 100,\n",
    "        \"max_gap_length\": 10,\n",
    "        \"max_gap_count\": 5\n",
    "    }\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']\n",
    "train_params = parameters['train_params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:40:52.511690Z",
     "start_time": "2025-05-23T12:40:52.505527Z"
    }
   },
   "id": "c2c8104cca230c33",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:40:58 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:40:58 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:40:58 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:40:58 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:40:58 [INFO]: Epoch 001 - training loss (MSE): 0.9298\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 002 - training loss (MSE): 0.3694\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 003 - training loss (MSE): 0.3279\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 004 - training loss (MSE): 0.2394\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 005 - training loss (MSE): 0.1888\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 006 - training loss (MSE): 0.1894\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 007 - training loss (MSE): 0.1609\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 008 - training loss (MSE): 0.1566\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 009 - training loss (MSE): 0.1515\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 010 - training loss (MSE): 0.1325\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 011 - training loss (MSE): 0.1349\n",
      "2025-05-23 15:40:58 [INFO]: Epoch 012 - training loss (MSE): 0.1277\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 013 - training loss (MSE): 0.1297\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 014 - training loss (MSE): 0.1230\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 015 - training loss (MSE): 0.1189\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 016 - training loss (MSE): 0.1171\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 017 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 018 - training loss (MSE): 0.1081\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 019 - training loss (MSE): 0.1128\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 020 - training loss (MSE): 0.1082\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 021 - training loss (MSE): 0.1007\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 022 - training loss (MSE): 0.0970\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 023 - training loss (MSE): 0.0906\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 024 - training loss (MSE): 0.0987\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 025 - training loss (MSE): 0.0959\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 026 - training loss (MSE): 0.0917\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 027 - training loss (MSE): 0.0865\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 028 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 029 - training loss (MSE): 0.0820\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 030 - training loss (MSE): 0.0778\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 031 - training loss (MSE): 0.0791\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 032 - training loss (MSE): 0.0762\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 033 - training loss (MSE): 0.0714\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 034 - training loss (MSE): 0.0774\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 035 - training loss (MSE): 0.0762\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 036 - training loss (MSE): 0.0795\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 038 - training loss (MSE): 0.0699\n",
      "2025-05-23 15:40:59 [INFO]: Epoch 039 - training loss (MSE): 0.0687\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 040 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 041 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 042 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 043 - training loss (MSE): 0.0766\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 044 - training loss (MSE): 0.0707\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 045 - training loss (MSE): 0.0741\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 046 - training loss (MSE): 0.0753\n",
      "2025-05-23 15:41:00 [INFO]: Epoch 047 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:41:00 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:41:00 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:41:43 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:41:43 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:41:43 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:41:43 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:41:43 [INFO]: Epoch 001 - training loss (MSE): 0.9298\n",
      "2025-05-23 15:41:43 [INFO]: Epoch 002 - training loss (MSE): 0.3694\n",
      "2025-05-23 15:41:43 [INFO]: Epoch 003 - training loss (MSE): 0.3279\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 004 - training loss (MSE): 0.2394\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 005 - training loss (MSE): 0.1888\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 006 - training loss (MSE): 0.1894\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 007 - training loss (MSE): 0.1609\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 008 - training loss (MSE): 0.1566\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 009 - training loss (MSE): 0.1515\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 010 - training loss (MSE): 0.1325\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 011 - training loss (MSE): 0.1349\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 012 - training loss (MSE): 0.1277\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 013 - training loss (MSE): 0.1297\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 014 - training loss (MSE): 0.1230\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 015 - training loss (MSE): 0.1189\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 016 - training loss (MSE): 0.1171\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 017 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 018 - training loss (MSE): 0.1081\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 019 - training loss (MSE): 0.1128\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 020 - training loss (MSE): 0.1082\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 021 - training loss (MSE): 0.1007\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 022 - training loss (MSE): 0.0970\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 023 - training loss (MSE): 0.0906\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 024 - training loss (MSE): 0.0987\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 025 - training loss (MSE): 0.0959\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 026 - training loss (MSE): 0.0917\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 027 - training loss (MSE): 0.0865\n",
      "2025-05-23 15:41:44 [INFO]: Epoch 028 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 029 - training loss (MSE): 0.0820\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 030 - training loss (MSE): 0.0778\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 031 - training loss (MSE): 0.0791\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 032 - training loss (MSE): 0.0762\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 033 - training loss (MSE): 0.0714\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 034 - training loss (MSE): 0.0774\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 035 - training loss (MSE): 0.0762\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 036 - training loss (MSE): 0.0795\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 038 - training loss (MSE): 0.0699\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 039 - training loss (MSE): 0.0687\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 040 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 041 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 042 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 043 - training loss (MSE): 0.0766\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 044 - training loss (MSE): 0.0707\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 045 - training loss (MSE): 0.0741\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 046 - training loss (MSE): 0.0753\n",
      "2025-05-23 15:41:45 [INFO]: Epoch 047 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:41:45 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:41:45 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'SoftImpute': {'mae': 0.41203221368374465,\n  'mse': 0.2634149142886794,\n  'rmse': 0.5132396265767867,\n  'r2': 0.7064356564029036,\n  'euclidean_distance': 246.3550207568576},\n 'IterativeSVD': {'mae': 0.41144195584551724,\n  'mse': 0.26352372539406554,\n  'rmse': 0.5133456198255377,\n  'r2': 0.7063143912087326,\n  'euclidean_distance': 246.4058975162581},\n 'SVT': {'mae': 0.15912930640468634,\n  'mse': 0.047144200742406314,\n  'rmse': 0.2171271533972808,\n  'r2': 0.9474598604914717,\n  'euclidean_distance': 104.22103363069478},\n 'TimesNet': {'mae': 0.5200315556751784,\n  'mse': 0.5230378115091425,\n  'rmse': 0.7232135310606007,\n  'r2': 0.41709734915058294,\n  'euclidean_distance': 347.1424949090883},\n 'Ensemble_Model': {'mae': 0.11133512281620736,\n  'mse': 0.02168590507192086,\n  'rmse': 0.1472613495521512,\n  'r2': 0.9758320119992501,\n  'euclidean_distance': 70.68544778503258}}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, metrics = tsi.train_ensemble(ground_truth = df_scaled, \n",
    "                                    algorithms=algorithms,\n",
    "                                    params=params, \n",
    "                                    train_params=train_params,\n",
    "                                    time_column=time_column, \n",
    "                                    header=header, \n",
    "                                    sep=sep, \n",
    "                                    is_multivariate=is_multivariate, \n",
    "                                    areaVStime=areaVStime, \n",
    "                                    preprocessing=preprocessing, \n",
    "                                    index=index)\n",
    "metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:46.042744Z",
     "start_time": "2025-05-23T12:40:52.512781Z"
    }
   },
   "id": "e8463e4866230439",
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation with Ensemble Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d95313e789bb4fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"SoftImpute\", \"IterativeSVD\", \"SVT\", \"TimesNet\"],\n",
    "    \"params\": tsi.default_imputation_params()\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:46.047093Z",
     "start_time": "2025-05-23T12:41:46.044341Z"
    }
   },
   "id": "7316c22364f48853",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SoftImpute\n",
      "Executing IterativeSVD\n",
      "Executing SVT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:41:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:41:54 [INFO]: No given device, using default device: cuda\n",
      "2025-05-23 15:41:54 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:41:54 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 17,153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:41:54 [INFO]: Epoch 001 - training loss (MSE): 0.9286\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 002 - training loss (MSE): 0.3693\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 003 - training loss (MSE): 0.3291\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 004 - training loss (MSE): 0.2408\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 005 - training loss (MSE): 0.1910\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 006 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 007 - training loss (MSE): 0.1611\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 008 - training loss (MSE): 0.1604\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 009 - training loss (MSE): 0.1511\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 010 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 011 - training loss (MSE): 0.1371\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 012 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 013 - training loss (MSE): 0.1310\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 014 - training loss (MSE): 0.1254\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 015 - training loss (MSE): 0.1198\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 016 - training loss (MSE): 0.1193\n",
      "2025-05-23 15:41:54 [INFO]: Epoch 017 - training loss (MSE): 0.1192\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 018 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 019 - training loss (MSE): 0.1164\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 020 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 021 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 022 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 023 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 024 - training loss (MSE): 0.1003\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 025 - training loss (MSE): 0.0950\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 026 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 027 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 028 - training loss (MSE): 0.0832\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 029 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 030 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 031 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 032 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 033 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 035 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 036 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 037 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 038 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 039 - training loss (MSE): 0.0698\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 040 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 041 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 042 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 043 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 044 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:41:55 [INFO]: Epoch 045 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:41:56 [INFO]: Epoch 046 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:41:56 [INFO]: Epoch 047 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:41:56 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:41:56 [INFO]: Finished training. The best model is from epoch#37.\n"
     ]
    }
   ],
   "source": [
    "model_imputed_df = tsi.run_imputation_ensemble(missing = df_missing, \n",
    "                                               algorithms=algorithms,\n",
    "                                               params=params, \n",
    "                                               model=model,\n",
    "                                               time_column=time_column,\n",
    "                                               header=header, \n",
    "                                               sep=sep, \n",
    "                                               is_multivariate=is_multivariate, \n",
    "                                               areaVStime=areaVStime, \n",
    "                                               preprocessing=preprocessing,\n",
    "                                               index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:56.234408Z",
     "start_time": "2025-05-23T12:41:46.048283Z"
    }
   },
   "id": "b2cf91b790d58cd6",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "           time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0    2017-01-02                -2.152305                 -1.347635   \n1    2017-01-03                -2.053810                 -1.189826   \n2    2017-01-04                -1.992251                 -1.325091   \n3    2017-01-05                -1.924536                 -1.336363   \n4    2017-01-06                -1.912224                 -1.313819   \n..          ...                      ...                       ...   \n516  2018-12-25                -1.509627                  0.405175   \n517  2018-12-26                -1.509627                  0.405175   \n518  2018-12-27                -1.310175                  0.433355   \n519  2018-12-28                -1.027002                  0.861694   \n520  2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                  -1.730486  ...                          -0.465855   \n4                  -0.054313  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-2.152305</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-2.053810</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-1.992251</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-1.924536</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>-1.730486</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-1.912224</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>-0.054313</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = model_imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "model_imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:56.247092Z",
     "start_time": "2025-05-23T12:41:56.236768Z"
    }
   },
   "id": "83aeb8e669ea512d",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform to the original"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25b846d5086c6be8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:56.264616Z",
     "start_time": "2025-05-23T12:41:56.247821Z"
    }
   },
   "id": "6830254ed3603138",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_imputed_df_orig = tsi.dataframe_inverse_scaler(df_input=model_imputed_df, \n",
    "                                                     scaler=scaler,\n",
    "                                                     time_column=time_column, \n",
    "                                                     header=header, \n",
    "                                                     sep=sep, \n",
    "                                                     preprocessing=preprocessing, \n",
    "                                                     index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:56.274595Z",
     "start_time": "2025-05-23T12:41:56.265355Z"
    }
   },
   "id": "49767446566c180",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "           time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0    2017-01-02                -2.152305                 -1.347635   \n1    2017-01-03                -2.053810                 -1.189826   \n2    2017-01-04                -1.992251                 -1.325091   \n3    2017-01-05                -1.924536                 -1.336363   \n4    2017-01-06                -1.912224                 -1.313819   \n..          ...                      ...                       ...   \n516  2018-12-25                -1.509627                  0.405175   \n517  2018-12-26                -1.509627                  0.405175   \n518  2018-12-27                -1.310175                  0.433355   \n519  2018-12-28                -1.027002                  0.861694   \n520  2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                  -1.730486  ...                          -0.465855   \n4                  -0.054313  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-2.152305</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-2.053810</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-1.992251</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-1.924536</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>-1.730486</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-1.912224</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>-0.054313</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_imputed_df_orig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:56.291671Z",
     "start_time": "2025-05-23T12:41:56.275462Z"
    }
   },
   "id": "f16c0b260edc85cf",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter tuning - Get params dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "321c4103cc259476"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "params = tsi.default_hyperparameter_tuning_imputation_params()\n",
    "algorithm: str = 'TimesNet'\n",
    "time_column: str = 'time'\n",
    "is_multivariate: bool = False\n",
    "areaVStime: int = 0\n",
    "preprocessing: bool = False\n",
    "index: bool = False\n",
    "n_trials = 20"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:41:56.303517Z",
     "start_time": "2025-05-23T12:41:56.292554Z"
    }
   },
   "id": "a6f7d1f5084c21e1",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-23 15:41:56,311] A new study created in memory with name: no-name-8db6dc28-84f5-424c-a3fb-4f06c2da3316\n",
      "2025-05-23 15:41:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:41:56 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:41:56 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:41:56 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 690,305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:41:56 [INFO]: Epoch 001 - training loss (MSE): 0.7548\n",
      "2025-05-23 15:41:56 [INFO]: Epoch 002 - training loss (MSE): 0.3961\n",
      "2025-05-23 15:41:56 [INFO]: Epoch 003 - training loss (MSE): 0.2421\n",
      "2025-05-23 15:41:56 [INFO]: Epoch 004 - training loss (MSE): 0.1980\n",
      "2025-05-23 15:41:57 [INFO]: Epoch 005 - training loss (MSE): 0.1632\n",
      "2025-05-23 15:41:57 [INFO]: Epoch 006 - training loss (MSE): 0.1427\n",
      "2025-05-23 15:41:57 [INFO]: Epoch 007 - training loss (MSE): 0.1211\n",
      "2025-05-23 15:41:57 [INFO]: Epoch 008 - training loss (MSE): 0.1152\n",
      "2025-05-23 15:41:57 [INFO]: Epoch 009 - training loss (MSE): 0.1034\n",
      "2025-05-23 15:41:57 [INFO]: Epoch 010 - training loss (MSE): 0.0892\n",
      "2025-05-23 15:41:58 [INFO]: Epoch 011 - training loss (MSE): 0.0824\n",
      "2025-05-23 15:41:58 [INFO]: Epoch 012 - training loss (MSE): 0.0796\n",
      "2025-05-23 15:41:58 [INFO]: Epoch 013 - training loss (MSE): 0.0729\n",
      "2025-05-23 15:41:58 [INFO]: Epoch 014 - training loss (MSE): 0.0687\n",
      "2025-05-23 15:41:58 [INFO]: Epoch 015 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:41:58 [INFO]: Epoch 016 - training loss (MSE): 0.0611\n",
      "2025-05-23 15:41:58 [INFO]: Epoch 017 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:41:59 [INFO]: Epoch 018 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:41:59 [INFO]: Epoch 019 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:41:59 [INFO]: Epoch 020 - training loss (MSE): 0.0546\n",
      "2025-05-23 15:41:59 [INFO]: Epoch 021 - training loss (MSE): 0.0533\n",
      "2025-05-23 15:41:59 [INFO]: Epoch 022 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:41:59 [INFO]: Epoch 023 - training loss (MSE): 0.0496\n",
      "2025-05-23 15:41:59 [INFO]: Epoch 024 - training loss (MSE): 0.0469\n",
      "2025-05-23 15:42:00 [INFO]: Epoch 025 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:42:00 [INFO]: Epoch 026 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:42:00 [INFO]: Epoch 027 - training loss (MSE): 0.0451\n",
      "2025-05-23 15:42:00 [INFO]: Epoch 028 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:42:00 [INFO]: Epoch 029 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:42:00 [INFO]: Epoch 030 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:42:00 [INFO]: Epoch 031 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:42:01 [INFO]: Epoch 032 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:42:01 [INFO]: Epoch 033 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:42:01 [INFO]: Epoch 034 - training loss (MSE): 0.0412\n",
      "2025-05-23 15:42:01 [INFO]: Epoch 035 - training loss (MSE): 0.0348\n",
      "2025-05-23 15:42:01 [INFO]: Epoch 036 - training loss (MSE): 0.0386\n",
      "2025-05-23 15:42:01 [INFO]: Epoch 037 - training loss (MSE): 0.0371\n",
      "2025-05-23 15:42:02 [INFO]: Epoch 038 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:42:02 [INFO]: Epoch 039 - training loss (MSE): 0.0393\n",
      "2025-05-23 15:42:02 [INFO]: Epoch 040 - training loss (MSE): 0.0374\n",
      "2025-05-23 15:42:02 [INFO]: Epoch 041 - training loss (MSE): 0.0378\n",
      "2025-05-23 15:42:02 [INFO]: Epoch 042 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:42:02 [INFO]: Epoch 043 - training loss (MSE): 0.0381\n",
      "2025-05-23 15:42:02 [INFO]: Epoch 044 - training loss (MSE): 0.0365\n",
      "2025-05-23 15:42:03 [INFO]: Epoch 045 - training loss (MSE): 0.0347\n",
      "2025-05-23 15:42:03 [INFO]: Epoch 046 - training loss (MSE): 0.0362\n",
      "2025-05-23 15:42:03 [INFO]: Epoch 047 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:42:03 [INFO]: Epoch 048 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:42:03 [INFO]: Epoch 049 - training loss (MSE): 0.0330\n",
      "2025-05-23 15:42:03 [INFO]: Epoch 050 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:42:03 [INFO]: Epoch 051 - training loss (MSE): 0.0340\n",
      "2025-05-23 15:42:04 [INFO]: Epoch 052 - training loss (MSE): 0.0328\n",
      "2025-05-23 15:42:04 [INFO]: Epoch 053 - training loss (MSE): 0.0315\n",
      "2025-05-23 15:42:04 [INFO]: Epoch 054 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:42:04 [INFO]: Epoch 055 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:42:04 [INFO]: Epoch 056 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:42:04 [INFO]: Epoch 057 - training loss (MSE): 0.0343\n",
      "2025-05-23 15:42:04 [INFO]: Epoch 058 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:42:05 [INFO]: Epoch 059 - training loss (MSE): 0.0340\n",
      "2025-05-23 15:42:05 [INFO]: Epoch 060 - training loss (MSE): 0.0320\n",
      "2025-05-23 15:42:05 [INFO]: Epoch 061 - training loss (MSE): 0.0315\n",
      "2025-05-23 15:42:05 [INFO]: Epoch 062 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:42:05 [INFO]: Epoch 063 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:42:05 [INFO]: Epoch 064 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:42:05 [INFO]: Epoch 065 - training loss (MSE): 0.0322\n",
      "2025-05-23 15:42:06 [INFO]: Epoch 066 - training loss (MSE): 0.0312\n",
      "2025-05-23 15:42:06 [INFO]: Epoch 067 - training loss (MSE): 0.0319\n",
      "2025-05-23 15:42:06 [INFO]: Epoch 068 - training loss (MSE): 0.0309\n",
      "2025-05-23 15:42:06 [INFO]: Epoch 069 - training loss (MSE): 0.0343\n",
      "2025-05-23 15:42:06 [INFO]: Epoch 070 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:42:06 [INFO]: Epoch 071 - training loss (MSE): 0.0313\n",
      "2025-05-23 15:42:06 [INFO]: Epoch 072 - training loss (MSE): 0.0308\n",
      "2025-05-23 15:42:07 [INFO]: Epoch 073 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:42:07 [INFO]: Epoch 074 - training loss (MSE): 0.0290\n",
      "2025-05-23 15:42:07 [INFO]: Epoch 075 - training loss (MSE): 0.0286\n",
      "2025-05-23 15:42:07 [INFO]: Epoch 076 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:42:07 [INFO]: Epoch 077 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:42:07 [INFO]: Epoch 078 - training loss (MSE): 0.0295\n",
      "2025-05-23 15:42:07 [INFO]: Epoch 079 - training loss (MSE): 0.0299\n",
      "2025-05-23 15:42:08 [INFO]: Epoch 080 - training loss (MSE): 0.0292\n",
      "2025-05-23 15:42:08 [INFO]: Epoch 081 - training loss (MSE): 0.0289\n",
      "2025-05-23 15:42:08 [INFO]: Epoch 082 - training loss (MSE): 0.0293\n",
      "2025-05-23 15:42:08 [INFO]: Epoch 083 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:42:08 [INFO]: Epoch 084 - training loss (MSE): 0.0275\n",
      "2025-05-23 15:42:08 [INFO]: Epoch 085 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:42:09 [INFO]: Epoch 086 - training loss (MSE): 0.0303\n",
      "2025-05-23 15:42:09 [INFO]: Epoch 087 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:42:09 [INFO]: Epoch 088 - training loss (MSE): 0.0291\n",
      "2025-05-23 15:42:09 [INFO]: Epoch 089 - training loss (MSE): 0.0292\n",
      "2025-05-23 15:42:09 [INFO]: Epoch 090 - training loss (MSE): 0.0278\n",
      "2025-05-23 15:42:09 [INFO]: Epoch 091 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:42:09 [INFO]: Epoch 092 - training loss (MSE): 0.0280\n",
      "2025-05-23 15:42:10 [INFO]: Epoch 093 - training loss (MSE): 0.0299\n",
      "2025-05-23 15:42:10 [INFO]: Epoch 094 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:42:10 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:42:10 [INFO]: Finished training. The best model is from epoch#84.\n",
      "[I 2025-05-23 15:42:10,384] Trial 0 finished with value: 0.25377336635539527 and parameters: {'n_layers': 2, 'd_model': 128, 'd_ffn': 16, 'n_heads': 2, 'top_k': 3, 'n_kernels': 4, 'dropout': 0.4, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.1, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.005431095176981607, 'weight_decay': 0.0001106913243495668}. Best is trial 0 with value: 0.25377336635539527.\n",
      "2025-05-23 15:42:10 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:42:10 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:42:10 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:42:10 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 677,585\n",
      "2025-05-23 15:42:10 [INFO]: Epoch 001 - training loss (MSE): 0.5991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:42:10 [INFO]: Epoch 002 - training loss (MSE): 0.3987\n",
      "2025-05-23 15:42:10 [INFO]: Epoch 003 - training loss (MSE): 0.3297\n",
      "2025-05-23 15:42:11 [INFO]: Epoch 004 - training loss (MSE): 0.2795\n",
      "2025-05-23 15:42:11 [INFO]: Epoch 005 - training loss (MSE): 0.2680\n",
      "2025-05-23 15:42:11 [INFO]: Epoch 006 - training loss (MSE): 0.2344\n",
      "2025-05-23 15:42:11 [INFO]: Epoch 007 - training loss (MSE): 0.2125\n",
      "2025-05-23 15:42:11 [INFO]: Epoch 008 - training loss (MSE): 0.1994\n",
      "2025-05-23 15:42:11 [INFO]: Epoch 009 - training loss (MSE): 0.1760\n",
      "2025-05-23 15:42:11 [INFO]: Epoch 010 - training loss (MSE): 0.1657\n",
      "2025-05-23 15:42:12 [INFO]: Epoch 011 - training loss (MSE): 0.1500\n",
      "2025-05-23 15:42:12 [INFO]: Epoch 012 - training loss (MSE): 0.1414\n",
      "2025-05-23 15:42:12 [INFO]: Epoch 013 - training loss (MSE): 0.1246\n",
      "2025-05-23 15:42:12 [INFO]: Epoch 014 - training loss (MSE): 0.1218\n",
      "2025-05-23 15:42:12 [INFO]: Epoch 015 - training loss (MSE): 0.1105\n",
      "2025-05-23 15:42:12 [INFO]: Epoch 016 - training loss (MSE): 0.1046\n",
      "2025-05-23 15:42:12 [INFO]: Epoch 017 - training loss (MSE): 0.1018\n",
      "2025-05-23 15:42:13 [INFO]: Epoch 018 - training loss (MSE): 0.0957\n",
      "2025-05-23 15:42:13 [INFO]: Epoch 019 - training loss (MSE): 0.0922\n",
      "2025-05-23 15:42:13 [INFO]: Epoch 020 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:42:13 [INFO]: Epoch 021 - training loss (MSE): 0.0806\n",
      "2025-05-23 15:42:13 [INFO]: Epoch 022 - training loss (MSE): 0.0766\n",
      "2025-05-23 15:42:13 [INFO]: Epoch 023 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:42:13 [INFO]: Epoch 024 - training loss (MSE): 0.0709\n",
      "2025-05-23 15:42:14 [INFO]: Epoch 025 - training loss (MSE): 0.0686\n",
      "2025-05-23 15:42:14 [INFO]: Epoch 026 - training loss (MSE): 0.0658\n",
      "2025-05-23 15:42:14 [INFO]: Epoch 027 - training loss (MSE): 0.0652\n",
      "2025-05-23 15:42:14 [INFO]: Epoch 028 - training loss (MSE): 0.0667\n",
      "2025-05-23 15:42:14 [INFO]: Epoch 029 - training loss (MSE): 0.0657\n",
      "2025-05-23 15:42:14 [INFO]: Epoch 030 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:42:14 [INFO]: Epoch 031 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 032 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 033 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 034 - training loss (MSE): 0.0588\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 035 - training loss (MSE): 0.0532\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 036 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 037 - training loss (MSE): 0.0511\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 038 - training loss (MSE): 0.0492\n",
      "2025-05-23 15:42:15 [INFO]: Epoch 039 - training loss (MSE): 0.0542\n",
      "2025-05-23 15:42:16 [INFO]: Epoch 040 - training loss (MSE): 0.0490\n",
      "2025-05-23 15:42:16 [INFO]: Epoch 041 - training loss (MSE): 0.0536\n",
      "2025-05-23 15:42:16 [INFO]: Epoch 042 - training loss (MSE): 0.0522\n",
      "2025-05-23 15:42:16 [INFO]: Epoch 043 - training loss (MSE): 0.0492\n",
      "2025-05-23 15:42:16 [INFO]: Epoch 044 - training loss (MSE): 0.0509\n",
      "2025-05-23 15:42:16 [INFO]: Epoch 045 - training loss (MSE): 0.0469\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 046 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 047 - training loss (MSE): 0.0491\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 048 - training loss (MSE): 0.0480\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 049 - training loss (MSE): 0.0463\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 050 - training loss (MSE): 0.0479\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 051 - training loss (MSE): 0.0460\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 052 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:42:17 [INFO]: Epoch 053 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:42:18 [INFO]: Epoch 054 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:42:18 [INFO]: Epoch 055 - training loss (MSE): 0.0462\n",
      "2025-05-23 15:42:18 [INFO]: Epoch 056 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:42:18 [INFO]: Epoch 057 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:42:18 [INFO]: Epoch 058 - training loss (MSE): 0.0434\n",
      "2025-05-23 15:42:18 [INFO]: Epoch 059 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:42:18 [INFO]: Epoch 060 - training loss (MSE): 0.0404\n",
      "2025-05-23 15:42:19 [INFO]: Epoch 061 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:42:19 [INFO]: Epoch 062 - training loss (MSE): 0.0401\n",
      "2025-05-23 15:42:19 [INFO]: Epoch 063 - training loss (MSE): 0.0384\n",
      "2025-05-23 15:42:19 [INFO]: Epoch 064 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:42:19 [INFO]: Epoch 065 - training loss (MSE): 0.0402\n",
      "2025-05-23 15:42:19 [INFO]: Epoch 066 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:42:19 [INFO]: Epoch 067 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:42:20 [INFO]: Epoch 068 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:42:20 [INFO]: Epoch 069 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:42:20 [INFO]: Epoch 070 - training loss (MSE): 0.0416\n",
      "2025-05-23 15:42:20 [INFO]: Epoch 071 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:42:20 [INFO]: Epoch 072 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:42:20 [INFO]: Epoch 073 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:42:20 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:42:20 [INFO]: Finished training. The best model is from epoch#63.\n",
      "[I 2025-05-23 15:42:20,896] Trial 1 finished with value: 0.2375144542561454 and parameters: {'n_layers': 1, 'd_model': 128, 'd_ffn': 16, 'n_heads': 2, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0008505207524153307, 'weight_decay': 0.0003414234544289426}. Best is trial 1 with value: 0.2375144542561454.\n",
      "2025-05-23 15:42:20 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:42:20 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:42:20 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:42:21 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 2,029,809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:42:21 [INFO]: Epoch 001 - training loss (MSE): 0.4885\n",
      "2025-05-23 15:42:21 [INFO]: Epoch 002 - training loss (MSE): 0.2531\n",
      "2025-05-23 15:42:21 [INFO]: Epoch 003 - training loss (MSE): 0.1862\n",
      "2025-05-23 15:42:21 [INFO]: Epoch 004 - training loss (MSE): 0.1549\n",
      "2025-05-23 15:42:21 [INFO]: Epoch 005 - training loss (MSE): 0.1274\n",
      "2025-05-23 15:42:22 [INFO]: Epoch 006 - training loss (MSE): 0.1165\n",
      "2025-05-23 15:42:22 [INFO]: Epoch 007 - training loss (MSE): 0.0980\n",
      "2025-05-23 15:42:22 [INFO]: Epoch 008 - training loss (MSE): 0.0881\n",
      "2025-05-23 15:42:22 [INFO]: Epoch 009 - training loss (MSE): 0.0855\n",
      "2025-05-23 15:42:22 [INFO]: Epoch 010 - training loss (MSE): 0.0835\n",
      "2025-05-23 15:42:23 [INFO]: Epoch 011 - training loss (MSE): 0.0828\n",
      "2025-05-23 15:42:23 [INFO]: Epoch 012 - training loss (MSE): 0.0726\n",
      "2025-05-23 15:42:23 [INFO]: Epoch 013 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:42:23 [INFO]: Epoch 014 - training loss (MSE): 0.0693\n",
      "2025-05-23 15:42:23 [INFO]: Epoch 015 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:42:23 [INFO]: Epoch 016 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:42:24 [INFO]: Epoch 017 - training loss (MSE): 0.0647\n",
      "2025-05-23 15:42:24 [INFO]: Epoch 018 - training loss (MSE): 0.0621\n",
      "2025-05-23 15:42:24 [INFO]: Epoch 019 - training loss (MSE): 0.0608\n",
      "2025-05-23 15:42:24 [INFO]: Epoch 020 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:42:24 [INFO]: Epoch 021 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:42:25 [INFO]: Epoch 022 - training loss (MSE): 0.0568\n",
      "2025-05-23 15:42:25 [INFO]: Epoch 023 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:42:25 [INFO]: Epoch 024 - training loss (MSE): 0.0538\n",
      "2025-05-23 15:42:25 [INFO]: Epoch 025 - training loss (MSE): 0.0511\n",
      "2025-05-23 15:42:25 [INFO]: Epoch 026 - training loss (MSE): 0.0541\n",
      "2025-05-23 15:42:25 [INFO]: Epoch 027 - training loss (MSE): 0.0527\n",
      "2025-05-23 15:42:26 [INFO]: Epoch 028 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:42:26 [INFO]: Epoch 029 - training loss (MSE): 0.0504\n",
      "2025-05-23 15:42:26 [INFO]: Epoch 030 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:42:26 [INFO]: Epoch 031 - training loss (MSE): 0.0468\n",
      "2025-05-23 15:42:26 [INFO]: Epoch 032 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:42:27 [INFO]: Epoch 033 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:42:27 [INFO]: Epoch 034 - training loss (MSE): 0.0455\n",
      "2025-05-23 15:42:27 [INFO]: Epoch 035 - training loss (MSE): 0.0506\n",
      "2025-05-23 15:42:27 [INFO]: Epoch 036 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:42:27 [INFO]: Epoch 037 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:42:28 [INFO]: Epoch 038 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:42:28 [INFO]: Epoch 039 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:42:28 [INFO]: Epoch 040 - training loss (MSE): 0.0446\n",
      "2025-05-23 15:42:28 [INFO]: Epoch 041 - training loss (MSE): 0.0446\n",
      "2025-05-23 15:42:28 [INFO]: Epoch 042 - training loss (MSE): 0.0494\n",
      "2025-05-23 15:42:28 [INFO]: Epoch 043 - training loss (MSE): 0.0421\n",
      "2025-05-23 15:42:29 [INFO]: Epoch 044 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:42:29 [INFO]: Epoch 045 - training loss (MSE): 0.0432\n",
      "2025-05-23 15:42:29 [INFO]: Epoch 046 - training loss (MSE): 0.0450\n",
      "2025-05-23 15:42:29 [INFO]: Epoch 047 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:42:29 [INFO]: Epoch 048 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:42:30 [INFO]: Epoch 049 - training loss (MSE): 0.0412\n",
      "2025-05-23 15:42:30 [INFO]: Epoch 050 - training loss (MSE): 0.0409\n",
      "2025-05-23 15:42:30 [INFO]: Epoch 051 - training loss (MSE): 0.0421\n",
      "2025-05-23 15:42:30 [INFO]: Epoch 052 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:42:30 [INFO]: Epoch 053 - training loss (MSE): 0.0396\n",
      "2025-05-23 15:42:31 [INFO]: Epoch 054 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:42:31 [INFO]: Epoch 055 - training loss (MSE): 0.0429\n",
      "2025-05-23 15:42:31 [INFO]: Epoch 056 - training loss (MSE): 0.0393\n",
      "2025-05-23 15:42:31 [INFO]: Epoch 057 - training loss (MSE): 0.0395\n",
      "2025-05-23 15:42:31 [INFO]: Epoch 058 - training loss (MSE): 0.0418\n",
      "2025-05-23 15:42:31 [INFO]: Epoch 059 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:42:32 [INFO]: Epoch 060 - training loss (MSE): 0.0407\n",
      "2025-05-23 15:42:32 [INFO]: Epoch 061 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:42:32 [INFO]: Epoch 062 - training loss (MSE): 0.0384\n",
      "2025-05-23 15:42:32 [INFO]: Epoch 063 - training loss (MSE): 0.0409\n",
      "2025-05-23 15:42:32 [INFO]: Epoch 064 - training loss (MSE): 0.0364\n",
      "2025-05-23 15:42:33 [INFO]: Epoch 065 - training loss (MSE): 0.0402\n",
      "2025-05-23 15:42:33 [INFO]: Epoch 066 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:42:33 [INFO]: Epoch 067 - training loss (MSE): 0.0382\n",
      "2025-05-23 15:42:33 [INFO]: Epoch 068 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:42:33 [INFO]: Epoch 069 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:42:33 [INFO]: Epoch 070 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:42:34 [INFO]: Epoch 071 - training loss (MSE): 0.0370\n",
      "2025-05-23 15:42:34 [INFO]: Epoch 072 - training loss (MSE): 0.0390\n",
      "2025-05-23 15:42:34 [INFO]: Epoch 073 - training loss (MSE): 0.0381\n",
      "2025-05-23 15:42:34 [INFO]: Epoch 074 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:42:34 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:42:34 [INFO]: Finished training. The best model is from epoch#64.\n",
      "[I 2025-05-23 15:42:34,927] Trial 2 finished with value: 0.24376912720953683 and parameters: {'n_layers': 3, 'd_model': 16, 'd_ffn': 128, 'n_heads': 1, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.003964743616689867, 'weight_decay': 0.0004659747070765304}. Best is trial 1 with value: 0.2375144542561454.\n",
      "2025-05-23 15:42:35 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:42:35 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:42:35 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:42:35 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,377,793\n",
      "2025-05-23 15:42:35 [INFO]: Epoch 001 - training loss (MSE): 1.7119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:42:35 [INFO]: Epoch 002 - training loss (MSE): 1.4442\n",
      "2025-05-23 15:42:35 [INFO]: Epoch 003 - training loss (MSE): 1.2114\n",
      "2025-05-23 15:42:35 [INFO]: Epoch 004 - training loss (MSE): 1.0332\n",
      "2025-05-23 15:42:35 [INFO]: Epoch 005 - training loss (MSE): 0.8529\n",
      "2025-05-23 15:42:35 [INFO]: Epoch 006 - training loss (MSE): 0.6720\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 007 - training loss (MSE): 0.5716\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 008 - training loss (MSE): 0.4794\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 009 - training loss (MSE): 0.4137\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 010 - training loss (MSE): 0.3786\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 011 - training loss (MSE): 0.3586\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 012 - training loss (MSE): 0.3542\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 013 - training loss (MSE): 0.3496\n",
      "2025-05-23 15:42:36 [INFO]: Epoch 014 - training loss (MSE): 0.3382\n",
      "2025-05-23 15:42:37 [INFO]: Epoch 015 - training loss (MSE): 0.3284\n",
      "2025-05-23 15:42:37 [INFO]: Epoch 016 - training loss (MSE): 0.3063\n",
      "2025-05-23 15:42:37 [INFO]: Epoch 017 - training loss (MSE): 0.2986\n",
      "2025-05-23 15:42:37 [INFO]: Epoch 018 - training loss (MSE): 0.2904\n",
      "2025-05-23 15:42:37 [INFO]: Epoch 019 - training loss (MSE): 0.2838\n",
      "2025-05-23 15:42:37 [INFO]: Epoch 020 - training loss (MSE): 0.2761\n",
      "2025-05-23 15:42:37 [INFO]: Epoch 021 - training loss (MSE): 0.2787\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 022 - training loss (MSE): 0.2708\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 023 - training loss (MSE): 0.2620\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 024 - training loss (MSE): 0.2665\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 025 - training loss (MSE): 0.2552\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 026 - training loss (MSE): 0.2531\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 027 - training loss (MSE): 0.2403\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 028 - training loss (MSE): 0.2377\n",
      "2025-05-23 15:42:38 [INFO]: Epoch 029 - training loss (MSE): 0.2421\n",
      "2025-05-23 15:42:39 [INFO]: Epoch 030 - training loss (MSE): 0.2289\n",
      "2025-05-23 15:42:39 [INFO]: Epoch 031 - training loss (MSE): 0.2258\n",
      "2025-05-23 15:42:39 [INFO]: Epoch 032 - training loss (MSE): 0.2287\n",
      "2025-05-23 15:42:39 [INFO]: Epoch 033 - training loss (MSE): 0.2163\n",
      "2025-05-23 15:42:39 [INFO]: Epoch 034 - training loss (MSE): 0.2112\n",
      "2025-05-23 15:42:39 [INFO]: Epoch 035 - training loss (MSE): 0.2078\n",
      "2025-05-23 15:42:39 [INFO]: Epoch 036 - training loss (MSE): 0.2100\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 037 - training loss (MSE): 0.1985\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 038 - training loss (MSE): 0.1948\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 039 - training loss (MSE): 0.1958\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 040 - training loss (MSE): 0.1927\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 041 - training loss (MSE): 0.1872\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 042 - training loss (MSE): 0.1841\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 043 - training loss (MSE): 0.1840\n",
      "2025-05-23 15:42:40 [INFO]: Epoch 044 - training loss (MSE): 0.1756\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 045 - training loss (MSE): 0.1753\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 046 - training loss (MSE): 0.1679\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 047 - training loss (MSE): 0.1668\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 048 - training loss (MSE): 0.1627\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 049 - training loss (MSE): 0.1601\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 050 - training loss (MSE): 0.1631\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 051 - training loss (MSE): 0.1480\n",
      "2025-05-23 15:42:41 [INFO]: Epoch 052 - training loss (MSE): 0.1479\n",
      "2025-05-23 15:42:42 [INFO]: Epoch 053 - training loss (MSE): 0.1482\n",
      "2025-05-23 15:42:42 [INFO]: Epoch 054 - training loss (MSE): 0.1459\n",
      "2025-05-23 15:42:42 [INFO]: Epoch 055 - training loss (MSE): 0.1437\n",
      "2025-05-23 15:42:42 [INFO]: Epoch 056 - training loss (MSE): 0.1391\n",
      "2025-05-23 15:42:42 [INFO]: Epoch 057 - training loss (MSE): 0.1344\n",
      "2025-05-23 15:42:42 [INFO]: Epoch 058 - training loss (MSE): 0.1295\n",
      "2025-05-23 15:42:42 [INFO]: Epoch 059 - training loss (MSE): 0.1289\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 060 - training loss (MSE): 0.1288\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 061 - training loss (MSE): 0.1282\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 062 - training loss (MSE): 0.1271\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 063 - training loss (MSE): 0.1251\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 064 - training loss (MSE): 0.1201\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 065 - training loss (MSE): 0.1144\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 066 - training loss (MSE): 0.1110\n",
      "2025-05-23 15:42:43 [INFO]: Epoch 067 - training loss (MSE): 0.1163\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 068 - training loss (MSE): 0.1132\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 069 - training loss (MSE): 0.1089\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 070 - training loss (MSE): 0.1096\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 071 - training loss (MSE): 0.1056\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 072 - training loss (MSE): 0.1032\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 073 - training loss (MSE): 0.1078\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 074 - training loss (MSE): 0.1027\n",
      "2025-05-23 15:42:44 [INFO]: Epoch 075 - training loss (MSE): 0.1054\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 076 - training loss (MSE): 0.1028\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 077 - training loss (MSE): 0.1020\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 078 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 079 - training loss (MSE): 0.0980\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 080 - training loss (MSE): 0.0962\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 081 - training loss (MSE): 0.0945\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 082 - training loss (MSE): 0.0927\n",
      "2025-05-23 15:42:45 [INFO]: Epoch 083 - training loss (MSE): 0.0958\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 084 - training loss (MSE): 0.0905\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 085 - training loss (MSE): 0.0891\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 086 - training loss (MSE): 0.0896\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 087 - training loss (MSE): 0.0929\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 088 - training loss (MSE): 0.0861\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 089 - training loss (MSE): 0.0924\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 090 - training loss (MSE): 0.0886\n",
      "2025-05-23 15:42:46 [INFO]: Epoch 091 - training loss (MSE): 0.0842\n",
      "2025-05-23 15:42:47 [INFO]: Epoch 092 - training loss (MSE): 0.0849\n",
      "2025-05-23 15:42:47 [INFO]: Epoch 093 - training loss (MSE): 0.0873\n",
      "2025-05-23 15:42:47 [INFO]: Epoch 094 - training loss (MSE): 0.0824\n",
      "2025-05-23 15:42:47 [INFO]: Epoch 095 - training loss (MSE): 0.0798\n",
      "2025-05-23 15:42:47 [INFO]: Epoch 096 - training loss (MSE): 0.0818\n",
      "2025-05-23 15:42:47 [INFO]: Epoch 097 - training loss (MSE): 0.0790\n",
      "2025-05-23 15:42:47 [INFO]: Epoch 098 - training loss (MSE): 0.0811\n",
      "2025-05-23 15:42:48 [INFO]: Epoch 099 - training loss (MSE): 0.0818\n",
      "2025-05-23 15:42:48 [INFO]: Epoch 100 - training loss (MSE): 0.0795\n",
      "2025-05-23 15:42:48 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:42:48,330] Trial 3 finished with value: 0.3153889857663928 and parameters: {'n_layers': 2, 'd_model': 64, 'd_ffn': 64, 'n_heads': 1, 'top_k': 3, 'n_kernels': 4, 'dropout': 0.4, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.00014632223035577693, 'weight_decay': 0.000331603591492567}. Best is trial 1 with value: 0.2375144542561454.\n",
      "2025-05-23 15:42:48 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:42:48 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:42:48 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:42:48 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 431,201\n",
      "2025-05-23 15:42:48 [INFO]: Epoch 001 - training loss (MSE): 1.6065\n",
      "2025-05-23 15:42:48 [INFO]: Epoch 002 - training loss (MSE): 0.5707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:42:48 [INFO]: Epoch 003 - training loss (MSE): 0.4591\n",
      "2025-05-23 15:42:48 [INFO]: Epoch 004 - training loss (MSE): 0.4421\n",
      "2025-05-23 15:42:48 [INFO]: Epoch 005 - training loss (MSE): 0.3163\n",
      "2025-05-23 15:42:48 [INFO]: Epoch 006 - training loss (MSE): 0.2502\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 007 - training loss (MSE): 0.2623\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 008 - training loss (MSE): 0.2432\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 009 - training loss (MSE): 0.2096\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 010 - training loss (MSE): 0.1904\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 011 - training loss (MSE): 0.1840\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 012 - training loss (MSE): 0.1746\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 013 - training loss (MSE): 0.1558\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 014 - training loss (MSE): 0.1517\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 015 - training loss (MSE): 0.1430\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 016 - training loss (MSE): 0.1392\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 017 - training loss (MSE): 0.1289\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 018 - training loss (MSE): 0.1203\n",
      "2025-05-23 15:42:49 [INFO]: Epoch 019 - training loss (MSE): 0.1171\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 020 - training loss (MSE): 0.1126\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 021 - training loss (MSE): 0.1094\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 022 - training loss (MSE): 0.1074\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 023 - training loss (MSE): 0.1100\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 024 - training loss (MSE): 0.1030\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 025 - training loss (MSE): 0.0954\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 026 - training loss (MSE): 0.1005\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 027 - training loss (MSE): 0.0981\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 028 - training loss (MSE): 0.0957\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 029 - training loss (MSE): 0.0990\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 030 - training loss (MSE): 0.0928\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 031 - training loss (MSE): 0.1012\n",
      "2025-05-23 15:42:50 [INFO]: Epoch 032 - training loss (MSE): 0.0945\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 033 - training loss (MSE): 0.0923\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 034 - training loss (MSE): 0.0889\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 035 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 036 - training loss (MSE): 0.0919\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 037 - training loss (MSE): 0.0839\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 038 - training loss (MSE): 0.0882\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 039 - training loss (MSE): 0.0844\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 040 - training loss (MSE): 0.0874\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 041 - training loss (MSE): 0.0827\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 042 - training loss (MSE): 0.0827\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 043 - training loss (MSE): 0.0834\n",
      "2025-05-23 15:42:51 [INFO]: Epoch 044 - training loss (MSE): 0.0842\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 045 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 046 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 047 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 048 - training loss (MSE): 0.0801\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 049 - training loss (MSE): 0.0761\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 050 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 051 - training loss (MSE): 0.0713\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 052 - training loss (MSE): 0.0707\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 053 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 054 - training loss (MSE): 0.0743\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 055 - training loss (MSE): 0.0754\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 056 - training loss (MSE): 0.0692\n",
      "2025-05-23 15:42:52 [INFO]: Epoch 057 - training loss (MSE): 0.0696\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 058 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 059 - training loss (MSE): 0.0694\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 060 - training loss (MSE): 0.0673\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 061 - training loss (MSE): 0.0711\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 062 - training loss (MSE): 0.0666\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 063 - training loss (MSE): 0.0670\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 064 - training loss (MSE): 0.0657\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 065 - training loss (MSE): 0.0661\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 066 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 067 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 068 - training loss (MSE): 0.0664\n",
      "2025-05-23 15:42:53 [INFO]: Epoch 069 - training loss (MSE): 0.0658\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 070 - training loss (MSE): 0.0638\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 071 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 072 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 073 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 074 - training loss (MSE): 0.0603\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 075 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 076 - training loss (MSE): 0.0611\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 077 - training loss (MSE): 0.0612\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 078 - training loss (MSE): 0.0579\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 079 - training loss (MSE): 0.0587\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 080 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 081 - training loss (MSE): 0.0570\n",
      "2025-05-23 15:42:54 [INFO]: Epoch 082 - training loss (MSE): 0.0603\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 083 - training loss (MSE): 0.0590\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 084 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 085 - training loss (MSE): 0.0565\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 086 - training loss (MSE): 0.0553\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 087 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 088 - training loss (MSE): 0.0528\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 089 - training loss (MSE): 0.0586\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 090 - training loss (MSE): 0.0582\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 091 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 092 - training loss (MSE): 0.0561\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 093 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 094 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:42:55 [INFO]: Epoch 095 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 096 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 097 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 098 - training loss (MSE): 0.0534\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 099 - training loss (MSE): 0.0515\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 100 - training loss (MSE): 0.0539\n",
      "2025-05-23 15:42:56 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:42:56,530] Trial 4 finished with value: 0.25178786891492727 and parameters: {'n_layers': 3, 'd_model': 32, 'd_ffn': 64, 'n_heads': 1, 'top_k': 3, 'n_kernels': 3, 'dropout': 0.5, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0017597216454587137, 'weight_decay': 0.0009307823237220275}. Best is trial 1 with value: 0.2375144542561454.\n",
      "2025-05-23 15:42:56 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:42:56 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:42:56 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:42:56 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 258,881\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 001 - training loss (MSE): 0.7176\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 002 - training loss (MSE): 0.4686\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 003 - training loss (MSE): 0.3322\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 004 - training loss (MSE): 0.2884\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 005 - training loss (MSE): 0.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:42:56 [INFO]: Epoch 006 - training loss (MSE): 0.2648\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 007 - training loss (MSE): 0.2491\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 008 - training loss (MSE): 0.2238\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 009 - training loss (MSE): 0.2134\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 010 - training loss (MSE): 0.1875\n",
      "2025-05-23 15:42:56 [INFO]: Epoch 011 - training loss (MSE): 0.1773\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 012 - training loss (MSE): 0.1709\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 013 - training loss (MSE): 0.1652\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 014 - training loss (MSE): 0.1494\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 015 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 016 - training loss (MSE): 0.1418\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 017 - training loss (MSE): 0.1335\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 018 - training loss (MSE): 0.1316\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 019 - training loss (MSE): 0.1188\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 020 - training loss (MSE): 0.1122\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 021 - training loss (MSE): 0.1137\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 022 - training loss (MSE): 0.1035\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 023 - training loss (MSE): 0.0988\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 024 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 025 - training loss (MSE): 0.0996\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 026 - training loss (MSE): 0.0942\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 027 - training loss (MSE): 0.0890\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 028 - training loss (MSE): 0.0848\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 029 - training loss (MSE): 0.0883\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 030 - training loss (MSE): 0.0828\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 031 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 032 - training loss (MSE): 0.0861\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 033 - training loss (MSE): 0.0776\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 034 - training loss (MSE): 0.0763\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 035 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 036 - training loss (MSE): 0.0789\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 037 - training loss (MSE): 0.0764\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 038 - training loss (MSE): 0.0749\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 039 - training loss (MSE): 0.0726\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 040 - training loss (MSE): 0.0686\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 041 - training loss (MSE): 0.0697\n",
      "2025-05-23 15:42:57 [INFO]: Epoch 042 - training loss (MSE): 0.0695\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 043 - training loss (MSE): 0.0695\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 044 - training loss (MSE): 0.0669\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 045 - training loss (MSE): 0.0672\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 046 - training loss (MSE): 0.0651\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 047 - training loss (MSE): 0.0638\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 048 - training loss (MSE): 0.0649\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 049 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 050 - training loss (MSE): 0.0599\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 051 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 052 - training loss (MSE): 0.0611\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 053 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 054 - training loss (MSE): 0.0602\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 055 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 056 - training loss (MSE): 0.0584\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 057 - training loss (MSE): 0.0537\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 058 - training loss (MSE): 0.0565\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 059 - training loss (MSE): 0.0586\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 060 - training loss (MSE): 0.0566\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 061 - training loss (MSE): 0.0561\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 062 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 063 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 064 - training loss (MSE): 0.0559\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 065 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 066 - training loss (MSE): 0.0522\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 067 - training loss (MSE): 0.0548\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 068 - training loss (MSE): 0.0536\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 069 - training loss (MSE): 0.0525\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 070 - training loss (MSE): 0.0540\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 071 - training loss (MSE): 0.0526\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 072 - training loss (MSE): 0.0508\n",
      "2025-05-23 15:42:58 [INFO]: Epoch 073 - training loss (MSE): 0.0536\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 074 - training loss (MSE): 0.0522\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 075 - training loss (MSE): 0.0486\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 076 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 077 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 078 - training loss (MSE): 0.0477\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 079 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 080 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 081 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 082 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 083 - training loss (MSE): 0.0456\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 084 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 085 - training loss (MSE): 0.0480\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 086 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 087 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 088 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 089 - training loss (MSE): 0.0453\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 090 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 091 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 092 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 093 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 094 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 095 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 096 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 097 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 098 - training loss (MSE): 0.0445\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 099 - training loss (MSE): 0.0451\n",
      "2025-05-23 15:42:59 [INFO]: Epoch 100 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:42:59 [INFO]: Finished training. The best model is from epoch#95.\n",
      "[I 2025-05-23 15:42:59,967] Trial 5 finished with value: 0.258135312566468 and parameters: {'n_layers': 3, 'd_model': 32, 'd_ffn': 16, 'n_heads': 2, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0010035341501560358, 'weight_decay': 0.000815755185102926}. Best is trial 1 with value: 0.2375144542561454.\n",
      "2025-05-23 15:43:00 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:00 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:00 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:00 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,033,857\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 001 - training loss (MSE): 1.7772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:00 [INFO]: Epoch 002 - training loss (MSE): 1.6666\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 003 - training loss (MSE): 1.5384\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 004 - training loss (MSE): 1.4215\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 005 - training loss (MSE): 1.3596\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 006 - training loss (MSE): 1.2266\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 007 - training loss (MSE): 1.1303\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 008 - training loss (MSE): 1.0428\n",
      "2025-05-23 15:43:00 [INFO]: Epoch 009 - training loss (MSE): 0.9492\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 010 - training loss (MSE): 0.8988\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 011 - training loss (MSE): 0.8174\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 012 - training loss (MSE): 0.7346\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 013 - training loss (MSE): 0.6295\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 014 - training loss (MSE): 0.5925\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 015 - training loss (MSE): 0.5477\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 016 - training loss (MSE): 0.4883\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 017 - training loss (MSE): 0.4422\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 018 - training loss (MSE): 0.3871\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 019 - training loss (MSE): 0.3739\n",
      "2025-05-23 15:43:01 [INFO]: Epoch 020 - training loss (MSE): 0.3384\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 021 - training loss (MSE): 0.3255\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 022 - training loss (MSE): 0.3052\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 023 - training loss (MSE): 0.3034\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 024 - training loss (MSE): 0.2889\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 025 - training loss (MSE): 0.2844\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 026 - training loss (MSE): 0.2736\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 027 - training loss (MSE): 0.2713\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 028 - training loss (MSE): 0.2695\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 029 - training loss (MSE): 0.2653\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 030 - training loss (MSE): 0.2592\n",
      "2025-05-23 15:43:02 [INFO]: Epoch 031 - training loss (MSE): 0.2544\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 032 - training loss (MSE): 0.2561\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 033 - training loss (MSE): 0.2452\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 034 - training loss (MSE): 0.2456\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 035 - training loss (MSE): 0.2484\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 036 - training loss (MSE): 0.2391\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 037 - training loss (MSE): 0.2432\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 038 - training loss (MSE): 0.2425\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 039 - training loss (MSE): 0.2293\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 040 - training loss (MSE): 0.2273\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 041 - training loss (MSE): 0.2301\n",
      "2025-05-23 15:43:03 [INFO]: Epoch 042 - training loss (MSE): 0.2290\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 043 - training loss (MSE): 0.2300\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 044 - training loss (MSE): 0.2183\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 045 - training loss (MSE): 0.2138\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 046 - training loss (MSE): 0.2138\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 047 - training loss (MSE): 0.2090\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 048 - training loss (MSE): 0.2153\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 049 - training loss (MSE): 0.2027\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 050 - training loss (MSE): 0.2077\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 051 - training loss (MSE): 0.2071\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 052 - training loss (MSE): 0.2094\n",
      "2025-05-23 15:43:04 [INFO]: Epoch 053 - training loss (MSE): 0.2071\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 054 - training loss (MSE): 0.2012\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 055 - training loss (MSE): 0.1970\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 056 - training loss (MSE): 0.2021\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 057 - training loss (MSE): 0.1919\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 058 - training loss (MSE): 0.1992\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 059 - training loss (MSE): 0.1936\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 060 - training loss (MSE): 0.1889\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 061 - training loss (MSE): 0.1911\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 062 - training loss (MSE): 0.1824\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 063 - training loss (MSE): 0.1827\n",
      "2025-05-23 15:43:05 [INFO]: Epoch 064 - training loss (MSE): 0.1836\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 065 - training loss (MSE): 0.1782\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 066 - training loss (MSE): 0.1742\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 067 - training loss (MSE): 0.1820\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 068 - training loss (MSE): 0.1761\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 069 - training loss (MSE): 0.1762\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 070 - training loss (MSE): 0.1671\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 071 - training loss (MSE): 0.1697\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 072 - training loss (MSE): 0.1671\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 073 - training loss (MSE): 0.1676\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 074 - training loss (MSE): 0.1648\n",
      "2025-05-23 15:43:06 [INFO]: Epoch 075 - training loss (MSE): 0.1715\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 076 - training loss (MSE): 0.1660\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 077 - training loss (MSE): 0.1617\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 078 - training loss (MSE): 0.1583\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 079 - training loss (MSE): 0.1629\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 080 - training loss (MSE): 0.1609\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 081 - training loss (MSE): 0.1573\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 082 - training loss (MSE): 0.1549\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 083 - training loss (MSE): 0.1581\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 084 - training loss (MSE): 0.1525\n",
      "2025-05-23 15:43:07 [INFO]: Epoch 085 - training loss (MSE): 0.1485\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 086 - training loss (MSE): 0.1497\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 087 - training loss (MSE): 0.1438\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 088 - training loss (MSE): 0.1450\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 089 - training loss (MSE): 0.1450\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 090 - training loss (MSE): 0.1441\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 091 - training loss (MSE): 0.1416\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 092 - training loss (MSE): 0.1452\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 093 - training loss (MSE): 0.1389\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 094 - training loss (MSE): 0.1374\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 095 - training loss (MSE): 0.1384\n",
      "2025-05-23 15:43:08 [INFO]: Epoch 096 - training loss (MSE): 0.1377\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 097 - training loss (MSE): 0.1358\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 098 - training loss (MSE): 0.1317\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 099 - training loss (MSE): 0.1347\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 100 - training loss (MSE): 0.1328\n",
      "2025-05-23 15:43:09 [INFO]: Finished training. The best model is from epoch#98.\n",
      "[I 2025-05-23 15:43:09,463] Trial 6 finished with value: 0.33889592841585414 and parameters: {'n_layers': 3, 'd_model': 64, 'd_ffn': 32, 'n_heads': 3, 'top_k': 2, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 7.411200761456233e-05, 'weight_decay': 0.0009705778096597715}. Best is trial 1 with value: 0.2375144542561454.\n",
      "2025-05-23 15:43:09 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:09 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:09 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:09 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 689,153\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 001 - training loss (MSE): 0.9247\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 002 - training loss (MSE): 0.3808\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 003 - training loss (MSE): 0.2453\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 004 - training loss (MSE): 0.1743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:09 [INFO]: Epoch 005 - training loss (MSE): 0.1621\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 006 - training loss (MSE): 0.1306\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 007 - training loss (MSE): 0.1113\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 008 - training loss (MSE): 0.1010\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 009 - training loss (MSE): 0.0941\n",
      "2025-05-23 15:43:09 [INFO]: Epoch 010 - training loss (MSE): 0.0956\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 011 - training loss (MSE): 0.0811\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 012 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 013 - training loss (MSE): 0.0745\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 014 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 015 - training loss (MSE): 0.0654\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 016 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 017 - training loss (MSE): 0.0628\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 018 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 019 - training loss (MSE): 0.0541\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 020 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 021 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 022 - training loss (MSE): 0.0447\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 023 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 024 - training loss (MSE): 0.0436\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 025 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 026 - training loss (MSE): 0.0379\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 027 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 028 - training loss (MSE): 0.0366\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 029 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 030 - training loss (MSE): 0.0358\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 031 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 032 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 033 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:43:10 [INFO]: Epoch 034 - training loss (MSE): 0.0324\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 035 - training loss (MSE): 0.0333\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 036 - training loss (MSE): 0.0299\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 037 - training loss (MSE): 0.0314\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 038 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 039 - training loss (MSE): 0.0324\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 040 - training loss (MSE): 0.0304\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 041 - training loss (MSE): 0.0281\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 042 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 043 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 044 - training loss (MSE): 0.0287\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 045 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 046 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 047 - training loss (MSE): 0.0291\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 048 - training loss (MSE): 0.0298\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 049 - training loss (MSE): 0.0305\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 050 - training loss (MSE): 0.0316\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 051 - training loss (MSE): 0.0274\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 052 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 053 - training loss (MSE): 0.0292\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 054 - training loss (MSE): 0.0294\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 055 - training loss (MSE): 0.0289\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 056 - training loss (MSE): 0.0274\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 057 - training loss (MSE): 0.0277\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 058 - training loss (MSE): 0.0290\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 059 - training loss (MSE): 0.0266\n",
      "2025-05-23 15:43:11 [INFO]: Epoch 060 - training loss (MSE): 0.0265\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 061 - training loss (MSE): 0.0267\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 062 - training loss (MSE): 0.0270\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 063 - training loss (MSE): 0.0272\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 064 - training loss (MSE): 0.0255\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 065 - training loss (MSE): 0.0290\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 066 - training loss (MSE): 0.0259\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 067 - training loss (MSE): 0.0268\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 068 - training loss (MSE): 0.0269\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 069 - training loss (MSE): 0.0272\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 070 - training loss (MSE): 0.0277\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 071 - training loss (MSE): 0.0268\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 072 - training loss (MSE): 0.0267\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 073 - training loss (MSE): 0.0266\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 074 - training loss (MSE): 0.0283\n",
      "2025-05-23 15:43:12 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:43:12 [INFO]: Finished training. The best model is from epoch#64.\n",
      "[I 2025-05-23 15:43:12,655] Trial 7 finished with value: 0.23652038096985437 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0038565644906092124, 'weight_decay': 0.00023577029727229654}. Best is trial 7 with value: 0.23652038096985437.\n",
      "2025-05-23 15:43:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:12 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:12 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:12 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,353,153\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 001 - training loss (MSE): 1.0275\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 002 - training loss (MSE): 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:12 [INFO]: Epoch 003 - training loss (MSE): 0.7230\n",
      "2025-05-23 15:43:12 [INFO]: Epoch 004 - training loss (MSE): 0.5795\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 005 - training loss (MSE): 0.4570\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 006 - training loss (MSE): 0.3789\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 007 - training loss (MSE): 0.3223\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 008 - training loss (MSE): 0.3208\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 009 - training loss (MSE): 0.3252\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 010 - training loss (MSE): 0.3166\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 011 - training loss (MSE): 0.3054\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 012 - training loss (MSE): 0.2786\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 013 - training loss (MSE): 0.2731\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 014 - training loss (MSE): 0.2587\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 015 - training loss (MSE): 0.2577\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 016 - training loss (MSE): 0.2497\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 017 - training loss (MSE): 0.2456\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 018 - training loss (MSE): 0.2412\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 019 - training loss (MSE): 0.2324\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 020 - training loss (MSE): 0.2211\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 021 - training loss (MSE): 0.2230\n",
      "2025-05-23 15:43:13 [INFO]: Epoch 022 - training loss (MSE): 0.2155\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 023 - training loss (MSE): 0.2140\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 024 - training loss (MSE): 0.1995\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 025 - training loss (MSE): 0.1906\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 026 - training loss (MSE): 0.1908\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 027 - training loss (MSE): 0.1881\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 028 - training loss (MSE): 0.1808\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 029 - training loss (MSE): 0.1726\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 030 - training loss (MSE): 0.1703\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 031 - training loss (MSE): 0.1639\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 032 - training loss (MSE): 0.1590\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 033 - training loss (MSE): 0.1588\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 034 - training loss (MSE): 0.1503\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 035 - training loss (MSE): 0.1484\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 036 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 037 - training loss (MSE): 0.1420\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 038 - training loss (MSE): 0.1380\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 039 - training loss (MSE): 0.1415\n",
      "2025-05-23 15:43:14 [INFO]: Epoch 040 - training loss (MSE): 0.1328\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 041 - training loss (MSE): 0.1312\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 042 - training loss (MSE): 0.1280\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 043 - training loss (MSE): 0.1316\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 044 - training loss (MSE): 0.1236\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 045 - training loss (MSE): 0.1212\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 046 - training loss (MSE): 0.1149\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 047 - training loss (MSE): 0.1115\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 048 - training loss (MSE): 0.1092\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 049 - training loss (MSE): 0.1134\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 050 - training loss (MSE): 0.1097\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 051 - training loss (MSE): 0.1068\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 052 - training loss (MSE): 0.0994\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 053 - training loss (MSE): 0.1080\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 054 - training loss (MSE): 0.1041\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 055 - training loss (MSE): 0.1020\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 056 - training loss (MSE): 0.0986\n",
      "2025-05-23 15:43:15 [INFO]: Epoch 057 - training loss (MSE): 0.0962\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 058 - training loss (MSE): 0.0961\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 059 - training loss (MSE): 0.0912\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 060 - training loss (MSE): 0.0953\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 061 - training loss (MSE): 0.0912\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 062 - training loss (MSE): 0.0918\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 063 - training loss (MSE): 0.0900\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 064 - training loss (MSE): 0.0887\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 065 - training loss (MSE): 0.0859\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 066 - training loss (MSE): 0.0846\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 067 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 068 - training loss (MSE): 0.0821\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 069 - training loss (MSE): 0.0804\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 070 - training loss (MSE): 0.0803\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 071 - training loss (MSE): 0.0779\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 072 - training loss (MSE): 0.0770\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 073 - training loss (MSE): 0.0771\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 074 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:43:16 [INFO]: Epoch 075 - training loss (MSE): 0.0735\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 076 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 077 - training loss (MSE): 0.0747\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 078 - training loss (MSE): 0.0763\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 079 - training loss (MSE): 0.0715\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 080 - training loss (MSE): 0.0701\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 081 - training loss (MSE): 0.0712\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 082 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 083 - training loss (MSE): 0.0732\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 084 - training loss (MSE): 0.0669\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 085 - training loss (MSE): 0.0696\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 086 - training loss (MSE): 0.0718\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 087 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 088 - training loss (MSE): 0.0673\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 089 - training loss (MSE): 0.0675\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 090 - training loss (MSE): 0.0666\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 091 - training loss (MSE): 0.0681\n",
      "2025-05-23 15:43:17 [INFO]: Epoch 092 - training loss (MSE): 0.0662\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 093 - training loss (MSE): 0.0667\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 094 - training loss (MSE): 0.0666\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 095 - training loss (MSE): 0.0672\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 096 - training loss (MSE): 0.0697\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 097 - training loss (MSE): 0.0631\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 098 - training loss (MSE): 0.0626\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 099 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 100 - training loss (MSE): 0.0624\n",
      "2025-05-23 15:43:18 [INFO]: Finished training. The best model is from epoch#100.\n",
      "[I 2025-05-23 15:43:18,552] Trial 8 finished with value: 0.25138395308951633 and parameters: {'n_layers': 2, 'd_model': 64, 'd_ffn': 32, 'n_heads': 1, 'top_k': 1, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.1, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0002128524987103289, 'weight_decay': 0.0007940117135494467}. Best is trial 7 with value: 0.23652038096985437.\n",
      "2025-05-23 15:43:18 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:18 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:18 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:18 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 36,569\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 001 - training loss (MSE): 0.5229\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 002 - training loss (MSE): 0.2811\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 003 - training loss (MSE): 0.2065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:18 [INFO]: Epoch 004 - training loss (MSE): 0.1635\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 005 - training loss (MSE): 0.1304\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 006 - training loss (MSE): 0.1199\n",
      "2025-05-23 15:43:18 [INFO]: Epoch 007 - training loss (MSE): 0.1030\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 008 - training loss (MSE): 0.0861\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 009 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 010 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 011 - training loss (MSE): 0.0601\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 012 - training loss (MSE): 0.0587\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 013 - training loss (MSE): 0.0514\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 014 - training loss (MSE): 0.0505\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 015 - training loss (MSE): 0.0465\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 016 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 017 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 018 - training loss (MSE): 0.0418\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 019 - training loss (MSE): 0.0407\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 020 - training loss (MSE): 0.0361\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 021 - training loss (MSE): 0.0362\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 022 - training loss (MSE): 0.0359\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 023 - training loss (MSE): 0.0354\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 024 - training loss (MSE): 0.0350\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 025 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 026 - training loss (MSE): 0.0359\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 027 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:43:19 [INFO]: Epoch 028 - training loss (MSE): 0.0323\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 029 - training loss (MSE): 0.0301\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 030 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 031 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 032 - training loss (MSE): 0.0331\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 033 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 034 - training loss (MSE): 0.0322\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 035 - training loss (MSE): 0.0312\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 036 - training loss (MSE): 0.0323\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 037 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 038 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 039 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 040 - training loss (MSE): 0.0310\n",
      "2025-05-23 15:43:20 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:43:20 [INFO]: Finished training. The best model is from epoch#30.\n",
      "[I 2025-05-23 15:43:20,637] Trial 9 finished with value: 0.3094239771068529 and parameters: {'n_layers': 1, 'd_model': 64, 'd_ffn': 8, 'n_heads': 3, 'top_k': 3, 'n_kernels': 3, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.007691181798461813, 'weight_decay': 0.0006470205285361298}. Best is trial 7 with value: 0.23652038096985437.\n",
      "2025-05-23 15:43:20 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:20 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:20 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:20 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 172,673\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 001 - training loss (MSE): 1.7583\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 002 - training loss (MSE): 1.1200\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 003 - training loss (MSE): 0.5898\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 004 - training loss (MSE): 0.3335\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 005 - training loss (MSE): 0.2543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:20 [INFO]: Epoch 006 - training loss (MSE): 0.2499\n",
      "2025-05-23 15:43:20 [INFO]: Epoch 007 - training loss (MSE): 0.2499\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 008 - training loss (MSE): 0.2540\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 009 - training loss (MSE): 0.2520\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 010 - training loss (MSE): 0.2518\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 011 - training loss (MSE): 0.2534\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 012 - training loss (MSE): 0.2479\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 013 - training loss (MSE): 0.2366\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 014 - training loss (MSE): 0.2324\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 015 - training loss (MSE): 0.2200\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 016 - training loss (MSE): 0.2080\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 017 - training loss (MSE): 0.1974\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 018 - training loss (MSE): 0.1819\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 019 - training loss (MSE): 0.1780\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 020 - training loss (MSE): 0.1746\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 021 - training loss (MSE): 0.1545\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 022 - training loss (MSE): 0.1513\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 023 - training loss (MSE): 0.1367\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 024 - training loss (MSE): 0.1350\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 025 - training loss (MSE): 0.1282\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 026 - training loss (MSE): 0.1302\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 027 - training loss (MSE): 0.1187\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 028 - training loss (MSE): 0.1149\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 029 - training loss (MSE): 0.1068\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 030 - training loss (MSE): 0.1099\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 031 - training loss (MSE): 0.0999\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 032 - training loss (MSE): 0.1013\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 033 - training loss (MSE): 0.0940\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 034 - training loss (MSE): 0.0896\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 035 - training loss (MSE): 0.0914\n",
      "2025-05-23 15:43:21 [INFO]: Epoch 036 - training loss (MSE): 0.0893\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 037 - training loss (MSE): 0.0875\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 038 - training loss (MSE): 0.0847\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 039 - training loss (MSE): 0.0840\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 040 - training loss (MSE): 0.0829\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 041 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 042 - training loss (MSE): 0.0808\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 043 - training loss (MSE): 0.0840\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 044 - training loss (MSE): 0.0823\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 045 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 046 - training loss (MSE): 0.0738\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 047 - training loss (MSE): 0.0728\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 048 - training loss (MSE): 0.0749\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 049 - training loss (MSE): 0.0719\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 050 - training loss (MSE): 0.0739\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 051 - training loss (MSE): 0.0697\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 052 - training loss (MSE): 0.0671\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 053 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 054 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 055 - training loss (MSE): 0.0719\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 056 - training loss (MSE): 0.0700\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 057 - training loss (MSE): 0.0644\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 058 - training loss (MSE): 0.0672\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 059 - training loss (MSE): 0.0659\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 060 - training loss (MSE): 0.0670\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 061 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 062 - training loss (MSE): 0.0642\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 063 - training loss (MSE): 0.0663\n",
      "2025-05-23 15:43:22 [INFO]: Epoch 064 - training loss (MSE): 0.0641\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 065 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 066 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 067 - training loss (MSE): 0.0731\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 068 - training loss (MSE): 0.0649\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 069 - training loss (MSE): 0.0643\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 070 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 071 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 072 - training loss (MSE): 0.0622\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 073 - training loss (MSE): 0.0635\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 074 - training loss (MSE): 0.0640\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 075 - training loss (MSE): 0.0609\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 076 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 077 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 078 - training loss (MSE): 0.0609\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 079 - training loss (MSE): 0.0598\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 080 - training loss (MSE): 0.0608\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 081 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 082 - training loss (MSE): 0.0607\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 083 - training loss (MSE): 0.0576\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 084 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 085 - training loss (MSE): 0.0607\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 086 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 087 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 088 - training loss (MSE): 0.0588\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 089 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 090 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 091 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 092 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 093 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:43:23 [INFO]: Epoch 094 - training loss (MSE): 0.0566\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 095 - training loss (MSE): 0.0587\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 096 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 097 - training loss (MSE): 0.0537\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 098 - training loss (MSE): 0.0539\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 099 - training loss (MSE): 0.0564\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 100 - training loss (MSE): 0.0548\n",
      "2025-05-23 15:43:24 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:43:24,345] Trial 10 finished with value: 0.2714808487888748 and parameters: {'n_layers': 2, 'd_model': 8, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.1, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0022922125037629845, 'weight_decay': 6.245289685592381e-05}. Best is trial 7 with value: 0.23652038096985437.\n",
      "2025-05-23 15:43:24 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:24 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:24 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:24 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 677,585\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 001 - training loss (MSE): 0.6369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:24 [INFO]: Epoch 002 - training loss (MSE): 0.4173\n",
      "2025-05-23 15:43:24 [INFO]: Epoch 003 - training loss (MSE): 0.3887\n",
      "2025-05-23 15:43:25 [INFO]: Epoch 004 - training loss (MSE): 0.3141\n",
      "2025-05-23 15:43:25 [INFO]: Epoch 005 - training loss (MSE): 0.2890\n",
      "2025-05-23 15:43:25 [INFO]: Epoch 006 - training loss (MSE): 0.2720\n",
      "2025-05-23 15:43:25 [INFO]: Epoch 007 - training loss (MSE): 0.2512\n",
      "2025-05-23 15:43:25 [INFO]: Epoch 008 - training loss (MSE): 0.2292\n",
      "2025-05-23 15:43:25 [INFO]: Epoch 009 - training loss (MSE): 0.2062\n",
      "2025-05-23 15:43:25 [INFO]: Epoch 010 - training loss (MSE): 0.1981\n",
      "2025-05-23 15:43:26 [INFO]: Epoch 011 - training loss (MSE): 0.1780\n",
      "2025-05-23 15:43:26 [INFO]: Epoch 012 - training loss (MSE): 0.1706\n",
      "2025-05-23 15:43:26 [INFO]: Epoch 013 - training loss (MSE): 0.1524\n",
      "2025-05-23 15:43:26 [INFO]: Epoch 014 - training loss (MSE): 0.1503\n",
      "2025-05-23 15:43:26 [INFO]: Epoch 015 - training loss (MSE): 0.1375\n",
      "2025-05-23 15:43:26 [INFO]: Epoch 016 - training loss (MSE): 0.1309\n",
      "2025-05-23 15:43:26 [INFO]: Epoch 017 - training loss (MSE): 0.1273\n",
      "2025-05-23 15:43:27 [INFO]: Epoch 018 - training loss (MSE): 0.1197\n",
      "2025-05-23 15:43:27 [INFO]: Epoch 019 - training loss (MSE): 0.1145\n",
      "2025-05-23 15:43:27 [INFO]: Epoch 020 - training loss (MSE): 0.1062\n",
      "2025-05-23 15:43:27 [INFO]: Epoch 021 - training loss (MSE): 0.0997\n",
      "2025-05-23 15:43:27 [INFO]: Epoch 022 - training loss (MSE): 0.0953\n",
      "2025-05-23 15:43:27 [INFO]: Epoch 023 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:43:27 [INFO]: Epoch 024 - training loss (MSE): 0.0878\n",
      "2025-05-23 15:43:28 [INFO]: Epoch 025 - training loss (MSE): 0.0847\n",
      "2025-05-23 15:43:28 [INFO]: Epoch 026 - training loss (MSE): 0.0816\n",
      "2025-05-23 15:43:28 [INFO]: Epoch 027 - training loss (MSE): 0.0797\n",
      "2025-05-23 15:43:28 [INFO]: Epoch 028 - training loss (MSE): 0.0809\n",
      "2025-05-23 15:43:28 [INFO]: Epoch 029 - training loss (MSE): 0.0798\n",
      "2025-05-23 15:43:28 [INFO]: Epoch 030 - training loss (MSE): 0.0752\n",
      "2025-05-23 15:43:28 [INFO]: Epoch 031 - training loss (MSE): 0.0704\n",
      "2025-05-23 15:43:29 [INFO]: Epoch 032 - training loss (MSE): 0.0707\n",
      "2025-05-23 15:43:29 [INFO]: Epoch 033 - training loss (MSE): 0.0681\n",
      "2025-05-23 15:43:29 [INFO]: Epoch 034 - training loss (MSE): 0.0692\n",
      "2025-05-23 15:43:29 [INFO]: Epoch 035 - training loss (MSE): 0.0631\n",
      "2025-05-23 15:43:29 [INFO]: Epoch 036 - training loss (MSE): 0.0661\n",
      "2025-05-23 15:43:29 [INFO]: Epoch 037 - training loss (MSE): 0.0613\n",
      "2025-05-23 15:43:29 [INFO]: Epoch 038 - training loss (MSE): 0.0585\n",
      "2025-05-23 15:43:30 [INFO]: Epoch 039 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:43:30 [INFO]: Epoch 040 - training loss (MSE): 0.0588\n",
      "2025-05-23 15:43:30 [INFO]: Epoch 041 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:43:30 [INFO]: Epoch 042 - training loss (MSE): 0.0612\n",
      "2025-05-23 15:43:30 [INFO]: Epoch 043 - training loss (MSE): 0.0582\n",
      "2025-05-23 15:43:30 [INFO]: Epoch 044 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:43:30 [INFO]: Epoch 045 - training loss (MSE): 0.0550\n",
      "2025-05-23 15:43:31 [INFO]: Epoch 046 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:43:31 [INFO]: Epoch 047 - training loss (MSE): 0.0569\n",
      "2025-05-23 15:43:31 [INFO]: Epoch 048 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:43:31 [INFO]: Epoch 049 - training loss (MSE): 0.0516\n",
      "2025-05-23 15:43:31 [INFO]: Epoch 050 - training loss (MSE): 0.0547\n",
      "2025-05-23 15:43:31 [INFO]: Epoch 051 - training loss (MSE): 0.0525\n",
      "2025-05-23 15:43:31 [INFO]: Epoch 052 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:43:32 [INFO]: Epoch 053 - training loss (MSE): 0.0520\n",
      "2025-05-23 15:43:32 [INFO]: Epoch 054 - training loss (MSE): 0.0519\n",
      "2025-05-23 15:43:32 [INFO]: Epoch 055 - training loss (MSE): 0.0524\n",
      "2025-05-23 15:43:32 [INFO]: Epoch 056 - training loss (MSE): 0.0488\n",
      "2025-05-23 15:43:32 [INFO]: Epoch 057 - training loss (MSE): 0.0505\n",
      "2025-05-23 15:43:32 [INFO]: Epoch 058 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:43:32 [INFO]: Epoch 059 - training loss (MSE): 0.0474\n",
      "2025-05-23 15:43:33 [INFO]: Epoch 060 - training loss (MSE): 0.0468\n",
      "2025-05-23 15:43:33 [INFO]: Epoch 061 - training loss (MSE): 0.0467\n",
      "2025-05-23 15:43:33 [INFO]: Epoch 062 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:43:33 [INFO]: Epoch 063 - training loss (MSE): 0.0437\n",
      "2025-05-23 15:43:33 [INFO]: Epoch 064 - training loss (MSE): 0.0478\n",
      "2025-05-23 15:43:33 [INFO]: Epoch 065 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:43:33 [INFO]: Epoch 066 - training loss (MSE): 0.0455\n",
      "2025-05-23 15:43:34 [INFO]: Epoch 067 - training loss (MSE): 0.0458\n",
      "2025-05-23 15:43:34 [INFO]: Epoch 068 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:43:34 [INFO]: Epoch 069 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:43:34 [INFO]: Epoch 070 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:43:34 [INFO]: Epoch 071 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:43:34 [INFO]: Epoch 072 - training loss (MSE): 0.0459\n",
      "2025-05-23 15:43:34 [INFO]: Epoch 073 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:43:34 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:43:34 [INFO]: Finished training. The best model is from epoch#63.\n",
      "[I 2025-05-23 15:43:35,068] Trial 11 finished with value: 0.24093841386033335 and parameters: {'n_layers': 1, 'd_model': 128, 'd_ffn': 16, 'n_heads': 2, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.2, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0005597413939486375, 'weight_decay': 0.0002858718430285778}. Best is trial 7 with value: 0.23652038096985437.\n",
      "2025-05-23 15:43:35 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:35 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:35 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:35 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 339,625\n",
      "2025-05-23 15:43:35 [INFO]: Epoch 001 - training loss (MSE): 0.4977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:35 [INFO]: Epoch 002 - training loss (MSE): 0.4054\n",
      "2025-05-23 15:43:35 [INFO]: Epoch 003 - training loss (MSE): 0.3669\n",
      "2025-05-23 15:43:35 [INFO]: Epoch 004 - training loss (MSE): 0.3398\n",
      "2025-05-23 15:43:35 [INFO]: Epoch 005 - training loss (MSE): 0.3162\n",
      "2025-05-23 15:43:35 [INFO]: Epoch 006 - training loss (MSE): 0.2808\n",
      "2025-05-23 15:43:36 [INFO]: Epoch 007 - training loss (MSE): 0.2731\n",
      "2025-05-23 15:43:36 [INFO]: Epoch 008 - training loss (MSE): 0.2506\n",
      "2025-05-23 15:43:36 [INFO]: Epoch 009 - training loss (MSE): 0.2382\n",
      "2025-05-23 15:43:36 [INFO]: Epoch 010 - training loss (MSE): 0.2289\n",
      "2025-05-23 15:43:36 [INFO]: Epoch 011 - training loss (MSE): 0.2123\n",
      "2025-05-23 15:43:36 [INFO]: Epoch 012 - training loss (MSE): 0.2059\n",
      "2025-05-23 15:43:36 [INFO]: Epoch 013 - training loss (MSE): 0.1926\n",
      "2025-05-23 15:43:37 [INFO]: Epoch 014 - training loss (MSE): 0.1873\n",
      "2025-05-23 15:43:37 [INFO]: Epoch 015 - training loss (MSE): 0.1715\n",
      "2025-05-23 15:43:37 [INFO]: Epoch 016 - training loss (MSE): 0.1575\n",
      "2025-05-23 15:43:37 [INFO]: Epoch 017 - training loss (MSE): 0.1553\n",
      "2025-05-23 15:43:37 [INFO]: Epoch 018 - training loss (MSE): 0.1444\n",
      "2025-05-23 15:43:37 [INFO]: Epoch 019 - training loss (MSE): 0.1344\n",
      "2025-05-23 15:43:37 [INFO]: Epoch 020 - training loss (MSE): 0.1283\n",
      "2025-05-23 15:43:38 [INFO]: Epoch 021 - training loss (MSE): 0.1196\n",
      "2025-05-23 15:43:38 [INFO]: Epoch 022 - training loss (MSE): 0.1116\n",
      "2025-05-23 15:43:38 [INFO]: Epoch 023 - training loss (MSE): 0.1145\n",
      "2025-05-23 15:43:38 [INFO]: Epoch 024 - training loss (MSE): 0.1089\n",
      "2025-05-23 15:43:38 [INFO]: Epoch 025 - training loss (MSE): 0.0982\n",
      "2025-05-23 15:43:38 [INFO]: Epoch 026 - training loss (MSE): 0.0990\n",
      "2025-05-23 15:43:38 [INFO]: Epoch 027 - training loss (MSE): 0.0926\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 028 - training loss (MSE): 0.0938\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 029 - training loss (MSE): 0.0858\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 030 - training loss (MSE): 0.0871\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 031 - training loss (MSE): 0.0865\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 032 - training loss (MSE): 0.0876\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 033 - training loss (MSE): 0.0799\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 034 - training loss (MSE): 0.0788\n",
      "2025-05-23 15:43:39 [INFO]: Epoch 035 - training loss (MSE): 0.0771\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 036 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 037 - training loss (MSE): 0.0811\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 038 - training loss (MSE): 0.0769\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 039 - training loss (MSE): 0.0705\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 040 - training loss (MSE): 0.0704\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 041 - training loss (MSE): 0.0710\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 042 - training loss (MSE): 0.0685\n",
      "2025-05-23 15:43:40 [INFO]: Epoch 043 - training loss (MSE): 0.0719\n",
      "2025-05-23 15:43:41 [INFO]: Epoch 044 - training loss (MSE): 0.0736\n",
      "2025-05-23 15:43:41 [INFO]: Epoch 045 - training loss (MSE): 0.0712\n",
      "2025-05-23 15:43:41 [INFO]: Epoch 046 - training loss (MSE): 0.0711\n",
      "2025-05-23 15:43:41 [INFO]: Epoch 047 - training loss (MSE): 0.0728\n",
      "2025-05-23 15:43:41 [INFO]: Epoch 048 - training loss (MSE): 0.0678\n",
      "2025-05-23 15:43:41 [INFO]: Epoch 049 - training loss (MSE): 0.0639\n",
      "2025-05-23 15:43:41 [INFO]: Epoch 050 - training loss (MSE): 0.0631\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 051 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 052 - training loss (MSE): 0.0650\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 053 - training loss (MSE): 0.0628\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 054 - training loss (MSE): 0.0634\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 055 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 056 - training loss (MSE): 0.0657\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 057 - training loss (MSE): 0.0646\n",
      "2025-05-23 15:43:42 [INFO]: Epoch 058 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:43:43 [INFO]: Epoch 059 - training loss (MSE): 0.0653\n",
      "2025-05-23 15:43:43 [INFO]: Epoch 060 - training loss (MSE): 0.0596\n",
      "2025-05-23 15:43:43 [INFO]: Epoch 061 - training loss (MSE): 0.0612\n",
      "2025-05-23 15:43:43 [INFO]: Epoch 062 - training loss (MSE): 0.0593\n",
      "2025-05-23 15:43:43 [INFO]: Epoch 063 - training loss (MSE): 0.0623\n",
      "2025-05-23 15:43:43 [INFO]: Epoch 064 - training loss (MSE): 0.0618\n",
      "2025-05-23 15:43:43 [INFO]: Epoch 065 - training loss (MSE): 0.0586\n",
      "2025-05-23 15:43:44 [INFO]: Epoch 066 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:43:44 [INFO]: Epoch 067 - training loss (MSE): 0.0607\n",
      "2025-05-23 15:43:44 [INFO]: Epoch 068 - training loss (MSE): 0.0558\n",
      "2025-05-23 15:43:44 [INFO]: Epoch 069 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:43:44 [INFO]: Epoch 070 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:43:44 [INFO]: Epoch 071 - training loss (MSE): 0.0600\n",
      "2025-05-23 15:43:44 [INFO]: Epoch 072 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 073 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 074 - training loss (MSE): 0.0568\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 075 - training loss (MSE): 0.0577\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 076 - training loss (MSE): 0.0564\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 077 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 078 - training loss (MSE): 0.0554\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 079 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:43:45 [INFO]: Epoch 080 - training loss (MSE): 0.0573\n",
      "2025-05-23 15:43:46 [INFO]: Epoch 081 - training loss (MSE): 0.0590\n",
      "2025-05-23 15:43:46 [INFO]: Epoch 082 - training loss (MSE): 0.0599\n",
      "2025-05-23 15:43:46 [INFO]: Epoch 083 - training loss (MSE): 0.0561\n",
      "2025-05-23 15:43:46 [INFO]: Epoch 084 - training loss (MSE): 0.0559\n",
      "2025-05-23 15:43:46 [INFO]: Epoch 085 - training loss (MSE): 0.0597\n",
      "2025-05-23 15:43:46 [INFO]: Epoch 086 - training loss (MSE): 0.0580\n",
      "2025-05-23 15:43:46 [INFO]: Epoch 087 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 088 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:43:47 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:43:47 [INFO]: Finished training. The best model is from epoch#78.\n",
      "[I 2025-05-23 15:43:47,248] Trial 12 finished with value: 0.2736473437231825 and parameters: {'n_layers': 1, 'd_model': 128, 'd_ffn': 8, 'n_heads': 3, 'top_k': 2, 'n_kernels': 5, 'dropout': 0.3, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.5, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0004127289522954379, 'weight_decay': 0.00026744327270321225}. Best is trial 7 with value: 0.23652038096985437.\n",
      "2025-05-23 15:43:47 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:47 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:47 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:47 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,737\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 001 - training loss (MSE): 1.0762\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 002 - training loss (MSE): 0.4315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:47 [INFO]: Epoch 003 - training loss (MSE): 0.3429\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 004 - training loss (MSE): 0.3151\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 005 - training loss (MSE): 0.2428\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 006 - training loss (MSE): 0.2308\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 007 - training loss (MSE): 0.2081\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 008 - training loss (MSE): 0.1810\n",
      "2025-05-23 15:43:47 [INFO]: Epoch 009 - training loss (MSE): 0.1724\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 010 - training loss (MSE): 0.1517\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 011 - training loss (MSE): 0.1469\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 012 - training loss (MSE): 0.1338\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 013 - training loss (MSE): 0.1281\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 014 - training loss (MSE): 0.1146\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 015 - training loss (MSE): 0.1083\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 016 - training loss (MSE): 0.1016\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 017 - training loss (MSE): 0.0955\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 018 - training loss (MSE): 0.0926\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 019 - training loss (MSE): 0.0833\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 020 - training loss (MSE): 0.0883\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 021 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 022 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:43:48 [INFO]: Epoch 023 - training loss (MSE): 0.0806\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 024 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 025 - training loss (MSE): 0.0743\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 026 - training loss (MSE): 0.0687\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 027 - training loss (MSE): 0.0703\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 028 - training loss (MSE): 0.0706\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 029 - training loss (MSE): 0.0654\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 030 - training loss (MSE): 0.0639\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 031 - training loss (MSE): 0.0620\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 032 - training loss (MSE): 0.0613\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 033 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 034 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 035 - training loss (MSE): 0.0578\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 036 - training loss (MSE): 0.0574\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 037 - training loss (MSE): 0.0599\n",
      "2025-05-23 15:43:49 [INFO]: Epoch 038 - training loss (MSE): 0.0584\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 039 - training loss (MSE): 0.0577\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 040 - training loss (MSE): 0.0585\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 041 - training loss (MSE): 0.0580\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 042 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 043 - training loss (MSE): 0.0565\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 044 - training loss (MSE): 0.0543\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 045 - training loss (MSE): 0.0528\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 046 - training loss (MSE): 0.0529\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 047 - training loss (MSE): 0.0487\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 048 - training loss (MSE): 0.0513\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 049 - training loss (MSE): 0.0492\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 050 - training loss (MSE): 0.0550\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 051 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:43:50 [INFO]: Epoch 052 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 053 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 054 - training loss (MSE): 0.0483\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 055 - training loss (MSE): 0.0480\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 056 - training loss (MSE): 0.0494\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 057 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 058 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 059 - training loss (MSE): 0.0459\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 060 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 061 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 062 - training loss (MSE): 0.0457\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 063 - training loss (MSE): 0.0449\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 064 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 065 - training loss (MSE): 0.0422\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 066 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:43:51 [INFO]: Epoch 067 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 068 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 069 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 070 - training loss (MSE): 0.0450\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 071 - training loss (MSE): 0.0391\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 072 - training loss (MSE): 0.0403\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 073 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 074 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 075 - training loss (MSE): 0.0414\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 076 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 077 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 078 - training loss (MSE): 0.0407\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 079 - training loss (MSE): 0.0390\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 080 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:43:52 [INFO]: Epoch 081 - training loss (MSE): 0.0359\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 082 - training loss (MSE): 0.0373\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 083 - training loss (MSE): 0.0371\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 084 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 085 - training loss (MSE): 0.0348\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 086 - training loss (MSE): 0.0368\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 087 - training loss (MSE): 0.0384\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 088 - training loss (MSE): 0.0379\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 089 - training loss (MSE): 0.0344\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 090 - training loss (MSE): 0.0374\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 091 - training loss (MSE): 0.0332\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 092 - training loss (MSE): 0.0346\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 093 - training loss (MSE): 0.0350\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 094 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:43:53 [INFO]: Epoch 095 - training loss (MSE): 0.0341\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 096 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 097 - training loss (MSE): 0.0353\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 098 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 099 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 100 - training loss (MSE): 0.0328\n",
      "2025-05-23 15:43:54 [INFO]: Finished training. The best model is from epoch#100.\n",
      "[I 2025-05-23 15:43:54,484] Trial 13 finished with value: 0.23603596570103588 and parameters: {'n_layers': 1, 'd_model': 32, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 5, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0012427183476582465, 'weight_decay': 0.0005072807145484922}. Best is trial 13 with value: 0.23603596570103588.\n",
      "2025-05-23 15:43:54 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:54 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:54 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:54 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 287,457\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 001 - training loss (MSE): 0.8872\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 002 - training loss (MSE): 0.4284\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 003 - training loss (MSE): 0.2586\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 004 - training loss (MSE): 0.2776\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 005 - training loss (MSE): 0.2127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:54 [INFO]: Epoch 006 - training loss (MSE): 0.1963\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 007 - training loss (MSE): 0.1796\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 008 - training loss (MSE): 0.1570\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 009 - training loss (MSE): 0.1401\n",
      "2025-05-23 15:43:54 [INFO]: Epoch 010 - training loss (MSE): 0.1380\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 011 - training loss (MSE): 0.1121\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 012 - training loss (MSE): 0.1069\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 013 - training loss (MSE): 0.0973\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 014 - training loss (MSE): 0.0912\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 015 - training loss (MSE): 0.0969\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 016 - training loss (MSE): 0.0838\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 017 - training loss (MSE): 0.0853\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 018 - training loss (MSE): 0.0786\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 019 - training loss (MSE): 0.0751\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 020 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 021 - training loss (MSE): 0.0724\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 022 - training loss (MSE): 0.0679\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 023 - training loss (MSE): 0.0699\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 024 - training loss (MSE): 0.0645\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 025 - training loss (MSE): 0.0625\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 026 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 027 - training loss (MSE): 0.0629\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 028 - training loss (MSE): 0.0585\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 029 - training loss (MSE): 0.0582\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 030 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 031 - training loss (MSE): 0.0532\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 032 - training loss (MSE): 0.0546\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 033 - training loss (MSE): 0.0525\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 034 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 035 - training loss (MSE): 0.0515\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 036 - training loss (MSE): 0.0455\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 037 - training loss (MSE): 0.0481\n",
      "2025-05-23 15:43:55 [INFO]: Epoch 038 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 039 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 040 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 041 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 042 - training loss (MSE): 0.0460\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 043 - training loss (MSE): 0.0412\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 044 - training loss (MSE): 0.0426\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 045 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 046 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 047 - training loss (MSE): 0.0405\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 048 - training loss (MSE): 0.0403\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 049 - training loss (MSE): 0.0399\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 050 - training loss (MSE): 0.0375\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 051 - training loss (MSE): 0.0370\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 052 - training loss (MSE): 0.0380\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 053 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 054 - training loss (MSE): 0.0375\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 055 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 056 - training loss (MSE): 0.0362\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 057 - training loss (MSE): 0.0357\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 058 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 059 - training loss (MSE): 0.0352\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 060 - training loss (MSE): 0.0360\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 061 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 062 - training loss (MSE): 0.0343\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 063 - training loss (MSE): 0.0354\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 064 - training loss (MSE): 0.0351\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 065 - training loss (MSE): 0.0347\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 066 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 067 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 068 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:43:56 [INFO]: Epoch 069 - training loss (MSE): 0.0328\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 070 - training loss (MSE): 0.0345\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 071 - training loss (MSE): 0.0340\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 072 - training loss (MSE): 0.0349\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 073 - training loss (MSE): 0.0313\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 074 - training loss (MSE): 0.0329\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 075 - training loss (MSE): 0.0346\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 076 - training loss (MSE): 0.0316\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 077 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 078 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 079 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 080 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 081 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 082 - training loss (MSE): 0.0331\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 083 - training loss (MSE): 0.0324\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 084 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 085 - training loss (MSE): 0.0346\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 086 - training loss (MSE): 0.0313\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 087 - training loss (MSE): 0.0323\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 088 - training loss (MSE): 0.0321\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 089 - training loss (MSE): 0.0318\n",
      "2025-05-23 15:43:57 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:43:57 [INFO]: Finished training. The best model is from epoch#79.\n",
      "[I 2025-05-23 15:43:57,727] Trial 14 finished with value: 0.27747036842386014 and parameters: {'n_layers': 1, 'd_model': 32, 'd_ffn': 128, 'n_heads': 3, 'top_k': 1, 'n_kernels': 3, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.002365300835186382, 'weight_decay': 0.0005401266377398488}. Best is trial 13 with value: 0.23603596570103588.\n",
      "2025-05-23 15:43:57 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:43:57 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:43:57 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:43:57 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,377,793\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 001 - training loss (MSE): 0.6846\n",
      "2025-05-23 15:43:57 [INFO]: Epoch 002 - training loss (MSE): 0.3880\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 003 - training loss (MSE): 0.2990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:43:58 [INFO]: Epoch 004 - training loss (MSE): 0.2497\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 005 - training loss (MSE): 0.2141\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 006 - training loss (MSE): 0.1874\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 007 - training loss (MSE): 0.1794\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 008 - training loss (MSE): 0.1536\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 009 - training loss (MSE): 0.1418\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 010 - training loss (MSE): 0.1325\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 011 - training loss (MSE): 0.1168\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 012 - training loss (MSE): 0.1030\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 013 - training loss (MSE): 0.0986\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 014 - training loss (MSE): 0.0960\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 015 - training loss (MSE): 0.0884\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 016 - training loss (MSE): 0.0900\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 017 - training loss (MSE): 0.0837\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 018 - training loss (MSE): 0.0768\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 019 - training loss (MSE): 0.0734\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 020 - training loss (MSE): 0.0750\n",
      "2025-05-23 15:43:58 [INFO]: Epoch 021 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 022 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 023 - training loss (MSE): 0.0686\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 024 - training loss (MSE): 0.0645\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 025 - training loss (MSE): 0.0653\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 026 - training loss (MSE): 0.0655\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 027 - training loss (MSE): 0.0627\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 028 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 029 - training loss (MSE): 0.0601\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 030 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 031 - training loss (MSE): 0.0562\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 032 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 033 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 034 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 035 - training loss (MSE): 0.0502\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 036 - training loss (MSE): 0.0524\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 037 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 038 - training loss (MSE): 0.0499\n",
      "2025-05-23 15:43:59 [INFO]: Epoch 039 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 040 - training loss (MSE): 0.0508\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 041 - training loss (MSE): 0.0477\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 042 - training loss (MSE): 0.0488\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 043 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 044 - training loss (MSE): 0.0451\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 045 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 046 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 047 - training loss (MSE): 0.0434\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 048 - training loss (MSE): 0.0402\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 049 - training loss (MSE): 0.0420\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 050 - training loss (MSE): 0.0435\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 051 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 052 - training loss (MSE): 0.0419\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 053 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 054 - training loss (MSE): 0.0415\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 055 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 056 - training loss (MSE): 0.0394\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 057 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:44:00 [INFO]: Epoch 058 - training loss (MSE): 0.0375\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 059 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 060 - training loss (MSE): 0.0372\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 061 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 062 - training loss (MSE): 0.0367\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 063 - training loss (MSE): 0.0361\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 064 - training loss (MSE): 0.0369\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 065 - training loss (MSE): 0.0374\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 066 - training loss (MSE): 0.0356\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 067 - training loss (MSE): 0.0360\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 068 - training loss (MSE): 0.0351\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 069 - training loss (MSE): 0.0361\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 070 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 071 - training loss (MSE): 0.0348\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 072 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 073 - training loss (MSE): 0.0332\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 074 - training loss (MSE): 0.0343\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 075 - training loss (MSE): 0.0352\n",
      "2025-05-23 15:44:01 [INFO]: Epoch 076 - training loss (MSE): 0.0353\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 077 - training loss (MSE): 0.0320\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 078 - training loss (MSE): 0.0338\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 079 - training loss (MSE): 0.0336\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 080 - training loss (MSE): 0.0338\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 081 - training loss (MSE): 0.0342\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 082 - training loss (MSE): 0.0335\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 083 - training loss (MSE): 0.0317\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 084 - training loss (MSE): 0.0325\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 085 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 086 - training loss (MSE): 0.0331\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 087 - training loss (MSE): 0.0336\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 088 - training loss (MSE): 0.0307\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 089 - training loss (MSE): 0.0318\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 090 - training loss (MSE): 0.0315\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 091 - training loss (MSE): 0.0304\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 092 - training loss (MSE): 0.0333\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 093 - training loss (MSE): 0.0297\n",
      "2025-05-23 15:44:02 [INFO]: Epoch 094 - training loss (MSE): 0.0327\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 095 - training loss (MSE): 0.0321\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 096 - training loss (MSE): 0.0312\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 097 - training loss (MSE): 0.0318\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 098 - training loss (MSE): 0.0320\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 099 - training loss (MSE): 0.0325\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 100 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:44:03 [INFO]: Finished training. The best model is from epoch#93.\n",
      "[I 2025-05-23 15:44:03,432] Trial 15 finished with value: 0.24043875341970783 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 4, 'dropout': 0.2, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.4, 'apply_nonstationary_norm': False, 'num_workers': 0, 'patience': 10, 'lr': 0.0013185610521158162, 'weight_decay': 0.0004898357806420887}. Best is trial 13 with value: 0.23603596570103588.\n",
      "2025-05-23 15:44:03 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:44:03 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:44:03 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:44:03 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,897\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 001 - training loss (MSE): 0.6073\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 002 - training loss (MSE): 0.2817\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 003 - training loss (MSE): 0.1957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:44:03 [INFO]: Epoch 004 - training loss (MSE): 0.1483\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 005 - training loss (MSE): 0.1280\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 006 - training loss (MSE): 0.1032\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 007 - training loss (MSE): 0.0918\n",
      "2025-05-23 15:44:03 [INFO]: Epoch 008 - training loss (MSE): 0.0843\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 009 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 010 - training loss (MSE): 0.0728\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 011 - training loss (MSE): 0.0655\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 012 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 013 - training loss (MSE): 0.0590\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 014 - training loss (MSE): 0.0518\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 015 - training loss (MSE): 0.0497\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 016 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 017 - training loss (MSE): 0.0439\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 018 - training loss (MSE): 0.0423\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 019 - training loss (MSE): 0.0381\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 020 - training loss (MSE): 0.0357\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 021 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 022 - training loss (MSE): 0.0306\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 023 - training loss (MSE): 0.0326\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 024 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 025 - training loss (MSE): 0.0266\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 026 - training loss (MSE): 0.0265\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 027 - training loss (MSE): 0.0269\n",
      "2025-05-23 15:44:04 [INFO]: Epoch 028 - training loss (MSE): 0.0260\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 029 - training loss (MSE): 0.0254\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 030 - training loss (MSE): 0.0241\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 031 - training loss (MSE): 0.0247\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 032 - training loss (MSE): 0.0220\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 033 - training loss (MSE): 0.0249\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 034 - training loss (MSE): 0.0231\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 035 - training loss (MSE): 0.0250\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 036 - training loss (MSE): 0.0234\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 037 - training loss (MSE): 0.0232\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 038 - training loss (MSE): 0.0230\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 039 - training loss (MSE): 0.0240\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 040 - training loss (MSE): 0.0241\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 041 - training loss (MSE): 0.0232\n",
      "2025-05-23 15:44:05 [INFO]: Epoch 042 - training loss (MSE): 0.0231\n",
      "2025-05-23 15:44:05 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:44:05 [INFO]: Finished training. The best model is from epoch#32.\n",
      "[I 2025-05-23 15:44:05,865] Trial 16 finished with value: 0.22284375653688068 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 5, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.003618468418728484, 'weight_decay': 0.00015422186712091382}. Best is trial 16 with value: 0.22284375653688068.\n",
      "2025-05-23 15:44:05 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:44:05 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:44:05 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:44:05 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 676,689\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 001 - training loss (MSE): 0.7073\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 002 - training loss (MSE): 0.5352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:44:06 [INFO]: Epoch 003 - training loss (MSE): 0.4075\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 004 - training loss (MSE): 0.3076\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 005 - training loss (MSE): 0.2270\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 006 - training loss (MSE): 0.2023\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 007 - training loss (MSE): 0.1947\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 008 - training loss (MSE): 0.1939\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 009 - training loss (MSE): 0.1961\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 010 - training loss (MSE): 0.1817\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 011 - training loss (MSE): 0.1833\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 012 - training loss (MSE): 0.1687\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 013 - training loss (MSE): 0.1667\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 014 - training loss (MSE): 0.1582\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 015 - training loss (MSE): 0.1530\n",
      "2025-05-23 15:44:06 [INFO]: Epoch 016 - training loss (MSE): 0.1476\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 017 - training loss (MSE): 0.1466\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 018 - training loss (MSE): 0.1402\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 019 - training loss (MSE): 0.1323\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 020 - training loss (MSE): 0.1299\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 021 - training loss (MSE): 0.1239\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 022 - training loss (MSE): 0.1197\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 023 - training loss (MSE): 0.1257\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 024 - training loss (MSE): 0.1145\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 025 - training loss (MSE): 0.1187\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 026 - training loss (MSE): 0.1117\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 027 - training loss (MSE): 0.1129\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 028 - training loss (MSE): 0.1043\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 029 - training loss (MSE): 0.1079\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 030 - training loss (MSE): 0.1026\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 031 - training loss (MSE): 0.1077\n",
      "2025-05-23 15:44:07 [INFO]: Epoch 032 - training loss (MSE): 0.0970\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 033 - training loss (MSE): 0.0963\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 034 - training loss (MSE): 0.0973\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 035 - training loss (MSE): 0.0971\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 036 - training loss (MSE): 0.0930\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 037 - training loss (MSE): 0.0881\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 038 - training loss (MSE): 0.0831\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 039 - training loss (MSE): 0.0877\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 040 - training loss (MSE): 0.0879\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 041 - training loss (MSE): 0.0840\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 042 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 043 - training loss (MSE): 0.0746\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 044 - training loss (MSE): 0.0802\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 045 - training loss (MSE): 0.0780\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 046 - training loss (MSE): 0.0725\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 047 - training loss (MSE): 0.0717\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 048 - training loss (MSE): 0.0683\n",
      "2025-05-23 15:44:08 [INFO]: Epoch 049 - training loss (MSE): 0.0658\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 050 - training loss (MSE): 0.0710\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 051 - training loss (MSE): 0.0660\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 052 - training loss (MSE): 0.0708\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 053 - training loss (MSE): 0.0680\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 054 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 055 - training loss (MSE): 0.0676\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 056 - training loss (MSE): 0.0646\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 057 - training loss (MSE): 0.0622\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 058 - training loss (MSE): 0.0635\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 059 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 060 - training loss (MSE): 0.0614\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 061 - training loss (MSE): 0.0589\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 062 - training loss (MSE): 0.0584\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 063 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 064 - training loss (MSE): 0.0591\n",
      "2025-05-23 15:44:09 [INFO]: Epoch 065 - training loss (MSE): 0.0571\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 066 - training loss (MSE): 0.0575\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 067 - training loss (MSE): 0.0609\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 068 - training loss (MSE): 0.0596\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 069 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 070 - training loss (MSE): 0.0574\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 071 - training loss (MSE): 0.0554\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 072 - training loss (MSE): 0.0572\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 073 - training loss (MSE): 0.0557\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 074 - training loss (MSE): 0.0516\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 075 - training loss (MSE): 0.0517\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 076 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 077 - training loss (MSE): 0.0560\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 078 - training loss (MSE): 0.0509\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 079 - training loss (MSE): 0.0499\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 080 - training loss (MSE): 0.0524\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 081 - training loss (MSE): 0.0509\n",
      "2025-05-23 15:44:10 [INFO]: Epoch 082 - training loss (MSE): 0.0501\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 083 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 084 - training loss (MSE): 0.0483\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 085 - training loss (MSE): 0.0516\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 086 - training loss (MSE): 0.0493\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 087 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 088 - training loss (MSE): 0.0497\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 089 - training loss (MSE): 0.0496\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 090 - training loss (MSE): 0.0503\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 091 - training loss (MSE): 0.0464\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 092 - training loss (MSE): 0.0461\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 093 - training loss (MSE): 0.0469\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 094 - training loss (MSE): 0.0444\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 095 - training loss (MSE): 0.0426\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 096 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 097 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 098 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:44:11 [INFO]: Epoch 099 - training loss (MSE): 0.0421\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 100 - training loss (MSE): 0.0456\n",
      "2025-05-23 15:44:12 [INFO]: Finished training. The best model is from epoch#97.\n",
      "[I 2025-05-23 15:44:12,190] Trial 17 finished with value: 0.27074294790524916 and parameters: {'n_layers': 1, 'd_model': 16, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 5, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.00030073823758577705, 'weight_decay': 0.0006527843667838277}. Best is trial 16 with value: 0.22284375653688068.\n",
      "2025-05-23 15:44:12 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:44:12 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:44:12 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:44:12 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 338,665\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 001 - training loss (MSE): 0.4981\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 002 - training loss (MSE): 0.2581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:44:12 [INFO]: Epoch 003 - training loss (MSE): 0.2283\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 004 - training loss (MSE): 0.1629\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 005 - training loss (MSE): 0.1165\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 006 - training loss (MSE): 0.1050\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 007 - training loss (MSE): 0.0807\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 008 - training loss (MSE): 0.0691\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 009 - training loss (MSE): 0.0673\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 010 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:44:12 [INFO]: Epoch 011 - training loss (MSE): 0.0622\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 012 - training loss (MSE): 0.0610\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 013 - training loss (MSE): 0.0523\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 014 - training loss (MSE): 0.0534\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 015 - training loss (MSE): 0.0499\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 016 - training loss (MSE): 0.0470\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 017 - training loss (MSE): 0.0495\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 018 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 019 - training loss (MSE): 0.0467\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 020 - training loss (MSE): 0.0459\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 021 - training loss (MSE): 0.0454\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 022 - training loss (MSE): 0.0446\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 023 - training loss (MSE): 0.0428\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 024 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 025 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 026 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 027 - training loss (MSE): 0.0447\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 028 - training loss (MSE): 0.0386\n",
      "2025-05-23 15:44:13 [INFO]: Epoch 029 - training loss (MSE): 0.0385\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 030 - training loss (MSE): 0.0364\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 031 - training loss (MSE): 0.0388\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 032 - training loss (MSE): 0.0396\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 033 - training loss (MSE): 0.0380\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 034 - training loss (MSE): 0.0367\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 035 - training loss (MSE): 0.0337\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 036 - training loss (MSE): 0.0339\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 037 - training loss (MSE): 0.0329\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 038 - training loss (MSE): 0.0304\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 039 - training loss (MSE): 0.0325\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 040 - training loss (MSE): 0.0310\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 041 - training loss (MSE): 0.0285\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 042 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 043 - training loss (MSE): 0.0310\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 044 - training loss (MSE): 0.0310\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 045 - training loss (MSE): 0.0300\n",
      "2025-05-23 15:44:14 [INFO]: Epoch 046 - training loss (MSE): 0.0296\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 047 - training loss (MSE): 0.0282\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 048 - training loss (MSE): 0.0311\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 049 - training loss (MSE): 0.0286\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 050 - training loss (MSE): 0.0319\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 051 - training loss (MSE): 0.0297\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 052 - training loss (MSE): 0.0334\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 053 - training loss (MSE): 0.0288\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 054 - training loss (MSE): 0.0289\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 055 - training loss (MSE): 0.0299\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 056 - training loss (MSE): 0.0302\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 057 - training loss (MSE): 0.0291\n",
      "2025-05-23 15:44:15 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:44:15 [INFO]: Finished training. The best model is from epoch#47.\n",
      "[I 2025-05-23 15:44:15,719] Trial 18 finished with value: 0.2379703274602086 and parameters: {'n_layers': 1, 'd_model': 8, 'd_ffn': 128, 'n_heads': 2, 'top_k': 1, 'n_kernels': 5, 'dropout': 0, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0.3, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.008215759484316036, 'weight_decay': 0.00016067793405038876}. Best is trial 16 with value: 0.22284375653688068.\n",
      "2025-05-23 15:44:15 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:44:15 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:44:15 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:44:15 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,897\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 001 - training loss (MSE): 0.7743\n",
      "2025-05-23 15:44:15 [INFO]: Epoch 002 - training loss (MSE): 0.4275\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 003 - training loss (MSE): 0.3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:44:16 [INFO]: Epoch 004 - training loss (MSE): 0.2833\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 005 - training loss (MSE): 0.2467\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 006 - training loss (MSE): 0.1983\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 007 - training loss (MSE): 0.1977\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 008 - training loss (MSE): 0.1720\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 009 - training loss (MSE): 0.1497\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 010 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 011 - training loss (MSE): 0.1359\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 012 - training loss (MSE): 0.1220\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 013 - training loss (MSE): 0.1156\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 014 - training loss (MSE): 0.1024\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 015 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 016 - training loss (MSE): 0.0956\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 017 - training loss (MSE): 0.0939\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 018 - training loss (MSE): 0.0910\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 019 - training loss (MSE): 0.0829\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 020 - training loss (MSE): 0.0809\n",
      "2025-05-23 15:44:16 [INFO]: Epoch 021 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 022 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 023 - training loss (MSE): 0.0773\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 024 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 025 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 026 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 027 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 028 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 029 - training loss (MSE): 0.0689\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 030 - training loss (MSE): 0.0633\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 031 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 032 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 033 - training loss (MSE): 0.0656\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 034 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 035 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 036 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 037 - training loss (MSE): 0.0598\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 038 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:44:17 [INFO]: Epoch 039 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 040 - training loss (MSE): 0.0627\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 041 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 042 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 043 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 044 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 045 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 046 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 047 - training loss (MSE): 0.0538\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 048 - training loss (MSE): 0.0514\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 049 - training loss (MSE): 0.0504\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 050 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 051 - training loss (MSE): 0.0520\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 052 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 053 - training loss (MSE): 0.0479\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 054 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 055 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 056 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 057 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:44:18 [INFO]: Epoch 058 - training loss (MSE): 0.0457\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 059 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 060 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 061 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 062 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 063 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 064 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 065 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 066 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 067 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 068 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 069 - training loss (MSE): 0.0429\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 070 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 071 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 072 - training loss (MSE): 0.0408\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 073 - training loss (MSE): 0.0419\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 074 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 075 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:44:19 [INFO]: Epoch 076 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 077 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 078 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 079 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 080 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 081 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 082 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 083 - training loss (MSE): 0.0401\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 084 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 085 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 086 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 087 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 088 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 089 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 090 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 091 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 092 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 093 - training loss (MSE): 0.0399\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 094 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:44:20 [INFO]: Epoch 095 - training loss (MSE): 0.0416\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 096 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:44:21 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:44:21 [INFO]: Finished training. The best model is from epoch#86.\n",
      "[I 2025-05-23 15:44:21,178] Trial 19 finished with value: 0.17498291877563021 and parameters: {'n_layers': 2, 'd_model': 32, 'd_ffn': 64, 'n_heads': 3, 'top_k': 1, 'n_kernels': 5, 'dropout': 0.5, 'epochs': 100, 'batch_size': 32, 'attn_dropout': 0, 'apply_nonstationary_norm': True, 'num_workers': 0, 'patience': 10, 'lr': 0.0023829279010544025, 'weight_decay': 0.0003987175700559951}. Best is trial 19 with value: 0.17498291877563021.\n"
     ]
    }
   ],
   "source": [
    "best_params, logs = tsi.perform_hyperparameter_tuning(missing_df=df_missing,\n",
    "                                                      groundTruth_df=df_scaled,\n",
    "                                                      algorithm=algorithm,\n",
    "                                                      params=params, \n",
    "                                                      n_trials=n_trials,\n",
    "                                                      time_column=time_column, \n",
    "                                                      is_multivariate=is_multivariate, \n",
    "                                                      areaVStime=areaVStime,\n",
    "                                                      preprocessing=preprocessing, \n",
    "                                                      index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:21.180194Z",
     "start_time": "2025-05-23T12:41:56.304325Z"
    }
   },
   "id": "514f32058521b4ba",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'TimesNet': {'n_layers': 2,\n  'd_model': 32,\n  'd_ffn': 64,\n  'n_heads': 3,\n  'top_k': 1,\n  'n_kernels': 5,\n  'dropout': 0.5,\n  'epochs': 100,\n  'batch_size': 32,\n  'attn_dropout': 0,\n  'apply_nonstationary_norm': True,\n  'num_workers': 0,\n  'patience': 10,\n  'lr': 0.0023829279010544025,\n  'weight_decay': 0.0003987175700559951,\n  'device': ['cuda']}}"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:21.186234Z",
     "start_time": "2025-05-23T12:44:21.181785Z"
    }
   },
   "id": "ff7eeb0fccada1d5",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'TimesNet': {'trial_number_0': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 128,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 3,\n     'n_kernels': 4,\n     'dropout': 0.4,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.1,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.005431095176981607,\n     'weight_decay': 0.0001106913243495668,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.36700547755144114,\n    'Mean square error': 0.25377336635539527,\n    'Root mean square error': 0.5037592345112845,\n    'Mean relative error': 0.4659437622217796,\n    'Euclidean Distance': 20.375800042143794,\n    'r2 score': 0.725798077587763,\n    'Execution Time': 14.068978786468506}},\n  'trial_number_1': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 128,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0008505207524153307,\n     'weight_decay': 0.0003414234544289426,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.35781928249541933,\n    'Mean square error': 0.2375144542561454,\n    'Root mean square error': 0.4873545467687209,\n    'Mean relative error': 0.45428112897319006,\n    'Euclidean Distance': 19.712271486641363,\n    'r2 score': 0.7433658193014556,\n    'Execution Time': 10.508395671844482}},\n  'trial_number_2': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 16,\n     'd_ffn': 128,\n     'n_heads': 1,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.003964743616689867,\n     'weight_decay': 0.0004659747070765304,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.36447654200364904,\n    'Mean square error': 0.24376912720953683,\n    'Root mean square error': 0.49372981195137167,\n    'Mean relative error': 0.46273306969638134,\n    'Euclidean Distance': 19.970135004921783,\n    'r2 score': 0.7366076501030461,\n    'Execution Time': 14.026697158813477}},\n  'trial_number_3': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 64,\n     'd_ffn': 64,\n     'n_heads': 1,\n     'top_k': 3,\n     'n_kernels': 4,\n     'dropout': 0.4,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.00014632223035577693,\n     'weight_decay': 0.000331603591492567,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.4180259958631467,\n    'Mean square error': 0.3153889857663928,\n    'Root mean square error': 0.5615950371632505,\n    'Mean relative error': 0.5307185236538599,\n    'Euclidean Distance': 22.71511348670349,\n    'r2 score': 0.6592224493579055,\n    'Execution Time': 13.39942216873169}},\n  'trial_number_4': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 1,\n     'top_k': 3,\n     'n_kernels': 3,\n     'dropout': 0.5,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0017597216454587137,\n     'weight_decay': 0.0009307823237220275,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.35287309937787575,\n    'Mean square error': 0.25178786891492727,\n    'Root mean square error': 0.5017846838185949,\n    'Mean relative error': 0.44800154103406165,\n    'Euclidean Distance': 20.295934409255988,\n    'r2 score': 0.7279434060079191,\n    'Execution Time': 8.196366548538208}},\n  'trial_number_5': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 32,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0010035341501560358,\n     'weight_decay': 0.000815755185102926,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3774826871816555,\n    'Mean square error': 0.258135312566468,\n    'Root mean square error': 0.5080701846856082,\n    'Mean relative error': 0.4792454450883632,\n    'Euclidean Distance': 20.550167185664012,\n    'r2 score': 0.7210849981432483,\n    'Execution Time': 3.431993246078491}},\n  'trial_number_6': {'params': {'TimesNet': {'n_layers': 3,\n     'd_model': 64,\n     'd_ffn': 32,\n     'n_heads': 3,\n     'top_k': 2,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 7.411200761456233e-05,\n     'weight_decay': 0.0009705778096597715,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.438623888474074,\n    'Mean square error': 0.33889592841585414,\n    'Root mean square error': 0.5821476860864897,\n    'Mean relative error': 0.5568692493623896,\n    'Euclidean Distance': 23.546416688921855,\n    'r2 score': 0.6338232163450537,\n    'Execution Time': 9.49230694770813}},\n  'trial_number_7': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0038565644906092124,\n     'weight_decay': 0.00023577029727229654,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3481014670492553,\n    'Mean square error': 0.23652038096985437,\n    'Root mean square error': 0.48633361077541654,\n    'Mean relative error': 0.441943559736426,\n    'Euclidean Distance': 19.670977181286194,\n    'r2 score': 0.7444399147041147,\n    'Execution Time': 3.1888561248779297}},\n  'trial_number_8': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 64,\n     'd_ffn': 32,\n     'n_heads': 1,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.1,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0002128524987103289,\n     'weight_decay': 0.0007940117135494467,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.37622365156082915,\n    'Mean square error': 0.25138395308951633,\n    'Root mean square error': 0.5013820430465339,\n    'Mean relative error': 0.4776469953925904,\n    'Euclidean Distance': 20.27964859790349,\n    'r2 score': 0.7283798367390514,\n    'Execution Time': 5.893574476242065}},\n  'trial_number_9': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 64,\n     'd_ffn': 8,\n     'n_heads': 3,\n     'top_k': 3,\n     'n_kernels': 3,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.007691181798461813,\n     'weight_decay': 0.0006470205285361298,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.40549022445200383,\n    'Mean square error': 0.3094239771068529,\n    'Root mean square error': 0.5562589119347688,\n    'Mean relative error': 0.514803326603861,\n    'Euclidean Distance': 22.499280578427648,\n    'r2 score': 0.6656676365150198,\n    'Execution Time': 2.080707311630249}},\n  'trial_number_10': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 8,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.1,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0022922125037629845,\n     'weight_decay': 6.245289685592381e-05,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3847413500366089,\n    'Mean square error': 0.2714808487888748,\n    'Root mean square error': 0.5210382411962435,\n    'Mean relative error': 0.48846091702600597,\n    'Euclidean Distance': 21.074692610299195,\n    'r2 score': 0.706665156769188,\n    'Execution Time': 3.6986093521118164}},\n  'trial_number_11': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 128,\n     'd_ffn': 16,\n     'n_heads': 2,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.2,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0005597413939486375,\n     'weight_decay': 0.0002858718430285778,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3626772420498353,\n    'Mean square error': 0.24093841386033335,\n    'Root mean square error': 0.4908547787893415,\n    'Mean relative error': 0.46044870981314784,\n    'Euclidean Distance': 19.853847110207774,\n    'r2 score': 0.7396662336466895,\n    'Execution Time': 10.714935064315796}},\n  'trial_number_12': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 128,\n     'd_ffn': 8,\n     'n_heads': 3,\n     'top_k': 2,\n     'n_kernels': 5,\n     'dropout': 0.3,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.5,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0004127289522954379,\n     'weight_decay': 0.00026744327270321225,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3920563241500038,\n    'Mean square error': 0.2736473437231825,\n    'Root mean square error': 0.5231131270797766,\n    'Mean relative error': 0.49774788075660154,\n    'Euclidean Distance': 21.158616550500813,\n    'r2 score': 0.7043242606995366,\n    'Execution Time': 12.171207666397095}},\n  'trial_number_13': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 32,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0012427183476582465,\n     'weight_decay': 0.0005072807145484922,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.35662590502456054,\n    'Mean square error': 0.23603596570103588,\n    'Root mean square error': 0.4858353277614092,\n    'Mean relative error': 0.4527660377210582,\n    'Euclidean Distance': 19.650822880655532,\n    'r2 score': 0.7449633250204277,\n    'Execution Time': 7.227560997009277}},\n  'trial_number_14': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 32,\n     'd_ffn': 128,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 3,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.002365300835186382,\n     'weight_decay': 0.0005401266377398488,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.37004609912111325,\n    'Mean square error': 0.27747036842386014,\n    'Root mean square error': 0.5267545618443756,\n    'Mean relative error': 0.4698040824086006,\n    'Euclidean Distance': 21.305903471607007,\n    'r2 score': 0.7001934855224157,\n    'Execution Time': 3.2344934940338135}},\n  'trial_number_15': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 4,\n     'dropout': 0.2,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.4,\n     'apply_nonstationary_norm': False,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0013185610521158162,\n     'weight_decay': 0.0004898357806420887,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3555614913014206,\n    'Mean square error': 0.24043875341970783,\n    'Root mean square error': 0.4903455449167534,\n    'Mean relative error': 0.4514146765968887,\n    'Euclidean Distance': 19.83324987475936,\n    'r2 score': 0.7402061163591289,\n    'Execution Time': 5.6966392993927}},\n  'trial_number_16': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.003618468418728484,\n     'weight_decay': 0.00015422186712091382,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3432717048840569,\n    'Mean square error': 0.22284375653688068,\n    'Root mean square error': 0.47206329717197953,\n    'Mean relative error': 0.4358117778107093,\n    'Euclidean Distance': 19.093778717015052,\n    'r2 score': 0.7592174966288456,\n    'Execution Time': 2.4237430095672607}},\n  'trial_number_17': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 16,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.00030073823758577705,\n     'weight_decay': 0.0006527843667838277,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.3857037526675316,\n    'Mean square error': 0.27074294790524916,\n    'Root mean square error': 0.5203296531096889,\n    'Mean relative error': 0.4896827666442079,\n    'Euclidean Distance': 21.046031995912863,\n    'r2 score': 0.7074624580925921,\n    'Execution Time': 6.316457509994507}},\n  'trial_number_18': {'params': {'TimesNet': {'n_layers': 1,\n     'd_model': 8,\n     'd_ffn': 128,\n     'n_heads': 2,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0.3,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.008215759484316036,\n     'weight_decay': 0.00016067793405038876,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.35351846596752096,\n    'Mean square error': 0.2379703274602086,\n    'Root mean square error': 0.48782202436975786,\n    'Mean relative error': 0.4488208872160253,\n    'Euclidean Distance': 19.73117978542848,\n    'r2 score': 0.742873248663624,\n    'Execution Time': 3.519547700881958}},\n  'trial_number_19': {'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.5,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0023829279010544025,\n     'weight_decay': 0.0003987175700559951,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.30306023132060383,\n    'Mean square error': 0.17498291877563021,\n    'Root mean square error': 0.4183095968007789,\n    'Mean relative error': 0.38475999133155303,\n    'Euclidean Distance': 16.91957609152579,\n    'r2 score': 0.8109310941228248,\n    'Execution Time': 5.449551582336426}},\n  'best_trial': {'number': 19,\n   'params': {'TimesNet': {'n_layers': 2,\n     'd_model': 32,\n     'd_ffn': 64,\n     'n_heads': 3,\n     'top_k': 1,\n     'n_kernels': 5,\n     'dropout': 0.5,\n     'epochs': 100,\n     'batch_size': 32,\n     'attn_dropout': 0,\n     'apply_nonstationary_norm': True,\n     'num_workers': 0,\n     'patience': 10,\n     'lr': 0.0023829279010544025,\n     'weight_decay': 0.0003987175700559951,\n     'device': ['cuda']}},\n   'metrics': {'Missing value percentage': 3.2709532949456173,\n    'Mean absolute error': 0.30306023132060383,\n    'Mean square error': 0.17498291877563021,\n    'Root mean square error': 0.4183095968007789,\n    'Mean relative error': 0.38475999133155303,\n    'Euclidean Distance': 16.91957609152579,\n    'r2 score': 0.8109310941228248,\n    'Execution Time': 5.449551582336426}}}}"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:21.207380Z",
     "start_time": "2025-05-23T12:44:21.187671Z"
    }
   },
   "id": "c674552e92b97760",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imputation - with tuned parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df0d9a533d7501dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"TimesNet\"],\n",
    "    \"params\": best_params\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:21.217094Z",
     "start_time": "2025-05-23T12:44:21.208429Z"
    }
   },
   "id": "42ddb27d5b693f9b",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:44:21 [INFO]: Have set the random seed as 2025 for numpy and pytorch.\n",
      "2025-05-23 15:44:21 [INFO]: Using the given device: cuda\n",
      "2025-05-23 15:44:21 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-23 15:44:21 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 1,352,897\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 001 - training loss (MSE): 0.7743\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 002 - training loss (MSE): 0.4275\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 003 - training loss (MSE): 0.3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing TimesNet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 15:44:21 [INFO]: Epoch 004 - training loss (MSE): 0.2833\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 005 - training loss (MSE): 0.2467\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 006 - training loss (MSE): 0.1983\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 007 - training loss (MSE): 0.1977\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 008 - training loss (MSE): 0.1720\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 009 - training loss (MSE): 0.1497\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 010 - training loss (MSE): 0.1443\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 011 - training loss (MSE): 0.1359\n",
      "2025-05-23 15:44:21 [INFO]: Epoch 012 - training loss (MSE): 0.1220\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 013 - training loss (MSE): 0.1156\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 014 - training loss (MSE): 0.1024\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 015 - training loss (MSE): 0.0995\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 016 - training loss (MSE): 0.0956\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 017 - training loss (MSE): 0.0939\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 018 - training loss (MSE): 0.0910\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 019 - training loss (MSE): 0.0829\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 020 - training loss (MSE): 0.0809\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 021 - training loss (MSE): 0.0814\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 022 - training loss (MSE): 0.0760\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 023 - training loss (MSE): 0.0773\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 024 - training loss (MSE): 0.0733\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 025 - training loss (MSE): 0.0723\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 026 - training loss (MSE): 0.0690\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 027 - training loss (MSE): 0.0727\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 028 - training loss (MSE): 0.0684\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 029 - training loss (MSE): 0.0689\n",
      "2025-05-23 15:44:22 [INFO]: Epoch 030 - training loss (MSE): 0.0633\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 031 - training loss (MSE): 0.0648\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 032 - training loss (MSE): 0.0616\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 033 - training loss (MSE): 0.0656\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 034 - training loss (MSE): 0.0619\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 035 - training loss (MSE): 0.0637\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 036 - training loss (MSE): 0.0604\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 037 - training loss (MSE): 0.0598\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 038 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 039 - training loss (MSE): 0.0595\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 040 - training loss (MSE): 0.0627\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 041 - training loss (MSE): 0.0563\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 042 - training loss (MSE): 0.0552\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 043 - training loss (MSE): 0.0592\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 044 - training loss (MSE): 0.0551\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 045 - training loss (MSE): 0.0521\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 046 - training loss (MSE): 0.0510\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 047 - training loss (MSE): 0.0538\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 048 - training loss (MSE): 0.0514\n",
      "2025-05-23 15:44:23 [INFO]: Epoch 049 - training loss (MSE): 0.0504\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 050 - training loss (MSE): 0.0507\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 051 - training loss (MSE): 0.0520\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 052 - training loss (MSE): 0.0512\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 053 - training loss (MSE): 0.0479\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 054 - training loss (MSE): 0.0482\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 055 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 056 - training loss (MSE): 0.0475\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 057 - training loss (MSE): 0.0466\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 058 - training loss (MSE): 0.0457\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 059 - training loss (MSE): 0.0473\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 060 - training loss (MSE): 0.0443\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 061 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 062 - training loss (MSE): 0.0472\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 063 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 064 - training loss (MSE): 0.0471\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 065 - training loss (MSE): 0.0452\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 066 - training loss (MSE): 0.0438\n",
      "2025-05-23 15:44:24 [INFO]: Epoch 067 - training loss (MSE): 0.0440\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 068 - training loss (MSE): 0.0442\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 069 - training loss (MSE): 0.0429\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 070 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 071 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 072 - training loss (MSE): 0.0408\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 073 - training loss (MSE): 0.0419\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 074 - training loss (MSE): 0.0441\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 075 - training loss (MSE): 0.0417\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 076 - training loss (MSE): 0.0425\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 077 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 078 - training loss (MSE): 0.0410\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 079 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 080 - training loss (MSE): 0.0431\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 081 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 082 - training loss (MSE): 0.0411\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 083 - training loss (MSE): 0.0401\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 084 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 085 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:44:25 [INFO]: Epoch 086 - training loss (MSE): 0.0383\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 087 - training loss (MSE): 0.0413\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 088 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 089 - training loss (MSE): 0.0398\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 090 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 091 - training loss (MSE): 0.0389\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 092 - training loss (MSE): 0.0406\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 093 - training loss (MSE): 0.0400\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 094 - training loss (MSE): 0.0397\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 095 - training loss (MSE): 0.0416\n",
      "2025-05-23 15:44:26 [INFO]: Epoch 096 - training loss (MSE): 0.0392\n",
      "2025-05-23 15:44:26 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-05-23 15:44:26 [INFO]: Finished training. The best model is from epoch#86.\n"
     ]
    }
   ],
   "source": [
    "dict_of_imputed_dfs = tsi.run_imputation(missing = df_missing, \n",
    "                                         algorithms=algorithms, \n",
    "                                         params=params, \n",
    "                                         time_column=time_column,\n",
    "                                         header=header, \n",
    "                                         sep=sep, \n",
    "                                         is_multivariate=is_multivariate, \n",
    "                                         areaVStime=areaVStime, \n",
    "                                         preprocessing=preprocessing, \n",
    "                                         index=index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:26.684316Z",
     "start_time": "2025-05-23T12:44:21.218614Z"
    }
   },
   "id": "89faf7d0a9056753",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "           time  3i Group PLC_035999.txt  Admiral Group_036346.txt  \\\n0    2017-01-02                -2.152305                 -1.347635   \n1    2017-01-03                -2.053810                 -1.189826   \n2    2017-01-04                -1.992251                 -1.325091   \n3    2017-01-05                -1.924536                 -1.336363   \n4    2017-01-06                -1.912224                 -1.313819   \n..          ...                      ...                       ...   \n516  2018-12-25                -1.509627                  0.405175   \n517  2018-12-26                -1.509627                  0.405175   \n518  2018-12-27                -1.310175                  0.433355   \n519  2018-12-28                -1.027002                  0.861694   \n520  2018-12-31                -1.297863                  1.132225   \n\n     Anglo American PLC_035918.txt  Antofagasta PLC_028149.txt  \\\n0                        -1.294240                   -2.295047   \n1                        -1.286110                   -2.136391   \n2                        -1.365373                   -2.120526   \n3                        -1.355211                   -2.035909   \n4                        -1.395858                   -2.094083   \n..                             ...                         ...   \n516                       1.060066                   -1.271187   \n517                       1.060066                   -1.271187   \n518                       0.958448                   -1.406574   \n519                       1.121038                   -1.173878   \n520                       1.095023                   -1.150609   \n\n     Ashtead Group_028090.txt  Associated British Foods PLC_035919.txt  \\\n0                   -1.199292                                 0.108603   \n1                   -1.136860                                -0.002687   \n2                   -1.155222                                -0.333285   \n3                   -1.122170                                -0.087792   \n4                   -1.155222                                -0.185989   \n..                        ...                                      ...   \n516                 -0.967925                                -2.068105   \n517                 -0.967925                                -2.068105   \n518                 -1.175421                                -2.195762   \n519                 -0.978943                                -2.100838   \n520                 -0.989960                                -2.189215   \n\n     Astrazeneca PLC_035998.txt  Aviva PLC_035907.txt  \\\n0                     -1.456547             -0.317386   \n1                     -1.453486             -0.134716   \n2                     -1.376940             -0.145305   \n3                     -1.226910             -0.222080   \n4                     -1.225889             -0.237964   \n..                          ...                   ...   \n516                    1.653256             -3.340717   \n517                    1.653256             -3.340717   \n518                    1.161321             -3.541919   \n519                    1.434845             -3.224231   \n520                    1.473629             -3.253352   \n\n     Barclays PLC_035976.txt  ...  Standard Chartered PLC_035959.txt  \\\n0                   1.288425  ...                          -0.841939   \n1                   1.719911  ...                          -0.641924   \n2                   1.878207  ...                          -0.565862   \n3                   1.731811  ...                          -0.465855   \n4                   1.277106  ...                          -0.389793   \n..                       ...  ...                                ...   \n516                -2.628650  ...                          -1.847647   \n517                -2.628650  ...                          -1.847647   \n518                -2.662352  ...                          -1.995545   \n519                -2.455035  ...                          -1.723694   \n520                -2.435631  ...                          -1.606784   \n\n     Standard Life Aberdeen Plc_036365.txt  Taylor Wimpey PLC_036366.txt  \\\n0                                 0.109476                     -1.712247   \n1                                 0.235964                     -1.596214   \n2                                 0.226663                     -1.253914   \n3                                -0.054214                     -0.789779   \n4                                -0.039333                     -0.841994   \n..                                     ...                           ...   \n516                              -2.137541                     -2.842126   \n517                              -2.137541                     -2.842126   \n518                              -2.152421                     -2.898693   \n519                              -1.996172                     -2.675328   \n520                              -2.034304                     -2.713039   \n\n     Tesco PLC_035966.txt  TUI AG_02821N.txt  Unilever PLC_035922.txt  \\\n0                0.012647          -0.987098                -2.869013   \n1               -0.013695          -1.006700                -2.931364   \n2               -0.038155          -0.967495                -2.886828   \n3               -0.280875          -0.947893                -2.845854   \n4               -0.263941          -0.977297                -2.863669   \n..                    ...                ...                      ...   \n516             -0.621435          -1.151267                 0.237849   \n517             -0.621435          -1.151267                 0.237849   \n518             -0.638369          -1.325238                -0.063217   \n519             -0.542410          -1.180671                 0.097114   \n520             -0.617672          -1.170870                 0.038326   \n\n     United Utilities Group PLC_036341.txt  Vodafone Group PLC_035943.txt  \\\n0                                 0.733771                      -0.007477   \n1                                 0.710001                       0.100982   \n2                                 0.638689                       0.241340   \n3                                 0.724263                       0.400838   \n4                                 0.695738                       0.481651   \n..                                     ...                            ...   \n516                              -0.878827                      -1.936340   \n517                              -0.878827                      -1.936340   \n518                              -1.002434                      -2.063088   \n519                              -0.802761                      -1.932938   \n520                              -0.833187                      -2.004393   \n\n     Whitbread PLC_035895.txt  WPP PLC_035947.txt  \n0                   -0.898946            1.592327  \n1                   -0.881773            1.624480  \n2                   -0.713477            1.578037  \n3                   -0.469618            1.642343  \n4                   -0.332233            1.678069  \n..                        ...                 ...  \n516                  1.656414           -1.858805  \n517                  1.656414           -1.858805  \n518                  1.501856           -1.929542  \n519                  1.766322           -1.848087  \n520                  1.859057           -1.870951  \n\n[521 rows x 97 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>3i Group PLC_035999.txt</th>\n      <th>Admiral Group_036346.txt</th>\n      <th>Anglo American PLC_035918.txt</th>\n      <th>Antofagasta PLC_028149.txt</th>\n      <th>Ashtead Group_028090.txt</th>\n      <th>Associated British Foods PLC_035919.txt</th>\n      <th>Astrazeneca PLC_035998.txt</th>\n      <th>Aviva PLC_035907.txt</th>\n      <th>Barclays PLC_035976.txt</th>\n      <th>...</th>\n      <th>Standard Chartered PLC_035959.txt</th>\n      <th>Standard Life Aberdeen Plc_036365.txt</th>\n      <th>Taylor Wimpey PLC_036366.txt</th>\n      <th>Tesco PLC_035966.txt</th>\n      <th>TUI AG_02821N.txt</th>\n      <th>Unilever PLC_035922.txt</th>\n      <th>United Utilities Group PLC_036341.txt</th>\n      <th>Vodafone Group PLC_035943.txt</th>\n      <th>Whitbread PLC_035895.txt</th>\n      <th>WPP PLC_035947.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-01-02</td>\n      <td>-2.152305</td>\n      <td>-1.347635</td>\n      <td>-1.294240</td>\n      <td>-2.295047</td>\n      <td>-1.199292</td>\n      <td>0.108603</td>\n      <td>-1.456547</td>\n      <td>-0.317386</td>\n      <td>1.288425</td>\n      <td>...</td>\n      <td>-0.841939</td>\n      <td>0.109476</td>\n      <td>-1.712247</td>\n      <td>0.012647</td>\n      <td>-0.987098</td>\n      <td>-2.869013</td>\n      <td>0.733771</td>\n      <td>-0.007477</td>\n      <td>-0.898946</td>\n      <td>1.592327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-01-03</td>\n      <td>-2.053810</td>\n      <td>-1.189826</td>\n      <td>-1.286110</td>\n      <td>-2.136391</td>\n      <td>-1.136860</td>\n      <td>-0.002687</td>\n      <td>-1.453486</td>\n      <td>-0.134716</td>\n      <td>1.719911</td>\n      <td>...</td>\n      <td>-0.641924</td>\n      <td>0.235964</td>\n      <td>-1.596214</td>\n      <td>-0.013695</td>\n      <td>-1.006700</td>\n      <td>-2.931364</td>\n      <td>0.710001</td>\n      <td>0.100982</td>\n      <td>-0.881773</td>\n      <td>1.624480</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-01-04</td>\n      <td>-1.992251</td>\n      <td>-1.325091</td>\n      <td>-1.365373</td>\n      <td>-2.120526</td>\n      <td>-1.155222</td>\n      <td>-0.333285</td>\n      <td>-1.376940</td>\n      <td>-0.145305</td>\n      <td>1.878207</td>\n      <td>...</td>\n      <td>-0.565862</td>\n      <td>0.226663</td>\n      <td>-1.253914</td>\n      <td>-0.038155</td>\n      <td>-0.967495</td>\n      <td>-2.886828</td>\n      <td>0.638689</td>\n      <td>0.241340</td>\n      <td>-0.713477</td>\n      <td>1.578037</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-01-05</td>\n      <td>-1.924536</td>\n      <td>-1.336363</td>\n      <td>-1.355211</td>\n      <td>-2.035909</td>\n      <td>-1.122170</td>\n      <td>-0.087792</td>\n      <td>-1.226910</td>\n      <td>-0.222080</td>\n      <td>1.731811</td>\n      <td>...</td>\n      <td>-0.465855</td>\n      <td>-0.054214</td>\n      <td>-0.789779</td>\n      <td>-0.280875</td>\n      <td>-0.947893</td>\n      <td>-2.845854</td>\n      <td>0.724263</td>\n      <td>0.400838</td>\n      <td>-0.469618</td>\n      <td>1.642343</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-01-06</td>\n      <td>-1.912224</td>\n      <td>-1.313819</td>\n      <td>-1.395858</td>\n      <td>-2.094083</td>\n      <td>-1.155222</td>\n      <td>-0.185989</td>\n      <td>-1.225889</td>\n      <td>-0.237964</td>\n      <td>1.277106</td>\n      <td>...</td>\n      <td>-0.389793</td>\n      <td>-0.039333</td>\n      <td>-0.841994</td>\n      <td>-0.263941</td>\n      <td>-0.977297</td>\n      <td>-2.863669</td>\n      <td>0.695738</td>\n      <td>0.481651</td>\n      <td>-0.332233</td>\n      <td>1.678069</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>2018-12-25</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>517</th>\n      <td>2018-12-26</td>\n      <td>-1.509627</td>\n      <td>0.405175</td>\n      <td>1.060066</td>\n      <td>-1.271187</td>\n      <td>-0.967925</td>\n      <td>-2.068105</td>\n      <td>1.653256</td>\n      <td>-3.340717</td>\n      <td>-2.628650</td>\n      <td>...</td>\n      <td>-1.847647</td>\n      <td>-2.137541</td>\n      <td>-2.842126</td>\n      <td>-0.621435</td>\n      <td>-1.151267</td>\n      <td>0.237849</td>\n      <td>-0.878827</td>\n      <td>-1.936340</td>\n      <td>1.656414</td>\n      <td>-1.858805</td>\n    </tr>\n    <tr>\n      <th>518</th>\n      <td>2018-12-27</td>\n      <td>-1.310175</td>\n      <td>0.433355</td>\n      <td>0.958448</td>\n      <td>-1.406574</td>\n      <td>-1.175421</td>\n      <td>-2.195762</td>\n      <td>1.161321</td>\n      <td>-3.541919</td>\n      <td>-2.662352</td>\n      <td>...</td>\n      <td>-1.995545</td>\n      <td>-2.152421</td>\n      <td>-2.898693</td>\n      <td>-0.638369</td>\n      <td>-1.325238</td>\n      <td>-0.063217</td>\n      <td>-1.002434</td>\n      <td>-2.063088</td>\n      <td>1.501856</td>\n      <td>-1.929542</td>\n    </tr>\n    <tr>\n      <th>519</th>\n      <td>2018-12-28</td>\n      <td>-1.027002</td>\n      <td>0.861694</td>\n      <td>1.121038</td>\n      <td>-1.173878</td>\n      <td>-0.978943</td>\n      <td>-2.100838</td>\n      <td>1.434845</td>\n      <td>-3.224231</td>\n      <td>-2.455035</td>\n      <td>...</td>\n      <td>-1.723694</td>\n      <td>-1.996172</td>\n      <td>-2.675328</td>\n      <td>-0.542410</td>\n      <td>-1.180671</td>\n      <td>0.097114</td>\n      <td>-0.802761</td>\n      <td>-1.932938</td>\n      <td>1.766322</td>\n      <td>-1.848087</td>\n    </tr>\n    <tr>\n      <th>520</th>\n      <td>2018-12-31</td>\n      <td>-1.297863</td>\n      <td>1.132225</td>\n      <td>1.095023</td>\n      <td>-1.150609</td>\n      <td>-0.989960</td>\n      <td>-2.189215</td>\n      <td>1.473629</td>\n      <td>-3.253352</td>\n      <td>-2.435631</td>\n      <td>...</td>\n      <td>-1.606784</td>\n      <td>-2.034304</td>\n      <td>-2.713039</td>\n      <td>-0.617672</td>\n      <td>-1.170870</td>\n      <td>0.038326</td>\n      <td>-0.833187</td>\n      <td>-2.004393</td>\n      <td>1.859057</td>\n      <td>-1.870951</td>\n    </tr>\n  </tbody>\n</table>\n<p>521 rows × 97 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df =dict_of_imputed_dfs['TimesNet']\n",
    "missing = imputed_df.isnull().sum().sum()\n",
    "print(f\"Missing values count: {missing}\")\n",
    "imputed_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:26.707967Z",
     "start_time": "2025-05-23T12:44:26.686924Z"
    }
   },
   "id": "5aa6e17c3777bcdc",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac362e9e3b4b3159"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Missing value percentage': 3.2752072528280314,\n 'Mean absolute error': 0.3063827921796403,\n 'Mean square error': 0.17785181238002215,\n 'Root mean square error': 0.4217248064556105,\n 'Mean relative error': 0.38669484887531286,\n 'Euclidean Distance': 16.89006533609493,\n 'r2 score': 0.8098906878343326}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_scaled = df_scaled.set_index(df_scaled.columns[0])\n",
    "new_df_scaled = new_df_scaled.iloc[:, 2:].T\n",
    "new_df_scaled.columns.name = ''\n",
    "new_df_scaled.index.name = 'Date'\n",
    "\n",
    "new_df_missing = df_missing.set_index(df_missing.columns[0])\n",
    "new_df_missing = new_df_missing.iloc[:, 2:].T\n",
    "new_df_missing.columns.name = ''\n",
    "new_df_missing.index.name = 'Date'\n",
    "\n",
    "new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "new_imputed_df = new_imputed_df.iloc[:, 2:].T\n",
    "new_imputed_df.columns.name = ''\n",
    "new_imputed_df.index.name = 'Date'\n",
    "\n",
    "hyperTuned_metrics = tsi.compute_metrics(new_df_scaled, new_df_missing, new_imputed_df)\n",
    "hyperTuned_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:26.725964Z",
     "start_time": "2025-05-23T12:44:26.709707Z"
    }
   },
   "id": "8922dc79d44d2758",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'Missing value percentage': 3.2709532949456173,\n 'Mean absolute error': 0.5507467259643313,\n 'Mean square error': 0.5909499795177864,\n 'Root mean square error': 0.768732710061037,\n 'Mean relative error': 0.6992184510139345,\n 'Euclidean Distance': 31.093313855089477,\n 'r2 score': 0.3614790127096249}"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:26.734471Z",
     "start_time": "2025-05-23T12:44:26.727531Z"
    }
   },
   "id": "8dbd6417564d5daf",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-23T12:44:26.737932Z",
     "start_time": "2025-05-23T12:44:26.736134Z"
    }
   },
   "id": "27d6ed5f21a45316",
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
