{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAI time series imputation - Demonstration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gmarinos/.conda/envs/llm_ts_imp/lib/python3.8/site-packages/pypots/nn/modules/reformer/local_attention.py:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "/home/gmarinos/.conda/envs/llm_ts_imp/lib/python3.8/site-packages/pypots/nn/modules/reformer/local_attention.py:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import joblib \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from imputation import * \n",
    "\n",
    "\n",
    "country = 'france'\n",
    "\n",
    "df_missing = pd.read_csv(f'...../{country}/df_missing.csv')\n",
    "df_missing_scaled = pd.read_csv(f'..../{country}/df_missing_scaled.csv')\n",
    "low_missing_values_combined_lai_df = pd.read_csv(f'.../{country}/low_missing_values_combined_lai_df.csv')\n",
    "low_missing_values_series_array = np.load(f\"..../{country}/low_missing_values_series_array.npy\", allow_pickle=True)\n",
    "scaler = joblib.load(f'..../{country}/scaler.pkl')\n",
    "\n",
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    'train_params': {\n",
    "        \"gap_type\": \"random\",\n",
    "        \"miss_perc\": 0.1,\n",
    "        \"gap_length\": 7,\n",
    "        \"max_gap_length\": 7,\n",
    "        \"max_gap_count\": 3,\n",
    "        \"random_seed\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "train_params = parameters['train_params']\n",
    "\n",
    "df_scaled, scaler = dataframe_scaler(df_input=low_missing_values_combined_lai_df.drop(columns=['Unnamed: 0']), \n",
    "                                time_column=time_column, \n",
    "                                header=header, \n",
    "                                sep=sep, \n",
    "                                preprocessing=preprocessing, \n",
    "                                index=index)\n",
    "\n",
    "scaler = joblib.load(f'.../{country}/scaler.pkl')\n",
    "\n",
    "for i in [df_missing, df_missing_scaled, low_missing_values_combined_lai_df, df_scaled]:\n",
    "    if \"Unnamed: 0\" in i.columns:\n",
    "        i.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State of the art Deep Learning algorithms for time series imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'time_column' : 'time',\n",
    "    'sep' : ',',\n",
    "    'header' : 0,\n",
    "    'is_multivariate': False,\n",
    "    'areaVStime': 0,\n",
    "    'preprocessing': False,\n",
    "    'index': False,\n",
    "    \"algorithms\": [\"TimesNet\", \n",
    "                   \"saits\", \n",
    "                   \"csdi\",\n",
    "                   ],\n",
    "    \"params\": { \n",
    "        \"saits\": {\n",
    "            \"n_layers\": 2,\n",
    "            \"d_model\": 32,\n",
    "            \"d_ffn\": 128,\n",
    "            \"n_heads\": 2,\n",
    "            \"d_k\": 16,\n",
    "            \"d_v\": 128,\n",
    "            \"dropout\": 0,\n",
    "            \"epochs\": 200,\n",
    "            \"batch_size\": 32,\n",
    "            \"ORT_weight\": 1,\n",
    "            \"MIT_weight\": 1,\n",
    "            \"attn_dropout\": 0.3,\n",
    "            \"diagonal_attention_mask\": True,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 20,\n",
    "            \"lr\": 0.001224014111277117,\n",
    "            \"device\": \"cuda:1\"\n",
    "        },\n",
    "        \"TimesNet\": {\n",
    "            \"n_layers\": 1,\n",
    "            \"d_model\": 128,\n",
    "            \"d_ffn\": 32,\n",
    "            \"n_heads\": 1,\n",
    "            \"top_k\": 2,\n",
    "            \"n_kernels\": 3,\n",
    "            \"dropout\": 0.3,\n",
    "            \"epochs\": 200,\n",
    "            \"batch_size\": 32,\n",
    "            \"attn_dropout\": 0.4,\n",
    "            \"apply_nonstationary_norm\": True,\n",
    "            \"num_workers\": 0,\n",
    "            \"patience\": 20,\n",
    "            \"lr\": 0.0015351577260316839,\n",
    "            \"device\": \"cuda:1\"\n",
    "        },\n",
    "\t\t\"csdi\":\n",
    "\t\t{\n",
    "\t\t\t\"n_layers\": 4,\n",
    "\t\t\t\"n_channels\": 32,\n",
    "\t\t\t\"n_heads\": 4,\n",
    "\t\t\t\"d_time_embedding\": 64,\n",
    "\t\t\t\"d_feature_embedding\": 3,\n",
    "\t\t\t\"d_diffusion_embedding\": 64,\n",
    "\t\t\t\"is_unconditional\": False,\n",
    "\t\t\t\"beta_start\": 0.0001,\n",
    "\t\t\t\"beta_end\": 0.1,\n",
    "\t\t\t\"epochs\": 10,\n",
    "\t\t\t\"batch_size\": 32,\n",
    "\t\t\t\"n_diffusion_steps\": 50,\n",
    "\t\t\t\"num_workers\": 0,\n",
    "\t\t\t\"patience\": 3,\n",
    "\t\t\t\"lr\": 0.001,\n",
    "\t\t\t\"device\":  \"cuda:1\"\n",
    "\t\t},\n",
    "    }\n",
    "}\n",
    "\n",
    "time_column = parameters['time_column']\n",
    "header = parameters['header']\n",
    "sep = parameters['sep']\n",
    "is_multivariate = parameters['is_multivariate']\n",
    "areaVStime = parameters['areaVStime']\n",
    "preprocessing = parameters['preprocessing']\n",
    "index = parameters['index']\n",
    "algorithms = parameters['algorithms']\n",
    "params = parameters['params']\n",
    "\n",
    "dict_of_imputed_dfs = run_imputation(missing = df_missing_scaled, ##df_missing,\n",
    "                                         algorithms=algorithms, \n",
    "                                         params=params, \n",
    "                                         time_column=time_column,\n",
    "                                         header=header, \n",
    "                                         sep=sep, \n",
    "                                         is_multivariate=is_multivariate, \n",
    "                                         areaVStime=areaVStime, \n",
    "                                         preprocessing=preprocessing, \n",
    "                                         index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = 'both'\n",
    "variable_name = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if scaling == False: \n",
    "\n",
    "    ## Check if the variable exists in the global namespace #it could be also the locals()\n",
    "    if variable_name in globals():\n",
    "        print(f\"The variable '{variable_name}' exists.\")\n",
    "        pass\n",
    "    else: \n",
    "        print(f\"The variable '{variable_name}' does not exist. We create it\")\n",
    "        results = {}\n",
    "\n",
    "    for i in dict_of_imputed_dfs.keys():\n",
    "\n",
    "        new_df = low_missing_values_combined_lai_df.set_index(df_missing.columns[0])\n",
    "\n",
    "        new_df_missing = df_missing[low_missing_values_series_array].set_index(df_missing.columns[0])\n",
    "\n",
    "        imputed_df = dict_of_imputed_dfs[i][low_missing_values_series_array]\n",
    "\n",
    "        new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "\n",
    "        results[i+\"_\"+\"tuned\"] = dict(compute_metrics(new_df, new_df_missing, new_imputed_df))\n",
    "\n",
    "        print(i, compute_metrics(new_df, new_df_missing, new_imputed_df), '\\n')\n",
    "\n",
    "\n",
    "elif scaling == True:\n",
    "\n",
    "    dict_of_imputed_dfs_inversed = {}\n",
    "\n",
    "    for key, values in dict_of_imputed_dfs.items():\n",
    "\n",
    "        dict_of_imputed_dfs_inversed[key] = dataframe_inverse_scaler(df_input=dict_of_imputed_dfs[key], \n",
    "                                                        scaler=scaler,\n",
    "                                                        time_column=time_column, \n",
    "                                                        header=header, \n",
    "                                                        sep=sep, \n",
    "                                                        preprocessing=preprocessing, \n",
    "                                                        index=index)\n",
    "\n",
    "    df_scaled_inversed = dataframe_inverse_scaler(df_input=df_scaled, \n",
    "                                                        scaler=scaler,\n",
    "                                                        time_column=time_column, \n",
    "                                                        header=header, \n",
    "                                                        sep=sep, \n",
    "                                                        preprocessing=preprocessing, \n",
    "                                                        index=index)\n",
    "\n",
    "    df_missing_inversed = dataframe_inverse_scaler(df_input=df_missing, \n",
    "                                                        scaler=scaler,\n",
    "                                                        time_column=time_column, \n",
    "                                                        header=header, \n",
    "                                                        sep=sep, \n",
    "                                                        preprocessing=preprocessing, \n",
    "                                                        index=index)\n",
    "\n",
    "    ########################\n",
    "\n",
    "    ## Check if the variable exists in the global namespace #it could be also the locals()\n",
    "    if variable_name in globals():\n",
    "        print(f\"The variable '{variable_name}' exists.\")\n",
    "        pass\n",
    "    else: \n",
    "        print(f\"The variable '{variable_name}' does not exist. We create it\")\n",
    "        results = {}\n",
    "\n",
    "    for i in dict_of_imputed_dfs_inversed.keys():\n",
    "\n",
    "        # new_df_scaled = df_scaled[low_missing_values_series_array].set_index(df_scaled.columns[0])\n",
    "\n",
    "        # new_df_missing = df_missing[low_missing_values_series_array].set_index(df_missing.columns[0])\n",
    "\n",
    "        new_df_scaled = df_scaled_inversed[low_missing_values_series_array].set_index(df_scaled.columns[0])\n",
    "\n",
    "        new_df_missing = df_missing_inversed[low_missing_values_series_array].set_index(df_missing.columns[0])\n",
    "\n",
    "        imputed_df = dict_of_imputed_dfs_inversed[i][low_missing_values_series_array]\n",
    "\n",
    "        new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "\n",
    "        results[i+\"_\"+\"tuned\"] = dict(compute_metrics(new_df_scaled, new_df_missing, new_imputed_df))\n",
    "\n",
    "        print(i, compute_metrics(new_df_scaled, new_df_missing, new_imputed_df), '\\n')\n",
    "\n",
    "    #########################\n",
    "\n",
    "elif scaling == 'both':\n",
    "\n",
    "    dict_of_imputed_dfs_inversed = {}\n",
    "\n",
    "    for key, values in dict_of_imputed_dfs.items():\n",
    "\n",
    "        dict_of_imputed_dfs_inversed[key] = dataframe_inverse_scaler(df_input=dict_of_imputed_dfs[key], \n",
    "                                                        scaler=scaler,\n",
    "                                                        time_column=time_column, \n",
    "                                                        header=header, \n",
    "                                                        sep=sep, \n",
    "                                                        preprocessing=preprocessing, \n",
    "                                                        index=index)\n",
    "\n",
    "    df_scaled_inversed = dataframe_inverse_scaler(df_input=df_scaled, \n",
    "                                                        scaler=scaler,\n",
    "                                                        time_column=time_column, \n",
    "                                                        header=header, \n",
    "                                                        sep=sep, \n",
    "                                                        preprocessing=preprocessing, \n",
    "                                                        index=index)\n",
    "\n",
    "    df_missing_inversed = dataframe_inverse_scaler(df_input=df_missing_scaled, \n",
    "                                                        scaler=scaler,\n",
    "                                                        time_column=time_column, \n",
    "                                                        header=header, \n",
    "                                                        sep=sep, \n",
    "                                                        preprocessing=preprocessing, \n",
    "                                                        index=index)\n",
    "\n",
    "    ########################\n",
    "\n",
    "    ## Check if the variable exists in the global namespace #it could be also the locals()\n",
    "    if variable_name in globals():\n",
    "        print(f\"The variable '{variable_name}' exists.\")\n",
    "        pass\n",
    "    else: \n",
    "        print(f\"The variable '{variable_name}' does not exist. We create it\")\n",
    "        results = {} \n",
    "\n",
    "    for i in dict_of_imputed_dfs_inversed.keys():\n",
    "\n",
    "        # new_df_scaled = df_scaled[low_missing_values_series_array].set_index(df_scaled.columns[0])\n",
    "\n",
    "        # new_df_missing = df_missing[low_missing_values_series_array].set_index(df_missing.columns[0])\n",
    "\n",
    "        new_df_scaled = df_scaled_inversed[low_missing_values_series_array].set_index(df_scaled.columns[0])\n",
    "\n",
    "        new_df_missing = df_missing_inversed[low_missing_values_series_array].set_index(df_missing_scaled.columns[0])\n",
    "\n",
    "        imputed_df = dict_of_imputed_dfs_inversed[i][low_missing_values_series_array]\n",
    "\n",
    "        new_imputed_df = imputed_df.set_index(imputed_df.columns[0])\n",
    "\n",
    "        results[i+\"_\"+\"tuned\"] = dict(compute_metrics(new_df_scaled, new_df_missing, new_imputed_df))\n",
    "\n",
    "        print(i, compute_metrics(new_df_scaled, new_df_missing, new_imputed_df), '\\n')\n",
    "\n",
    "    #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-based time series imputation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import *\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from imputation import * \n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "import os \n",
    "import uuid\n",
    "from typing import Dict\n",
    "from utility_functions import * \n",
    "import time \n",
    "from tenacity import RetryError\n",
    "from langchain_core.messages import (AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage)\n",
    "import json \n",
    "from datetime import datetime\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "import argparse\n",
    "from langchain_ollama import ChatOllama\n",
    "from ollama import chat\n",
    "from pydantic import BaseModel\n",
    "from typing import List \n",
    "import math \n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain import hub\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "import geopandas as gpd \n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from LLM4TS_imputation.examples_creation import generate_llm_examples\n",
    "from LLM4TS_imputation.utils import handle_llm_output\n",
    "\n",
    "\n",
    "def inspect(state):\n",
    "    \"\"\"Print the state passed between Runnables in a langchain and pass it on\"\"\"\n",
    "    print(state)\n",
    "    return state\n",
    "\n",
    "\n",
    "def impute_missing_values(series):\n",
    "    return np.where(np.isnan(series), np.nanmean(series), series)\n",
    "\n",
    "# Example: Convert a time series into a fixed-length embedding\n",
    "def extract_features(series):\n",
    "    return np.array([\n",
    "        np.mean(series),\n",
    "        np.std(series),\n",
    "        np.min(series),\n",
    "        np.max(series),\n",
    "        np.percentile(series, 25),\n",
    "        np.percentile(series, 50),\n",
    "        np.percentile(series, 75)\n",
    "    ])\n",
    "\n",
    "# Extract embeddings for all time series in the dataset\n",
    "def build_embeddings(time_series_data):\n",
    "    return np.array([extract_features(ts) for ts in time_series_data])\n",
    "    #return np.array([ts for ts in time_series_data])\n",
    "    #return np.array([impute_missing_values(ts) for ts in time_series_data])\n",
    "\n",
    "# Build a FAISS index\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)  # L2 distance\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Query FAISS for k most similar time series\n",
    "def query_similar_time_series(query_series, index, time_series_data, k):\n",
    "    query_embedding = extract_features(query_series).reshape(1, -1)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [time_series_data[i] for i in indices[0]], indices, distances\n",
    "\n",
    "def build_rag(df_missing):\n",
    "    #TODO comment out the line below, because this was originally used \n",
    "    #time_series_data = df_missing.drop(columns=['time']).dropna().T.values  #.drop(columns=['time', serie])\n",
    "    time_series_data = df_missing.drop(columns=['time']).interpolate(method='linear').T.values\n",
    "    column_names = df_missing.drop(columns=['time']).columns  # Store column names #.drop(columns=['time', serie])\n",
    "    name_to_index = {name: idx for idx, name in enumerate(column_names)}  # Mapping column names to numeric indices\n",
    "    index_to_name = {idx: name for name, idx in name_to_index.items()}  # Reverse mapping for lookup\n",
    "    embeddings = build_embeddings(time_series_data)\n",
    "    index = build_faiss_index(embeddings)\n",
    "    return time_series_data, column_names, name_to_index, index_to_name, embeddings, index\n",
    "\n",
    "def build_rag(df_missing):\n",
    "    df_missing = df_missing.interpolate(method='linear')\n",
    "    time_series_data = df_missing.drop(columns=['time']).dropna().T.values  #.drop(columns=['time', serie])\n",
    "    column_names = df_missing.drop(columns=['time']).columns  # Store column names #.drop(columns=['time', serie])\n",
    "    name_to_index = {name: idx for idx, name in enumerate(column_names)}  # Mapping column names to numeric indices\n",
    "    index_to_name = {idx: name for name, idx in name_to_index.items()}  # Reverse mapping for lookup\n",
    "    embeddings = build_embeddings(time_series_data)\n",
    "    index = build_faiss_index(embeddings)\n",
    "    return time_series_data, column_names, name_to_index, index_to_name, embeddings, index\n",
    "\n",
    "\n",
    "####################\n",
    "### LLM prompting \n",
    "####################\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(3))\n",
    "def make_request(df_missing, serie, missing_date, chain):\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    human_message_content = f\"\"\"The following array represents the Leaf Area Index values for a crop for certain dates: [{df_missing[['time', serie]].dropna().values}]. \n",
    "    Estimate the float number of Leaf Area Index value for the requested date {missing_date[0]}. Return the float LAI value, the date itself, and give explanation.\"\"\"\n",
    "    \n",
    "    print('PROMPT GIVEN TO LLM:', human_message_content)\n",
    "\n",
    "    # Invoke the model with langchain\n",
    "    llm_response = chain.invoke({\"msgs\": [HumanMessage( content = human_message_content)]})  \n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return llm_response, elapsed_time\n",
    "\n",
    "#######################################################\n",
    "\n",
    "class AnswerandJustification(BaseModel):\n",
    "    '''An answer to the user question '''\n",
    "\n",
    "    #answer: str =  Field(description=\"The list of data and lai value pari that you were given but with imputed the missing value.\")\n",
    "    answer_value: float #= Field(description=\"The leaf area index float\")\n",
    "    answer_date: str #= Field(description=\"The requested date\")\n",
    "    answer_explanation: str #= Field(description=\"Explaination of the missing values fullfillment\")\n",
    "\n",
    "def create_examples(example, requested_date):\n",
    "    msgs=f\"The following array represents the Leaf Area Index values for a crop for certain dates: {example}. Estimate the float number Leaf Area Index value for the requested date {requested_date[0]}. Return the float LAI value, the date itself and give explanation.\"\n",
    "    answer_list_length = AnswerandJustification(\n",
    "    msgs=f\"The following array represents the Leaf Area Index values for a crop for certain dates: {example}. Estimate the float number Leaf Area Index value for the requested date {requested_date[0]}. Return the float LAI value, the date itself and give explanation.\",                   \n",
    "    answer_date = requested_date[0], \n",
    "    answer_value = requested_date[1],\n",
    "    answer_explanation = f'I guess that the missing LAI value for that date is {requested_date[1]} because of the season'\n",
    "    )\n",
    "    print('EXAMPLE MESSAGES -> create_examples function:', 'answer_date:', requested_date[0], 'answer_value:', requested_date[1])\n",
    "    return msgs, answer_list_length\n",
    "\n",
    "def tool_example_to_messages(example: Dict) -> List[BaseMessage]:\n",
    "    messages: List[BaseMessage] = [HumanMessage(content=example[\"input\"])]\n",
    "    openai_tool_calls = []\n",
    "    for tool_call in example[\"tool_calls\"]:\n",
    "        openai_tool_calls.append(\n",
    "            {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call.__class__.__name__,\n",
    "                    \"arguments\": tool_call.json(),\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    messages.append(\n",
    "        AIMessage(content=\"\", additional_kwargs={\"tool_calls\": openai_tool_calls})\n",
    "    )\n",
    "    tool_outputs = example.get(\"tool_outputs\") or [\n",
    "        \"You have correctly called this tool.\"\n",
    "    ] * len(openai_tool_calls)\n",
    "    for output, tool_call in zip(tool_outputs, openai_tool_calls):\n",
    "        messages.append(ToolMessage(content=output, tool_call_id=tool_call[\"id\"]))\n",
    "    return messages\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "#example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]\n",
    "\n",
    "mask = find_new_nan_mask(df1=low_missing_values_combined_lai_df, df2=df_missing)\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "##############\n",
    "## Groq API \n",
    "############## \n",
    "# model_name = \"llama3-70b-8192\" # \"llama-3.3-70b-versatile\" #\"llama3-70b-8192\" #llama-3.1-8b-instant\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"put your api key here\"\n",
    "# model = ChatGroq(temperature=0, model=model_name)\n",
    "\n",
    "##############\n",
    "## Ollama\n",
    "##############\n",
    "model_name = 'llama3.1'\n",
    "model = ChatOllama(\n",
    "                model=model_name,\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                keep_alive=-1,\n",
    "                format=\"json\" , \n",
    "                base_url = 'if you run with Ollama you need to install locally Ollama and put the base url here'\n",
    "            )\n",
    "\n",
    "structured_model = model.with_structured_output(AnswerandJustification)\n",
    "\n",
    "###########################\n",
    "## RAG\n",
    "###########################\n",
    "\n",
    "time_series_data, column_names, name_to_index, index_to_name, embeddings, index = build_rag(df_missing)\n",
    "\n",
    "\n",
    "###########################\n",
    "## Main Loop \n",
    "###########################\n",
    "\n",
    "llm_responses = {}\n",
    "llm_responses_time_per_serie = {}\n",
    "llm_explanations = {}\n",
    "distances_dict = {}\n",
    "\n",
    "for i, (serie, serie_values) in enumerate(df_missing.items()):\n",
    "\n",
    "    if serie != 'time':\n",
    "\n",
    "        llm_responses[serie] = []\n",
    "\n",
    "        a = df_missing[['time', serie]]\n",
    "        b = mask[['time', serie]]\n",
    "\n",
    "        llm_responses_time_per_serie[serie] = []\n",
    "\n",
    "        distances_dict[serie] = []\n",
    "        \n",
    "        for missing_date in a[b.iloc[:, 1]].values:\n",
    "            \n",
    "            print('\\n')\n",
    "            print('ITERATION:', i, 'SERIE:', serie, 'MISSING DATE:', missing_date)\n",
    "\n",
    "            ###########################\n",
    "            ## RAG usage\n",
    "            ###########################\n",
    "            query_series = df_missing[serie].dropna().values.flatten()\n",
    "\n",
    "            k = 2\n",
    "            similar_series, indices, distances = query_similar_time_series(query_series, index, time_series_data, k)\n",
    "            similar_series_names = [index_to_name[idx] for idx in indices[0]]  # Convert numeric indices to column names\n",
    "            distances_dict[serie].append([missing_date[0], distances[0]])\n",
    "\n",
    "            examples = []\n",
    "            context_examples_list = []\n",
    "            context_examples = {}\n",
    "            num_of_examples = 0\n",
    "\n",
    "            for i in range(len(similar_series_names)):\n",
    "                \n",
    "                #TODO comment out the below line of code \n",
    "                fin_arr = df_missing[['time', similar_series_names[i]]].dropna().values\n",
    "                #fin_arr = combined_lai[['time', similar_series_names[k]]].dropna().values\n",
    "\n",
    "                context_ex = fin_arr[fin_arr[:, 0]==missing_date[0]]\n",
    "\n",
    "                if len(context_ex) == 0:\n",
    "                    print(f'EXAMPLE MESSAGES -> for serie {similar_series_names[i]} Didnt find the same date in the available data')\n",
    "                    # Define the date to match\n",
    "                    date_to_match = missing_date[0]\n",
    "\n",
    "                    # Convert the target date to a datetime object\n",
    "                    target_date = datetime.strptime(date_to_match, '%Y-%m-%d')\n",
    "\n",
    "                    # Convert the first column of dates to datetime objects\n",
    "                    dates = np.array([datetime.strptime(date, '%Y-%m-%d') for date in fin_arr[:, 0]])\n",
    "\n",
    "                    # Find the index of the closest date\n",
    "                    date_diffs = np.abs(dates - target_date)\n",
    "                    closest_index = np.argmin(date_diffs)\n",
    "\n",
    "                    # Get the row with the closest date\n",
    "                    # closest_row = fin_arr[closest_index]\n",
    "                    # context_ex = fin_arr[fin_arr[:, 0]==closest_row[0]]\n",
    "\n",
    "                    random_row = fin_arr[closest_index]\n",
    "                    fin_arr = fin_arr[fin_arr[:, 0]==random_row[0]]\n",
    "\n",
    "                else:\n",
    "                    print(f'EXAMPLE MESSAGES -> for serie {similar_series_names[i]} Found the same date in the available data')\n",
    "                    ##---------------------------\n",
    "                    ##TODO to be removed this else statement, if I want to return to the previous implementation  \n",
    "                    ##---------------------------\n",
    "                    ## choose the instance that is similar in date and exlude it in order to create example \n",
    "                    random_row = context_ex[0]\n",
    "                    # Delete rows where the first column matches missing_date[0]\n",
    "                    fin_arr = fin_arr[fin_arr[:, 0] != missing_date[0]]\n",
    "\n",
    "                ## number of examples that i want to give to the llm\n",
    "                if num_of_examples < 2:\n",
    "                    msgs, answer_list_length = create_examples(example=fin_arr, requested_date=random_row)\n",
    "                    examples.append({\"input\": msgs, \"tool_calls\": [answer_list_length]})\n",
    "                    num_of_examples +=1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            example_msgs = [msg for ex in examples for msg in tool_example_to_messages(ex)]\n",
    "            print('EXAMPLE MESSAGES:', example_msgs)\n",
    "\n",
    "            #####################################\n",
    "            ## Prompt template\n",
    "            #####################################\n",
    "            template = \"\"\"\n",
    "            human: {msgs}\n",
    "            system: \"You are a data scientist specialized in ecological timeseries modeling.\"\n",
    "            \"\"\"\n",
    "            prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "            ##########################################################\n",
    "            ## Chain generation using examples retrieved with RAG \n",
    "            ##########################################################\n",
    "\n",
    "            chain = {\"msgs\": RunnablePassthrough()}| RunnableLambda(inspect)  | prompt.partial(examples=example_msgs) | structured_model \n",
    "\n",
    "            ######################################\n",
    "            ## # Invoke the model\n",
    "            ######################################\n",
    "\n",
    "            try:\n",
    "                # Invoke the model\n",
    "                llm_response, elapsed_time = make_request(df_missing, serie, missing_date, chain)\n",
    "\n",
    "                print('llm_response answer_value:', llm_response.answer_value)\n",
    "                print('llm_response answer_date:', llm_response.answer_date)\n",
    "                print('llm_response answer_explanation:', llm_response.answer_explanation)\n",
    "\n",
    "                print('similarity distances:', distances)\n",
    "                print('Real value:', low_missing_values_combined_lai_df.loc[low_missing_values_combined_lai_df[\"time\"] == missing_date[0], serie].iloc[0])\n",
    "                \n",
    "                llm_explanations[serie] = llm_response.answer_explanation\n",
    "                response = [llm_response.answer_date, llm_response.answer_value]\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle exceptions gracefully\n",
    "                response = [missing_date[0], e]\n",
    "                print(f\"Error: {e}\")\n",
    "            finally:\n",
    "                # Always record the response and elapsed time\n",
    "                print('RESPONSE:', response)\n",
    "                llm_responses[serie].append(response)\n",
    "                llm_responses_time_per_serie[serie].append([missing_date[0], elapsed_time])\n",
    "\n",
    "\n",
    "#############################\n",
    "### Handling the LLMs ouput \n",
    "#############################\n",
    "after_handling_output = handle_llm_output(df_missing, llm_responses)\n",
    "df_imputed = after_handling_output['df_imputed']\n",
    "llm_responses_updated = after_handling_output['llm_responses_updated']\n",
    "number_of_error_prediction_from_LLM = after_handling_output['number_of_error_prediction_from_LLM']\n",
    "number_of_error_prediction_from_LLM_list = after_handling_output['number_of_error_prediction_from_LLM_list']\n",
    "number_of_none_prediction_from_LLM = after_handling_output['number_of_none_prediction_from_LLM']\n",
    "number_of_error_prediction_from_LLM_list = after_handling_output['number_of_error_prediction_from_LLM_list']\n",
    "\n",
    "print('Number of Errors collected in the LLMs response:', number_of_error_prediction_from_LLM, '\\n',\n",
    "          'Number of None collected from LLM:', number_of_none_prediction_from_LLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "sorted_fig = 'True'\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Bar plot for each metric\n",
    "metrics = df.index\n",
    "\n",
    "# Create a 3x3 grid for 6 plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axes for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through the metrics to create bar plots\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "    if sorted_fig == 'True':\n",
    "        # Sort the data for the current metric\n",
    "        sorted_data = df.loc[metric].sort_values(ascending=False)\n",
    "        sorted_data.plot(kind='bar', ax=ax, title=metric)\n",
    "    else: \n",
    "        df.loc[metric].plot(kind='bar', ax=ax, title=metric)\n",
    "    ax.set_ylabel(metric)\n",
    "\n",
    "# Hide any unused subplots (if fewer than 9 plots)\n",
    "for j in range(len(metrics), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout to avoid overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_ts_imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
